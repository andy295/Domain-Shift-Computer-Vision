{"cells":[{"cell_type":"markdown","metadata":{"id":"rn1W7CwqyFWZ"},"source":["# **Deep Learning - Test-Time Adaptation**\n","---\n","**University of Trento, Academic Year 2023/2024**\n","\n","---\n","#### **Group 26**\n","> <a href=\"https://github.com/giuseppecurci\">Giuseppe Curci</a> \\\n","> 243049\n","\n","> <a href=\"https://github.com/andy295\">Andrea Cristiano</a> \\\n","> 229370\n","---\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CIljqMXC6NuG"},"outputs":[],"source":["# to replace with a requirements file\n","\n","!pip install ollama # if ollama is not available, install by executing\n","                    # the install_and_run_ollama.sh script\n","!pip install diffusers\n","!pip install bing_image_downloader\n","!pip install git+https://github.com/openai/CLIP.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f686ZK2OOXAc"},"outputs":[],"source":["import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset\n","import torchvision.transforms as T\n","import torchvision.models as models\n","from scipy import stats\n","import math\n","\n","from typing import List, Union, Dict\n","from PIL import Image\n","from tqdm import tqdm\n","from io import BytesIO\n","from pathlib import Path\n","import boto3 # read and write for AWS buckets\n","import json\n","import os\n","import random\n","import time\n","\n","from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, DPMSolverMultistepScheduler\n","import ollama\n","import clip\n","from bing_image_downloader import downloader\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"h62t4jTDPk12"},"source":["## **Introduction**\n","\n","### **Domain Shift**\n","Domain shift refers to the change in the data distribution between the training phase and the testing phase of a machine learning model. In other words, the data that the model encounters during deployment (inference) differs from the data it was trained on.\n","\n","Machine learning models are typically trained on a specific dataset. The fundamental assumption is that the training data distribution is as representative as possible of what the models will encounter during deployment. In the presence of domain shift, the model may not generalize well, leading to a degradation in performance.\n","\n","In real-world applications, it is easy to identify cases where data distributions may vary, and the causes can be numerous, such as changes in the environment, noise in the sensors used to acquire information, etc.\n","\n","A model trained on images depicting a scene during the spring season may perform poorly when tested on images of the same scene but taken during the winter season, even if the scene's content is the same.\n","\n","### **Test-Time Adaptation (TTA)**\n","Test-Time Adaptation refers to techniques that allow a previously trained model to adapt to a new data distribution during inference (testing), without the need for a new training phase.\n","\n","The goal is to improve the model's performance on the shifted domain by adapting its parameters or predictions based on the data encountered during testing."]},{"cell_type":"markdown","metadata":{"id":"PicQiiSaPzCV"},"source":["## **Pipeline**\n","\n","Before explaining the methods implemented, the following picture provides an overview of how predictions are made by our models.\n","\n","<p align=\"center\">\n","  <img src=\"images/prediction_pipeline_colored.png\" width=\"800\" height=\"400\">  \n","</p>\n","\n","1. **Image Classification and Generation**: Given a sample image for classification, the `top_j` images, previously generated using a diffusion model and a LLM, are retrieved based on cosine similarity between `CLIP` image embeddings. Details of this generation process are elaborated in subsequent sections.\n","\n","2. **Augmentation and Confidence Filtering**: `k` augmentations of the original image are generated, and the corresponding probability tensors are computed. An entropy-based confidence selection filter is then applied to identify the `top_aug` augmentations.\n","\n","3. **Marginal Output Distribution**: The probabilities of the generated images and the augmentations are combined, and the marginal output distribution is computed (refer to MEMO for more details).\n","\n","4. **Model Update**: The model is updated by minimizing the entropy of the marginal output distribution tensor. Multiple updates may occur, but the augmentations remain unchanged.\n","\n","5. **Final Prediction**: The updated model computes new probabilities on the previous augmentations (TTA). These are filtered again using the confidence selection mechanism, and the final prediction is obtained by applying `softmax` and `argmax` on the marginal output distribution. Note: Generated images are used only until the update step, with TTA performed exclusively on the augmentations.\n"]},{"cell_type":"markdown","metadata":{"id":"Ge0jVaTAWPYt"},"source":["## **Utils**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QdLmYm862XSu"},"source":["### **ImageNet-A masking**\n","\n","The **imagenetA_masking.json** file provides a masking for ImageNet-A dataset indices to the standard 1000-class ImageNet output indices used by pre-trained models in PyTorch's torchvision library.\n","\n","Each key in the file corresponds to an index in the standard 1000-class ImageNet output vector. The value associated with each key indicates whether that index should be considered when mapping the 1000-class output to a the smaller set of classes ImageNet-A.\n","\n","A value of -1 indicates that the corresponding index in the 1000-class output should be ignored in the subset of outputs for ImageNet-A.\n","A non-negative integer value indicates that the corresponding index in the 1000-class output should be included in the subset of outputs for ImageNet-A."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["imagenetA_masking = {\n","    \"0\": -1,\n","    \"1\": -1,\n","    \"2\": -1,\n","    \"3\": -1,\n","    \"4\": -1,\n","    \"5\": -1,\n","    \"6\": 0,\n","    \"7\": -1,\n","    \"8\": -1,\n","    \"9\": -1,\n","    \"10\": -1,\n","    \"11\": 1,\n","    \"12\": -1,\n","    \"13\": 2,\n","    \"14\": -1,\n","    \"15\": 3,\n","    \"16\": -1,\n","    \"17\": 4,\n","    \"18\": -1,\n","    \"19\": -1,\n","    \"20\": -1,\n","    \"21\": -1,\n","    \"22\": 5,\n","    \"23\": 6,\n","    \"24\": -1,\n","    \"25\": -1,\n","    \"26\": -1,\n","    \"27\": 7,\n","    \"28\": -1,\n","    \"29\": -1,\n","    \"30\": 8,\n","    \"31\": -1,\n","    \"32\": -1,\n","    \"33\": -1,\n","    \"34\": -1,\n","    \"35\": -1,\n","    \"36\": -1,\n","    \"37\": 9,\n","    \"38\": -1,\n","    \"39\": 10,\n","    \"40\": -1,\n","    \"41\": -1,\n","    \"42\": 11,\n","    \"43\": -1,\n","    \"44\": -1,\n","    \"45\": -1,\n","    \"46\": -1,\n","    \"47\": 12,\n","    \"48\": -1,\n","    \"49\": -1,\n","    \"50\": 13,\n","    \"51\": -1,\n","    \"52\": -1,\n","    \"53\": -1,\n","    \"54\": -1,\n","    \"55\": -1,\n","    \"56\": -1,\n","    \"57\": 14,\n","    \"58\": -1,\n","    \"59\": -1,\n","    \"60\": -1,\n","    \"61\": -1,\n","    \"62\": -1,\n","    \"63\": -1,\n","    \"64\": -1,\n","    \"65\": -1,\n","    \"66\": -1,\n","    \"67\": -1,\n","    \"68\": -1,\n","    \"69\": -1,\n","    \"70\": 15,\n","    \"71\": 16,\n","    \"72\": -1,\n","    \"73\": -1,\n","    \"872\": -1,\n","    \"873\": -1,\n","    \"874\": -1,\n","    \"875\": -1,\n","    \"876\": -1,\n","    \"877\": -1,\n","    \"878\": -1,\n","    \"879\": 171,\n","    \"880\": 172,\n","    \"881\": -1,\n","    \"882\": -1,\n","    \"883\": -1,\n","    \"884\": -1,\n","    \"885\": -1,\n","    \"886\": -1,\n","    \"887\": -1,\n","    \"888\": 173,\n","    \"889\": -1,\n","    \"890\": 174,\n","    \"891\": -1,\n","    \"892\": -1,\n","    \"893\": -1,\n","    \"894\": -1,\n","    \"895\": -1,\n","    \"896\": -1,\n","    \"897\": 175,\n","    \"898\": -1,\n","    \"899\": -1,\n","    \"900\": 176,\n","    \"901\": -1,\n","    \"902\": -1,\n","    \"903\": -1,\n","    \"904\": -1,\n","    \"905\": -1,\n","    \"906\": -1,\n","    \"907\": 177,\n","    \"908\": -1,\n","    \"909\": -1,\n","    \"910\": -1,\n","    \"911\": -1,\n","    \"912\": -1,\n","    \"913\": 178,\n","    \"914\": -1,\n","    \"915\": -1,\n","    \"916\": -1,\n","    \"917\": -1,\n","    \"918\": -1,\n","    \"919\": -1,\n","    \"920\": -1,\n","    \"921\": -1,\n","    \"922\": -1,\n","    \"923\": -1,\n","    \"924\": 179,\n","    \"925\": -1,\n","    \"926\": -1,\n","    \"927\": -1,\n","    \"928\": -1,\n","    \"929\": -1,\n","    \"930\": -1,\n","    \"931\": -1,\n","    \"932\": 180,\n","    \"933\": 181,\n","    \"934\": 182,\n","    \"935\": -1,\n","    \"936\": -1,\n","    \"937\": 183,\n","    \"938\": -1,\n","    \"939\": -1,\n","    \"940\": -1,\n","    \"941\": -1,\n","    \"942\": -1,\n","    \"943\": 184,\n","    \"944\": -1,\n","    \"945\": 185,\n","    \"946\": -1,\n","    \"947\": 186,\n","    \"948\": -1,\n","    \"949\": -1,\n","    \"950\": -1,\n","    \"951\": 187,\n","    \"952\": -1,\n","    \"953\": -1,\n","    \"954\": 188,\n","    \"955\": -1,\n","    \"956\": 189,\n","    \"957\": 190,\n","    \"958\": -1,\n","    \"959\": 191,\n","    \"960\": -1,\n","    \"961\": -1,\n","    \"962\": -1,\n","    \"963\": -1,\n","    \"964\": -1,\n","    \"965\": -1,\n","    \"966\": -1,\n","    \"967\": -1,\n","    \"968\": -1,\n","    \"969\": -1,\n","    \"970\": -1,\n","    \"971\": 192,\n","    \"972\": 193,\n","    \"973\": -1,\n","    \"974\": -1,\n","    \"975\": -1,\n","    \"976\": -1,\n","    \"977\": -1,\n","    \"978\": -1,\n","    \"979\": -1,\n","    \"980\": 194,\n","    \"981\": 195,\n","    \"982\": -1,\n","    \"983\": -1,\n","    \"984\": 196,\n","    \"985\": -1,\n","    \"986\": 197,\n","    \"987\": 198,\n","    \"988\": 199,\n","    \"989\": -1,\n","    \"990\": -1,\n","    \"991\": -1,\n","    \"992\": -1,\n","    \"993\": -1,\n","    \"994\": -1,\n","    \"995\": -1,\n","    \"996\": -1,\n","    \"997\": -1,\n","    \"998\": -1,\n","    \"999\": -1\n","}"]},{"cell_type":"markdown","metadata":{"id":"qLwYsF2LtEI6"},"source":["### **Recover dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7TDRMeMbWXmt"},"outputs":[],"source":["# Class that interacts with images stored in an Amazon S3 bucket.\n","# It allows to load and preprocess images on-the-fly during training or inference.\n","class S3ImageFolder(Dataset):\n","    def __init__(self, root, transform=None):\n","        self.s3_bucket = \"deeplearning2024-datasets\" # name of the bucket\n","        self.s3_region = \"eu-west-1\" # Ireland\n","        self.s3_client = boto3.client(\"s3\", region_name=self.s3_region, verify=True)\n","        self.transform = transform\n","\n","        # Get list of objects in the bucket\n","        response = self.s3_client.list_objects_v2(Bucket=self.s3_bucket, Prefix=root)\n","        objects = response.get(\"Contents\", [])\n","        while response.get(\"NextContinuationToken\"):\n","            response = self.s3_client.list_objects_v2(\n","                Bucket=self.s3_bucket,\n","                Prefix=root,\n","                ContinuationToken=response[\"NextContinuationToken\"]\n","            )\n","            objects.extend(response.get(\"Contents\", []))\n","\n","        # Iterate and keep valid files only\n","        self.instances = []\n","        for ds_idx, item in enumerate(objects):\n","            key = item[\"Key\"]\n","            path = Path(key)\n","\n","            # Check if file is valid\n","            if path.suffix.lower() not in (\".jpg\", \".jpeg\", \".png\", \".ppm\", \".bmp\", \".pgm\", \".tif\", \".tiff\", \".webp\"):\n","                continue\n","\n","            # Get label\n","            label = path.parent.name\n","\n","            # Keep track of valid instances\n","            self.instances.append((label, key))\n","\n","        # Sort classes in alphabetical order (as in ImageFolder)\n","        self.classes = sorted(set(label for label, _ in self.instances))\n","        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n","\n","    def __len__(self):\n","        return len(self.instances)\n","\n","    def __getitem__(self, idx):\n","        try:\n","            label, key = self.instances[idx]\n","\n","            # Download image from S3\n","            # response = self.s3_client.get_object(Bucket=self.s3_bucket, Key=key)\n","            # img_bytes = response[\"Body\"]._raw_stream.data\n","\n","            img_bytes = BytesIO()\n","            response = self.s3_client.download_fileobj(Bucket=self.s3_bucket, Key=key, Fileobj=img_bytes) # download each image\n","            # img_bytes = response[\"Body\"]._raw_stream.data\n","\n","            # Open image with PIL\n","            img = Image.open(img_bytes).convert(\"RGB\")\n","\n","            # Apply transformations if any\n","            if self.transform is not None:\n","                img = self.transform(img)\n","        except Exception as e:\n","            raise RuntimeError(f\"Error loading image at index {idx}: {str(e)}\")\n","\n","        return img, self.class_to_idx[label]\n","\n","# Function to create DataLoaders for training and evaluating models.\n","# Loads the dataset from the S3 bucket and optionally splits it into training,\n","# validation, and test sets. It then returns PyTorch DataLoader objects for these datasets.\n","def get_data(batch_size, img_root, seed = None, split_data = False, transform = None):\n","\n","    # Load data\n","    data = S3ImageFolder(root=img_root, transform=transform)\n","\n","    if split_data:\n","        # Create train and test splits (80/20)\n","        num_samples = len(data)\n","        training_samples = int(num_samples * 0.8 + 1)\n","        val_samples = int(num_samples * 0.1)\n","        test_samples = num_samples - training_samples - val_samples\n","\n","        torch.manual_seed(seed)\n","        training_data, val_data, test_data = torch.utils.data.random_split(data, [training_samples, val_samples, test_samples])\n","\n","        # Initialize dataloaders\n","        train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True, num_workers=4)\n","        val_loader = torch.utils.data.DataLoader(val_data, batch_size, shuffle=False, num_workers=4)\n","        test_loader = torch.utils.data.DataLoader(test_data, batch_size, shuffle=False, num_workers=4)\n","\n","        return train_loader, val_loader, test_loader\n","\n","    data_loader = torch.utils.data.DataLoader(data, batch_size, shuffle=False, num_workers=4)\n","    return data_loader"]},{"cell_type":"markdown","metadata":{"id":"FXKWLOkhWn1N"},"source":["## **Test-time adaptation methods**\n","\n","During the testing phase, Test-Time Adaptation (TTA) methods allow for modifications to be made to the model. This enables the model to adapt to new data distributions, even if it has not encountered them before, thus maintaining a certain level of reliability.\n","\n","Below, we outline the techniques used to enhance the model's performance, providing a brief introduction to each method followed by its implementation."]},{"cell_type":"markdown","metadata":{"id":"hWtwo8X73663"},"source":["### **MEMO: Test Time Robustness via Adaptation and Augmentation<sup>[1]</sup>**\n","\n","In this paper, the authors propose a method called MEMO (Marginal Entropy Minimization with One Test Point) designed to address the problem of robustness in deep neural networks when confronted with distribution shifts or unexpected perturbations. To achieve this, the method employs both adaptation and augmentation strategies.\n","\n","<p align=\"center\">\n","  <img src=\"images/MEMO.png\" width=\"600\" height=\"300\">  \n","</p>\n","\n","MEMO applies data augmentations to a single test input to generate various versions of the input, and from these, it calculates the marginal output distribution. The model's parameters are then updated to minimize the entropy of this marginal distribution. Finally, the model uses these updated parameters to make a prediction on the original test input.\n","\n","<br>\n","\n","---\n","**Algorithm 1** Test time robustness via MEMO\n","\n","---\n","\n","**Require:** trained model f<sub>θ</sub>, test point $x$, number of augmentations $B$, learning rate $η$, update rule $G$\n","\n","1. Sample a<sub>1</sub>...a<sub>B</sub> $\\overset{\\text{i.i.d.}}{\\sim}$ $\\mathcal{U}$ ($\\mathcal{A}$) and produce augmented points $\\tilde{\\mathbf{x}}_i = a_i(\\mathbf{x})$ for $i \\in \\{1, \\ldots, B\\}$\n","2. Compute estimate $\\tilde{p} = \\frac{1}{B} \\sum_{i=1}^B p_0(y|\\tilde{\\mathbf{x}}_i) \\approx p_0(y|\\mathbf{x})$ and $\\tilde{\\ell} = H(\\tilde{p}) \\approx \\ell(\\theta; \\mathbf{x})$\n","3. Adapt parameters via update rule $\\theta' \\leftarrow G(\\theta, \\eta, \\tilde{\\ell})$\n","4. Predict $\\hat{y} \\triangleq \\arg \\max_y p_{\\theta'}(y|\\mathbf{x})$\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1VxxoHWV84I4"},"outputs":[],"source":["def compute_entropy(probabilities):\n","    \"\"\"\n","    Takes a tensor of probabilities [1,Classes] and computes the entropy returned as one-dimensional tensor.\n","    \"\"\"\n","    # Ensure probabilities are normalized (sum to 1)\n","    if not torch.isclose(probabilities.sum(), torch.tensor(1.0)):\n","        raise ValueError(\"The probabilities should sum to 1.\")\n","\n","    # Compute entropy\n","    # Adding a small value to avoid log(0) issues\n","    epsilon = 1e-10\n","    probabilities = torch.clamp(probabilities, min=epsilon)\n","    entropy = -torch.sum(probabilities * torch.log(probabilities))\n","\n","    return entropy\n","\n","def get_best_augmentations(probabilities, top_k):\n","    \"\"\"\n","    Takes a tensor of probabilities with dimension [num_augmentations,classes] or [mc_models,num_augmentations,200]\n","    and outputs a tensor containing the probabilities corresponding to the augmentations\n","    with the lowest entropy of dimension [top_k, classes] or [mc_models, top_k, classes].\n","    ----------\n","    top_k: number of augmentations to select\n","    probabilities: a tensor of dimension [num_augmentations,200]\n","    \"\"\"\n","    if probabilities.dim() == 2:\n","        probabilities = probabilities.unsqueeze(0)\n","\n","    # nested list comprehension needed if probabilities is a 3D tensor (MC dropout)\n","    entropies = torch.tensor([[compute_entropy(prob) for prob in prob_set] for prob_set in probabilities])\n","    _, top_k_indices = torch.topk(entropies, top_k, largest=False, sorted=False)\n","    sorted_top_k_indices = torch.stack([indices[torch.argsort(entropies[i, indices])]\n","                                            for i, indices in enumerate(top_k_indices)])\n","    top_k_probabilities = torch.stack([probabilities[i][sorted_top_k_indices[i]]\n","                                        for i in range(probabilities.shape[0])])\n","    if top_k_probabilities.shape[0] == 1:\n","        top_k_probabilities = top_k_probabilities.squeeze(0)\n","\n","    return top_k_probabilities\n","\n","def get_test_augmentations(input, augmentations, num_augmentations, seed_augmentations):\n","    \"\"\"\n","    Takes a tensor image of dimension [C,H,W] and returns a tensor of augmentations of dimension [num_augmentations, C,H,W].\n","    The augmentations are produced by sampling different torchvision.transforms from \"augmentations\".\n","    ----------\n","    input: an image tensor of dimension [C,H,W]\n","    augmentations: a list of torchvision.transforms augmentations\n","    num_augmentations: the number of augmentations to produce\n","    seed_augmentations: seed to reproduce the sampling of augmentations\n","    \"\"\"\n","    torch.manual_seed(seed_augmentations)\n","    random.seed(seed_augmentations)\n","    sampled_augmentations = random.sample(augmentations, num_augmentations)\n","    test_augmentations = torch.zeros((num_augmentations, 3, 224, 224))\n","    for i, augmentation in enumerate(sampled_augmentations):\n","        transform_MEMO = T.Compose([\n","            T.ToPILImage(),\n","            augmentation,\n","            T.ToTensor(),\n","            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","        ])\n","        augmented_input = transform_MEMO(input.cpu())\n","        test_augmentations[i] = augmented_input\n","    return test_augmentations"]},{"cell_type":"markdown","metadata":{},"source":["### TTA: Test Time Augmentations\n","\n","Test Time Augmentation (TTA) is a technique used to improve the robustness and accuracy of a model's predictions during inference by applying multiple augmentations to the input data. The main idea is to simulate different variations of the test input, make predictions on each of these variations, and then combine these predictions to make a final decision. The predictions are usually combined by averaging and then the `argmax` function is used as usual."]},{"cell_type":"markdown","metadata":{"id":"1HVxP1v04Bhm"},"source":["### **Adaptive Batch Normalization - Improving robustness against common corruptions by covariate shift adaptation<sup>[3]<sup>**\n","\n","In this paper, the authors investigate methods to improve the robustness of machine learning models trained for computer vision against common image corruptions, such as blurring or compression artifacts. These corruptions often degrade model performance, reducing their effectiveness in real-world applications.\n","\n","Traditional approaches tend to underestimate model robustness in scenarios where models can adapt to corruptions found in multiple unlabeled examples. The authors argue that models should utilize these examples for unsupervised online adaptation, a strategy not commonly employed in current evaluations. Instead of relying on static batch normalization (BN) statistics computed during training, the authors propose that these statistics should be dynamically updated by the models using data from corrupted images encountered during testing. This adaptive approach can significantly enhance model performance under real-world conditions. The formula to compute the update is the following:\n","\n","<div align=\"center\">\n","\n","$\\mathcal{v} \\in \\{\\mu, \\sigma^2 \\}$ \\\n","$\\mathcal{v} = \\frac{N}{N+1}\\mathcal{v}_{\\text{train}} + \\frac{1}{N+1}\\mathcal{v}_{\\text{test}} $\n","\n","</div>\n","\n","Following the paper's suggestion we set `N` to 16."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NllCSGPu7X-P"},"outputs":[],"source":["def adaptive_bn_forward(self, input: torch.Tensor):\n","    \"\"\"\n","    Applies an adaptive batch normalization to the input tensor using precomputed running\n","    statistics that are updated in an adaptive manner using Schneider et al. [40] formula:\n","                        mean = N/(N+1)*mean_train + 1/(N+1)*mean_test\n","                        var = N/(N+1)*var_train + 1/(N+1)*var_test\n","    N corresponds to the weight that is given to the statistics of the pre-trained model.\n","    In the implementation, N/(N+1) corresponds to self.prior_strength and can be modified assigning\n","    a float/int to nn.BatchNorm2d.prior_strength.\n","    -----------\n","    input : input tensor of shape [N, C, H, W]\n","    \"\"\"\n","    # compute channel-wise statistics for the input\n","    point_mean = input.mean([0,2,3]).to(device = self.running_mean.device)\n","    point_var = input.var([0,2,3], unbiased=True).to(device = self.running_mean.device)\n","    # BN adaptation\n","    adapted_running_mean = self.prior_strength * self.running_mean + (1 - self.prior_strength) * point_mean\n","    adapted_running_var = self.prior_strength * self.running_var + (1 - self.prior_strength) * point_var\n","    # detach to avoid non-differentiable torch error\n","    adapted_running_mean = adapted_running_mean.detach()\n","    adapted_running_var = adapted_running_var.detach()\n","\n","    return torch.nn.functional.batch_norm(input, adapted_running_mean, adapted_running_var, self.weight, self.bias, False, 0, self.eps)"]},{"cell_type":"markdown","metadata":{"id":"Hlq8QGqJ6byL"},"source":["### **Monte Carlo Dropout - Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning<sup>[4]</sup>**\n","\n","In this paper, the authors propose Monte Carlo Dropout, a technique that leverages dropout, a regularization method commonly used during training, to also perform approximate Bayesian inference during the testing phase.\n","\n","During the model training phase, the dropout technique randomly \"drops\" or deactivates a fraction of neurons in the network during each forward pass. The probability of dropping neurons can be controlled using a specific parameter. The aim is to prevent neurons from memorizing specific inputs, thus reducing overfitting and encouraging the network to learn more general representations.\n","\n","However, during the model test phase, dropout is usually turned off, allowing the full network to make predictions. The key idea behind Monte Carlo Dropout is to keep dropout active during the test phase and perform multiple forward passes through the network. The result is that for each forward pass, a different dropout mask is used, which means a random subset of neurons is activated, allowing for different predictions. By averaging these predictions, it is possible to obtain both the final prediction and a measure of the model's uncertainty."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"diLFBJgTXA8d"},"outputs":[],"source":["class ResNet50Dropout(nn.Module):\n","    \"\"\"\n","    It creates a version of the ResNet-50 model that integrates dropout layers at\n","    various points in the architecture. By using dropout, the model can be trained\n","    with a regularization technique that allows for the implementation of\n","    Monte Carlo Dropout.\n","    ----------\n","    weights: Optional pre-trained weights for the ResNet-50 model.\n","    dropout_rate: The probability of dropping out neurons during training.\n","    A value of 0 means no dropout is applied and the architecture is identical to the\n","    original ResNet-50.\n","    \"\"\"\n","    def __init__(self, weights=None, dropout_rate=0., dropout_positions=None):\n","        super(ResNet50Dropout, self).__init__()\n","\n","        self.weights = weights\n","        self.model = models.resnet50(weights=self.weights)\n","        self.dropout_rate = dropout_rate\n","\n","        self.dropout_positions = []\n","        if self.dropout_rate > 0:\n","            self.dropout_positions = self.get_dropout_positions() if dropout_positions == None else dropout_positions[\"dropout_positions\"]\n","\n","        self._add_dropout()\n","\n","    # This method reads a JSON file that contains a list of layer names where\n","    # dropout should be applied.\n","    def get_dropout_positions(self):\n","        dropout_positions_path = \"INSERT PATH TO DROPOUT POSITIONS JSON FILE\"\n","        with open(dropout_positions_path, 'r') as json_file:\n","            dropout_positions = json.load(json_file)\n","        dropout_positions = dropout_positions[\"dropout_positions\"]\n","\n","        return dropout_positions\n","\n","    # This method adds dropout layers to the ResNet-50 model at the specified\n","    # positions, by looking at the dropout_positions list.\n","    # For each specified layer, the method wraps the original layer in\n","    # a nn.Sequential block, which includes the original layer followed by a\n","    # nn.Dropout layer with the specified dropout rate.\n","    def _add_dropout(self):\n","        if 'conv1' in self.dropout_positions:\n","            self.model.conv1 = nn.Sequential(\n","                self.model.conv1,\n","                nn.Dropout(p=self.dropout_rate)\n","            )\n","\n","        if 'layer1' in self.dropout_positions:\n","            self.model.layer1 = nn.Sequential(\n","                self.model.layer1,\n","                nn.Dropout(p=self.dropout_rate)\n","            )\n","\n","        if 'layer2' in self.dropout_positions:\n","            self.model.layer2 = nn.Sequential(\n","                self.model.layer2,\n","                nn.Dropout(p=self.dropout_rate)\n","            )\n","\n","        if 'layer3' in self.dropout_positions:\n","            self.model.layer3 = nn.Sequential(\n","                self.model.layer3,\n","                nn.Dropout(p=self.dropout_rate)\n","            )\n","\n","        if 'layer4' in self.dropout_positions:\n","            self.model.layer4 = nn.Sequential(\n","                self.model.layer4,\n","                nn.Dropout(p=self.dropout_rate)\n","            )\n","\n","        if 'avgpool' in self.dropout_positions:\n","            self.model.avgpool = nn.Sequential(\n","                self.model.avgpool,\n","                nn.Dropout(p=self.dropout_rate)\n","            )\n","\n","        if 'fc' in self.dropout_positions:\n","            self.model.fc = nn.Sequential(\n","                nn.Dropout(p=self.dropout_rate),\n","                self.model.fc\n","            )\n","\n","    def forward(self, x):\n","        return self.model(x)"]},{"cell_type":"markdown","metadata":{"id":"WrvcCQL716S2"},"source":["#### **Dropout positions**\n","\n","The **dropout_positions.json** file is to define the locations within a custom ResNet50 model where dropout layers should be inserted.\n","\n","The dropout layers are incorporated to enhance the proposed method using Monte Carlo Dropout, a technique that improves model robustness and uncertainty estimation. For more details, refer to the **ResNet50Dropout** section."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dropout_positions = {\n","    \"dropout_positions\": [\n","        \"conv1\",\n","        \"layer1\",\n","        \"layer2\",\n","        \"layer3\",\n","        \"layer4\",\n","        \"avgpool\",\n","        \"fc\"\n","    ]\n","}"]},{"cell_type":"markdown","metadata":{"id":"6X8HL-vDoMRf"},"source":["### **Efficient DiffTPT - Offline Data Augmentations with Diffusion and LLM**\n","\n","\n","Traditional data augmentation methods are limited by insufficient data diversity. We re-adapt the DiffTPT method proposed by the paper \n","\"DiffTPT - Diverse Data Augmentation with Diffusions for Effective Test-time Prompt Tuning<sup>[5]</sup>\" to use it with MEMO. \n","\n","<p align=\"center\">\n","  <img src=\"images/DiffTPT.png\" width=\"800\" height=\"300\">  \n","</p>\n","\n","DiffTPT relies on the CLIP image encoder to generate new images, which may limit the variability of the generated outputs. Additionally, generating images at test time slows down online inference and necessitates the creation of new images for each instance. To address these limitations, we propose a novel approach as follows:\n","\n","1. Query Definition: Formulate a query that includes the class name, image style, and other relevant details.\n","\n","2. Image Scraping: Using the query, scrape a small set of images from the internet for each class (e.g., 10 images per class). For domains with significant shifts or that are very abstract, incorporating these images aids in generating samples more aligned with the new data distribution, though it may reduce variability and may erronously bias the generation if the scraping is not performed appropriately.\n","\n","3. Prompt Generation: Using a similar query and a large language model (LLM), specifically \"llama3.1\" in our case, to generate a set of prompts for each class. The CLIP text embeddings for these prompts are then stored.\n","\n","4. Image Generation and Embedding Storage: Generate new images using stable diffusion based on the prompts and/or scraped images, and store their corresponding CLIP image embeddings.\n","\n","5. Cosine Similarity for Retrieval: At test time, use cosine similarity between the image embeddings of the previously generated images and/or the text embeddings to retrieve the most similar images. This method is computationally less expensive than generating new images for each test sample it should still enhance accuracy. For esample, while our number of generated images if fixed and does not depend on the number of samples to classify, the original method scales linearly with it. Which means that for the `Imagenet-A`, assuming 64 augmentations per sample, one needs to produce a total of 480,000 images, nearly 50 times the ones we used. Thus, not only our method is much more efficient and expensive, but it's also significantly faster assuming the same computational power. \n","\n","6. Data Augmentation: Augmented data is incorporated using both conventional methods and pre-trained stable diffusion models, albeit with varying percentages.\n","\n","We generated 30 images using the `t2i` pipeline and 25 images using the `iti` pipeline (due to lack of time), resulting in a total of 55 images per class. The entire dataset comprises approximately 11,000 images, generated over approximately 15 hours (17 hours including the generation of 20 prompts per class), excluding the time taken for scraping, which was minimal. So generating each image took nearly (4 seconds for Stable Diffusion) 5.5 seconds. This time investment is a fixed cost, and once a sufficiently diverse set of images is generated, further use of generative models is unnecessary. Unlike DiffTPT, which requires generating new images for each sample and discards a substantial portion of the outputs, our approach generates images in a more controlled manner, reducing the need for such discarding. \n","\n","Finally, for each class, we selected the `top-k` images using the cosine similarity filter, followed by an additional filter that imposes a minimum similarity threshold to mitigate the risk of including images from other classes. This means that the final number of generated augmentations used is not `top-k`, but a number between 0 and `top-k`.\n","\n","<p align=\"center\">\n","  <img src=\"images/image_generation_pipeline_colored.png\" width=\"600\" height=\"300\">  \n","</p>"]},{"cell_type":"markdown","metadata":{"id":"cb0esIOp9jQb"},"source":["#### **Image generation**\n","\n","To facilitate reproducibility we made the generated dataset available at the following Google Drive link: https://drive.google.com/file/d/1DIbKDdsGZD4pUizaL_rI_XTAS5ZmgU-v/view?usp=sharing"]},{"cell_type":"markdown","metadata":{},"source":["#### **ImageNet-A classes**\n","\n","The **imagenetA_classes.json** file provides a mapping between the synset IDs used in the ImageNet dataset and their corresponding class names. This mapping is essential for converting model outputs from synset IDs to human-readable labels.\n","\n","The class IDs are mainly used to create the directory structure where newly generated images will be stored, forming a secondary dataset. This dataset can then be utilized for training and inference activities. For more details, refer to the **ToDo** section."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["imagenetA_classes = {\n","\t\"n01498041\": \"stingray\",\n","\t\"n01531178\": \"goldfinch\",\n","\t\"n01534433\": \"junco\",\n","\t\"n01558993\": \"American robin\",\n","\t\"n01580077\": \"jay\",\n","\t\"n01614925\": \"bald eagle\",\n","\t\"n01616318\": \"vulture\",\n","\t\"n01631663\": \"newt\",\n","\t\"n01641577\": \"American bullfrog\",\n","\t\"n01669191\": \"box turtle\",\n","\t\"n01677366\": \"green iguana\",\n","\t\"n01687978\": \"agama\",\n","\t\"n01694178\": \"chameleon\",\n","\t\"n01698640\": \"American alligator\",\n","\t\"n01735189\": \"garter snake\",\n","\t\"n01770081\": \"harvestman\",\n","\t\"n01770393\": \"scorpion\",\n","\t\"n01774750\": \"tarantula\",\n","\t\"n01784675\": \"centipede\",\n","\t\"n01819313\": \"sulphur-crested cockatoo\",\n","\t\"n01820546\": \"lorikeet\",\n","\t\"n01833805\": \"hummingbird\",\n","\t\"n01843383\": \"toucan\",\n","\t\"n01847000\": \"duck\",\n","\t\"n01855672\": \"goose\",\n","\t\"n01882714\": \"koala\",\n","\t\"n01910747\": \"jellyfish\",\n","\t\"n01914609\": \"sea anemone\",\n","\t\"n01924916\": \"flatworm\",\n","\t\"n01944390\": \"snail\",\n","\t\"n01985128\": \"crayfish\",\n","\t\"n01986214\": \"hermit crab\",\n","\t\"n02007558\": \"flamingo\",\n","\t\"n02009912\": \"great egret\",\n","\t\"n02037110\": \"oystercatcher\",\n","\t\"n02051845\": \"pelican\",\n","\t\"n02077923\": \"sea lion\",\n","\t\"n02085620\": \"Chihuahua\",\n","\t\"n02099601\": \"Golden Retriever\",\n","\t\"n02106550\": \"Rottweiler\",\n","\t\"n02106662\": \"German Shepherd Dog\",\n","\t\"n02110958\": \"pug\",\n","\t\"n02119022\": \"red fox\",\n","\t\"n02123394\": \"Persian cat\",\n","\t\"n02127052\": \"lynx\",\n","\t\"n02129165\": \"lion\",\n","\t\"n02133161\": \"American black bear\",\n","\t\"n02137549\": \"mongoose\",\n","\t\"n02165456\": \"ladybug\",\n","\t\"n02174001\": \"rhinoceros beetle\",\n","\t\"n02177972\": \"weevil\",\n","\t\"n02190166\": \"fly\",\n","\t\"n02206856\": \"bee\",\n","\t\"n02219486\": \"ant\",\n","\t\"n02226429\": \"grasshopper\",\n","\t\"n02231487\": \"stick insect\",\n","\t\"n02233338\": \"cockroach\",\n","\t\"n02236044\": \"mantis\",\n","\t\"n02259212\": \"leafhopper\",\n","\t\"n02268443\": \"dragonfly\",\n","\t\"n02279972\": \"monarch butterfly\",\n","\t\"n02280649\": \"small white\",\n","\t\"n02281787\": \"gossamer-winged butterfly\",\n","\t\"n02317335\": \"starfish\",\n","\t\"n02325366\": \"cottontail rabbit\",\n","\t\"n02346627\": \"porcupine\",\n","\t\"n02356798\": \"fox squirrel\",\n","\t\"n02361337\": \"marmot\",\n","\t\"n02410509\": \"bison\",\n","\t\"n02445715\": \"skunk\",\n","\t\"n02454379\": \"armadillo\",\n","\t\"n02486410\": \"baboon\",\n","\t\"n02492035\": \"white-headed capuchin\",\n","\t\"n02504458\": \"African bush elephant\",\n","\t\"n02655020\": \"pufferfish\",\n","\t\"n02669723\": \"academic gown\",\n","\t\"n02672831\": \"accordion\",\n","\t\"n02676566\": \"acoustic guitar\",\n","\t\"n02690373\": \"airliner\",\n","\t\"n02701002\": \"ambulance\",\n","\t\"n02730930\": \"apron\",\n","\t\"n02777292\": \"balance beam\",\n","\t\"n02782093\": \"balloon\",\n","\t\"n02787622\": \"banjo\",\n","\t\"n02793495\": \"barn\",\n","\t\"n02797295\": \"wheelbarrow\",\n","\t\"n02802426\": \"basketball\",\n","\t\"n02814860\": \"lighthouse\",\n","\t\"n02815834\": \"beaker\",\n","\t\"n02837789\": \"bikini\",\n","\t\"n02879718\": \"bow\",\n","\t\"n02883205\": \"bow tie\",\n","\t\"n02895154\": \"breastplate\",\n","\t\"n02906734\": \"broom\",\n","\t\"n02948072\": \"candle\",\n","\t\"n02951358\": \"canoe\",\n","\t\"n02980441\": \"castle\",\n","\t\"n02992211\": \"cello\",\n","\t\"n02999410\": \"chain\",\n","\t\"n03014705\": \"chest\",\n","\t\"n03026506\": \"Christmas stocking\",\n","\t\"n03124043\": \"cowboy boot\",\n","\t\"n03125729\": \"cradle\",\n","\t\"n03187595\": \"rotary dial telephone\",\n","\t\"n03196217\": \"digital clock\",\n","\t\"n03223299\": \"doormat\",\n","\t\"n03250847\": \"drumstick\",\n","\t\"n03255030\": \"dumbbell\",\n","\t\"n03291819\": \"envelope\",\n","\t\"n03325584\": \"feather boa\",\n","\t\"n03355925\": \"flagpole\",\n","\t\"n03384352\": \"forklift\",\n","\t\"n03388043\": \"fountain\",\n","\t\"n03417042\": \"garbage truck\",\n","\t\"n03443371\": \"goblet\",\n","\t\"n03444034\": \"go-kart\",\n","\t\"n03445924\": \"golf cart\",\n","\t\"n03452741\": \"grand piano\",\n","\t\"n03483316\": \"hair dryer\",\n","\t\"n03584829\": \"clothes iron\",\n","\t\"n03590841\": \"jack-o'-lantern\",\n","\t\"n03594945\": \"jeep\",\n","\t\"n03617480\": \"kimono\",\n","\t\"n03666591\": \"lighter\",\n","\t\"n03670208\": \"limousine\",\n","\t\"n03717622\": \"manhole cover\",\n","\t\"n03720891\": \"maraca\",\n","\t\"n03721384\": \"marimba\",\n","\t\"n03724870\": \"mask\",\n","\t\"n03775071\": \"mitten\",\n","\t\"n03788195\": \"mosque\",\n","\t\"n03804744\": \"nail\",\n","\t\"n03837869\": \"obelisk\",\n","\t\"n03840681\": \"ocarina\",\n","\t\"n03854065\": \"organ\",\n","\t\"n03888257\": \"parachute\",\n","\t\"n03891332\": \"parking meter\",\n","\t\"n03935335\": \"piggy bank\",\n","\t\"n03982430\": \"billiard table\",\n","\t\"n04019541\": \"hockey puck\",\n","\t\"n04033901\": \"quill\",\n","\t\"n04039381\": \"racket\",\n","\t\"n04067472\": \"reel\",\n","\t\"n04086273\": \"revolver\",\n","\t\"n04099969\": \"rocking chair\",\n","\t\"n04118538\": \"rugby ball\",\n","\t\"n04131690\": \"salt shaker\",\n","\t\"n04133789\": \"sandal\",\n","\t\"n04141076\": \"saxophone\",\n","\t\"n04146614\": \"school bus\",\n","\t\"n04147183\": \"schooner\",\n","\t\"n04179913\": \"sewing machine\",\n","\t\"n04208210\": \"shovel\",\n","\t\"n04235860\": \"sleeping bag\",\n","\t\"n04252077\": \"snowmobile\",\n","\t\"n04252225\": \"snowplow\",\n","\t\"n04254120\": \"soap dispenser\",\n","\t\"n04270147\": \"spatula\",\n","\t\"n04275548\": \"spider web\",\n","\t\"n04310018\": \"steam locomotive\",\n","\t\"n04317175\": \"stethoscope\",\n","\t\"n04344873\": \"couch\",\n","\t\"n04347754\": \"submarine\",\n","\t\"n04355338\": \"sundial\",\n","\t\"n04366367\": \"suspension bridge\",\n","\t\"n04376876\": \"syringe\",\n","\t\"n04389033\": \"tank\",\n","\t\"n04399382\": \"teddy bear\",\n","\t\"n04442312\": \"toaster\",\n","\t\"n04456115\": \"torch\",\n","\t\"n04482393\": \"tricycle\",\n","\t\"n04507155\": \"umbrella\",\n","\t\"n04509417\": \"unicycle\",\n","\t\"n04532670\": \"viaduct\",\n","\t\"n04540053\": \"volleyball\",\n","\t\"n04554684\": \"washing machine\",\n","\t\"n04562935\": \"water tower\",\n","\t\"n04591713\": \"wine bottle\",\n","\t\"n04606251\": \"shipwreck\",\n","\t\"n07583066\": \"guacamole\",\n","\t\"n07695742\": \"pretzel\",\n","\t\"n07697313\": \"cheeseburger\",\n","\t\"n07697537\": \"hot dog\",\n","\t\"n07714990\": \"broccoli\",\n","\t\"n07718472\": \"cucumber\",\n","\t\"n07720875\": \"bell pepper\",\n","\t\"n07734744\": \"mushroom\",\n","\t\"n07749582\": \"lemon\",\n","\t\"n07753592\": \"banana\",\n","\t\"n07760859\": \"custard apple\",\n","\t\"n07768694\": \"pomegranate\",\n","\t\"n07831146\": \"carbonara\",\n","\t\"n09229709\": \"bubble\",\n","\t\"n09246464\": \"cliff\",\n","\t\"n09472597\": \"volcano\",\n","\t\"n09835506\": \"baseball player\",\n","\t\"n11879895\": \"rapeseed\",\n","\t\"n12057211\": \"yellow lady's slipper\",\n","\t\"n12144580\": \"corn\",\n","\t\"n12267677\": \"acorn\"\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9EbufrbhXD1H"},"outputs":[],"source":["def get_imagenetA_classes(imagenetA_classes : Union[str, Dict[str, str]] = None):\n","    \"\"\"\n","    ImageNet-A uses the same label structure as the original ImageNet (ImageNet-1K).\n","    Each class in ImageNet is represented by a synset ID (e.g., n01440764 for \"tench, Tinca tinca\").\n","    This function returns a dictionary that maps the synset IDs of ImageNet-A to the corresponding class names.\n","    ----------\n","    indices_in_1k: list of indices to map [B,1000] -> [B,200]\n","    \"\"\"\n","    if isinstance(imagenetA_classes,str):\n","        with open(imagenetA_classes, 'r') as json_file:\n","            imagenetA_classes_dict = json.load(json_file)\n","    else:\n","        imagenetA_classes_dict = imagenetA_classes\n","\n","    # ensure `class_dict` is a dictionary with keys as class IDs and values as class names\n","    class_dict = {k: v for k, v in imagenetA_classes_dict.items()}\n","    return class_dict\n","\n","def create_dir_generated_images(path:str, imageneA_classes: Union[str, Dict[str, str]] = None):\n","    \"\"\"\n","    Create directory where to store generated images.\n","    ---\n","    path (str): path where to create the directories\n","    \"\"\"\n","    classes = list(get_imagenetA_classes(imageneA_classes).values())\n","    for class_name in classes:\n","        class_path = os.path.join(path, class_name)\n","        os.makedirs(class_path, exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ithq1PDCXO6R"},"outputs":[],"source":["class ImageGenerator:\n","    \"\"\"\n","    A class containing all the functions to generate and save prompts, images and their respective\n","    CLIP embeddings.\n","    \"\"\"\n","    def __init__(self):\n","        pass \n","\n","    def get_text_embedding(self,clip_model, text: str):\n","        \"\"\"\n","        \n","        \"\"\"\n","        text_token = clip.tokenize(text).cuda()\n","        text_embedding = clip_model.encode_text(text_token).float()\n","        text_embedding /= text_embedding.norm()\n","        return text_embedding \n","        \n","    def generate_prompts(self, \n","                         num_prompts_per_class: int, \n","                         style_of_picture: str, \n","                         path: str, \n","                         context_llm: Union[str, Dict], \n","                         llm_model: str = \"llama3.1\", \n","                         clip_text_encoder: str = \"ViT-L/14\"\n","                        ) -> None:\n","        \"\"\"\n","        Generate image prompts for each class in the given directory path using a language model available in ollama library.\n","        The prompts will then be used to generate images using a diffusion model.\n","        ---\n","        num_prompts_per_class (int): Number of prompts to generate per class. This number doesn't correspond to the actual number \n","                                     of prompts that will be generated, but rather to the desired total number of prompts for each \n","                                     class e.g. if the class \"ant\" has already 10 prompts and num_prompts_per_class = 12, then only\n","                                     two prompts will be generated.\n","        style_of_picture (str): Style to be used in image prompts.\n","        path (str): Path to the generated dataset.\n","        context_llm (str, dict): A dict containing the context for the language model or a path to a json file containing it.\n","        llm_model (str): The language model to use for generating prompts.\n","        clip_text_encoder (str): The CLIP text encoder model to use. \n","        ---\n","        Returns:\n","            list: A list of class names for which prompts could not be generated due to bad prompts formatting.\n","        \"\"\"\n","        assert isinstance(llm_model,str), \"Model must be a str\"\n","        assert isinstance(context_llm, (str,list)), \"context_llm must be a str path or a list of dict\"\n","        assert isinstance(clip_text_encoder, str), \"clip_text_encoder must be a str. Use clip.available_models() to get valid strings.\"\n","        assert isinstance(style_of_picture, str), \"style_of_picture must be a str representing the style of the image that will be generated\"\n","        assert isinstance(num_prompts_per_class, int), \"num_prompts_per_class must be an int\"\n","\n","        # check that llm_model is available\n","        try:\n","          ollama.chat(llm_model)\n","        except ollama.ResponseError as e:\n","          print('Error:', e.error)\n","          if e.status_code == 404:\n","            # try to pull the model if it exists\n","            print(\"Pulling the model...\")\n","            ollama.pull(llm_model)\n","\n","        # load context_llm dict if needed\n","        if isinstance(context_llm,str):\n","            with open(context_llm, 'r') as file:\n","                context_llm = json.load(file) \n","\n","        skipped_classes = []\n","\n","        clip_model, _ = clip.load(clip_text_encoder)\n","        clip_model.cuda().eval()\n","\n","        class_list = os.listdir(path)\n","        with torch.no_grad():\n","            with tqdm(total=len(class_list), desc=\"Processing classes\") as pbar:\n","                for class_name in class_list:\n","                    pbar.set_description(f\"Processing class: {class_name}\")\n","                    sub_dir_class = os.path.join(path, class_name)\n","                    prompts_to_generate = num_prompts_per_class - len(os.listdir(sub_dir_class)) - 1 # -1 to account for scraped_img folder\n","                    if prompts_to_generate <= 0: \n","                        pbar.update(1)\n","                        continue\n","                    # sometimes the language model doesn't return an appropriate output, tolerance = number of possible attempts\n","                    tolerance = 6\n","                    gen_prompts = []\n","                    original_prompts_to_gen = prompts_to_generate\n","                    while tolerance>0:\n","                        prompts_generation_instruction = {\n","                            \"role\": \"user\",\n","                            \"content\": f\"class:{class_name}, number of prompts:{prompts_to_generate}, style of picture: {style_of_picture}\"\n","                        }\n","                        if len(context_llm) == 3:\n","                            context_llm.append(prompts_generation_instruction)\n","                        else:\n","                            # needed from the second iteration\n","                            context_llm[3] = prompts_generation_instruction\n","                        try:\n","                            response = ollama.chat(model=llm_model, messages=context_llm)\n","                            content = json.loads(response['message']['content'])  # json.loads to convert str to list\n","                            if len(content) > prompts_to_generate:\n","                                # enough or more than enough prompts generated\n","                                prompts_to_generate -= len(content)\n","                                gen_prompts.extend(content)\n","                                gen_prompts = gen_prompts[:original_prompts_to_gen]\n","                                tolerance = -1\n","                            else:\n","                                # more prompts neeeded\n","                                prompts_to_generate -= len(content)\n","                                gen_prompts.extend(content)\n","                        except Exception as e:\n","                            tolerance -= 1\n","    \n","                    if len(gen_prompts) != 1:\n","                        counter_flag = -1 \n","    \n","                    if tolerance == -1:\n","                        num_prompts_already_gen = len(os.listdir(sub_dir_class))\n","                        for i in range(num_prompts_already_gen, num_prompts_already_gen + len(gen_prompts)):\n","                            new_sub_dir = os.path.join(path, class_name, str(i))\n","                            os.makedirs(new_sub_dir, exist_ok=True)\n","                            prompt = gen_prompts[i - num_prompts_already_gen]\n","                            prompt_embedding = self.get_text_embedding(clip_model, prompt) # compute text CLIP embedding\n","                            with open(os.path.join(new_sub_dir, \"prompt.txt\"), 'w') as file:\n","                                file.write(prompt)\n","                            torch.save(prompt_embedding, os.path.join(new_sub_dir,\"prompt_clip_embedding.pt\"))\n","                    else:\n","                        # not even one prompt was generated, class entirely skipped\n","                        skipped_classes.append(class_name)\n","                        print(f\"Skipping class {class_name}.\")\n","\n","        return skipped_classes\n","    \n","    def get_image_embedding(self,clip_model, preprocess, image):\n","        image_preprocessed = preprocess(image).unsqueeze(0).cuda()\n","        image_embedding = clip_model.encode_image(image_preprocessed)\n","        image_embedding /= image_embedding.norm()\n","        return image_embedding\n","        \n","    def generate_images(self, \n","                        path: str, \n","                        num_images_per_class: int,  \n","                        image_generation_pipeline: Union[StableDiffusionPipeline, StableDiffusionImg2ImgPipeline], \n","                        num_inference_steps: int, \n","                        class_to_skip: List[str] = [],\n","                        guidance_scale: int = 9, \n","                        strength: float = 0.8, \n","                        clip_image_encoder: str = \"ViT-L/14\"\n","                       ) -> None:\n","        \"\"\"\n","        Generate images for each class in the specified directory using the given image generation pipeline.\n","        ---\n","        path (str): Path to the directory containing class subdirectories.\n","        num_images_per_class (int): Number of images to generate per class. \n","        class_to_skip (List[str]): List of classes to skip.\n","        image_generation_pipeline (Union[StableDiffusionPipeline, StableDiffusionImg2ImgPipeline]): The image generation pipeline to use.\n","        num_inference_steps (int): Number of inference steps for image generation.\n","        guidance_scale (int): Guidance scale for image generation. A higher guidance scale value encourages the model to generate \n","                                        images closely linked to the text prompt at the expense of lower image quality\n","        strength (float): Indicates extent to transform the reference scraped image. A value of 1 essentially ignores image. \n","        clip_image_encoder (str, optional): The CLIP image encoder model to use.\n","        ---\n","        Returns:\n","            None: This function does not return any value.\n","        \"\"\"\n","        assert isinstance(image_generation_pipeline, (StableDiffusionPipeline, StableDiffusionImg2ImgPipeline)), \\\n","            \"image_generation_pipeline must be one of StableDiffusionPipeline or StableDiffusionImg2ImgPipeline\"\n","        assert isinstance(clip_image_encoder, str), \"clip_image_encoder must be a str. Use clip.available_models() to get valid strings.\"\n","        assert isinstance(num_images_per_class, int), \"num_images_per_class must be an int\"\n","        \n","        random.seed(42)\n","\n","        print(\"Loading CLIP model...\")\n","        clip_model, preprocess = clip.load(clip_image_encoder)\n","        clip_model.cuda().eval()\n","\n","        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n","        class_list = os.listdir(path)\n","        with torch.no_grad():\n","            with tqdm(total=len(class_list), desc=\"Processing classes\") as pbar:\n","                for class_name in class_list:\n","                    pbar.set_description(f\"Processing class: {class_name}\")\n","                    if class_name in class_to_skip: \n","                        pbar.update(1)\n","                        continue\n","                    class_path = os.path.join(path, class_name)\n","                    num_prompts = len(os.listdir(class_path))\n","                    \n","                    if image_generation_pipeline.__class__.__name__ == \"StableDiffusionImg2ImgPipeline\":\n","                        print(\"Loading scraped images...\")\n","                        scraped_image_paths = os.path.join(class_path, \"scraped_images\") # go in scraped_images folder\n","                        scraped_images = []\n","                        for scraped_image_path in os.listdir(scraped_image_paths): # open and append scraped images\n","                            if scraped_image_path in (\".ipynb_checkpoints\"): continue\n","                            img_path = os.path.join(scraped_image_paths,scraped_image_path)\n","                            scraped_image = Image.open(img_path)\n","                            scraped_image = scraped_image.resize((512,512))\n","                            scraped_images.append(scraped_image)\n","                    \n","                    num_gen_images = 0 # needed if num_images < num_prompts\n","    \n","                    while num_gen_images < num_images_per_class:\n","                        for gen_images_class in os.listdir(class_path):\n","                            if gen_images_class in (\".ipynb_checkpoints\",\"scraped_images\"): continue\n","                            gen_image_class = os.path.join(class_path,gen_images_class)\n","                            # needed bc some folders don't have a prompt.txt due to some error during generation\n","                            try:\n","                                with open(os.path.join(gen_image_class, \"prompt.txt\"), 'r') as file:\n","                                    text_prompt = file.read()\n","                            except:\n","                                continue\n","                                \n","                            # get scraped images using the image-to-image generation pipeline\n","                            if isinstance(image_generation_pipeline, StableDiffusionImg2ImgPipeline):\n","                                try: # sometimes get weird OOM error\n","                                    i2i_image_path = os.path.join(gen_image_class,\"i2i_gen_images\")\n","                                    os.makedirs(i2i_image_path,exist_ok=True)\n","                                    scraped_image = random.sample(scraped_images,1)[0]\n","                                    with torch.no_grad():\n","                                        gen_image = image_generation_pipeline(prompt=text_prompt,\n","                                                                                image=scraped_image,\n","                                                                                strength=strength,\n","                                                                                guidance_scale=guidance_scale,\n","                                                                                num_inference_steps=num_inference_steps).images[0]\n","                                        del scraped_image\n","                                        gen_image_embedding = self.get_image_embedding(clip_model, preprocess, gen_image)\n","                                        save_gen_image_path = os.path.join(i2i_image_path,str(len(os.listdir(i2i_image_path))))\n","                                        os.makedirs(save_gen_image_path)\n","                                        torch.save(gen_image_embedding, os.path.join(save_gen_image_path, \"image_embedding.pt\"))\n","                                        gen_image.save(os.path.join(save_gen_image_path, \"image.png\"))\n","                                        num_gen_images += 1\n","                                except:\n","                                    print(\"Error occurred\")\n","                            else:\n","                                t2i_image_path = os.path.join(gen_image_class,\"t2i_gen_images\")\n","                                os.makedirs(t2i_image_path,exist_ok=True)\n","                                with torch.no_grad():\n","                                    gen_image = image_generation_pipeline(prompt=text_prompt,\n","                                                                            strength=strength,\n","                                                                            guidance_scale=guidance_scale,\n","                                                                            num_inference_steps=num_inference_steps).images[0]\n","                                    gen_image_embedding = self.get_image_embedding(clip_model, preprocess, gen_image)\n","                                    save_gen_image_path = os.path.join(t2i_image_path,str(len(os.listdir(t2i_image_path))))\n","                                    os.makedirs(save_gen_image_path)\n","                                    torch.save(gen_image_embedding, os.path.join(save_gen_image_path, \"image_embedding.pt\"))\n","                                    gen_image.save(os.path.join(save_gen_image_path, \"image.png\"))                        \n","                                    num_gen_images += 1\n","                            # break loop over the class prompts if generated enough images\n","                            if num_gen_images == num_images_per_class: break \n","                    pbar.update(1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def retrieve_gen_images(img: Union[torch.Tensor, Image.Image],  \n","                        num_images: int,\n","                        clip_model: clip.model.CLIP, \n","                        clip_preprocess: torchvision.transforms.transforms.Compose,\n","                        img_to_tensor_pipe: torchvision.transforms.transforms.Compose = None,\n","                        data_path: str = \"/home/sagemaker-user/Domain-Shift-Computer-Vision/imagenetA_generated\",\n","                        use_t2i_similarity: bool = False, \n","                        t2i_images: bool = True,\n","                        i2i_images: bool = False, \n","                        threshold: float = 0.0) -> Union[torch.Tensor, None]:\n","        \"\"\"\n","        Retrieve the most similar generated images based on CLIP embeddings.\n","        ---\n","        img (Union[torch.Tensor, Image.Image]): The input image to compare against. Can be a torch tensor or PIL image.\n","        num_images (int): The number of similar images to retrieve.\n","        clip_model: The preloaded CLIP model for generating embeddings.\n","        clip_preprocess: The preprocessing function for the CLIP model.\n","        img_to_tensor_pipe: A pipeline function that converts images to tensors.\n","        data_path (str): Path to the directory containing generated images.\n","        use_t2i_similarity (bool): Whether to average text-to-image similarity with image-to-image similarity.\n","        t2i_images (bool): Whether to include text-to-image generated images in the search.\n","        i2i_images (bool): Whether to include image-to-image generated images in the search.\n","        threshold (float): The minimum cosine similarity threshold for an image to be considered. \n","        ---\n","        Returns:\n","            Union[torch.Tensor, None]: A tensor containing the retrieved images. Returns None if no images are retrieved.\n","        \"\"\"\n","        assert i2i_images or t2i_images, \"One of t2i_images and i2i_images must be true\"\n","        assert isinstance(use_t2i_similarity, bool), \"use_t2i_similarity must be a bool\"\n","        assert isinstance(t2i_images, bool), \"t2i_images must be a bool\"\n","        assert isinstance(i2i_images, bool), \"i2i_images must be a bool\"\n","        assert isinstance(num_images, int), \"num_images must be an int\"\n","        assert isinstance(threshold, float) and 0 < threshold < 1, \"threshold must be a float and between 0 and 1\"\n","        \n","        if isinstance(img, torch.Tensor):\n","            img = T.ToPILImage()(img)\n","\n","        retrieved_images_paths = []\n","        retrieved_images_similarity = torch.zeros(num_images)\n","        with torch.no_grad():\n","            image_embedding = clip_model.encode_image(clip_preprocess(img).unsqueeze(0).cuda())\n","            image_embedding /= image_embedding.norm()\n","        \n","        for class_name in os.listdir(data_path):\n","            class_path = os.path.join(data_path, class_name)\n","            for gen_images_class in os.listdir(class_path):\n","                if gen_images_class in [\"scraped_images\", \".ipynb_checkpoints\"]: continue\n","                gen_images_class_path = os.path.join(class_path,gen_images_class)\n","                gen_prompt_embedding = torch.load(os.path.join(gen_images_class_path, \"prompt_clip_embedding.pt\"))\n","                t2i_similarity = F.cosine_similarity(image_embedding, gen_prompt_embedding)\n","                # Search in text-to-image generated images\n","                if t2i_images:\n","                    t2i_gen_images_main_path = os.path.join(gen_images_class_path,\"t2i_gen_images\")\n","                    for t2i_images_paths in os.listdir(t2i_gen_images_main_path):\n","                        t2i_image_path = os.path.join(t2i_gen_images_main_path,t2i_images_paths)\n","                        gen_image_embedding = torch.load(os.path.join(t2i_image_path, \"image_embedding.pt\"))\n","                        i2i_similarity = F.cosine_similarity(image_embedding, gen_image_embedding)\n","                        if use_t2i_similarity:\n","                            similarity = (i2i_similarity + t2i_similarity)/2 # avg similarity\n","                        else:\n","                            similarity = i2i_similarity\n","                        if similarity < threshold: continue\n","                        if len(retrieved_images_paths) < num_images:\n","                            retrieved_images_similarity[len(retrieved_images_paths)] = similarity\n","                            retrieved_images_paths.append(os.path.join(t2i_image_path, \"image.png\"))\n","                        else:\n","                            min_similarity, id_similarity = retrieved_images_similarity.min(dim=0)\n","                            if similarity > min_similarity:\n","                                retrieved_images_similarity[id_similarity] = similarity\n","                                retrieved_images_paths[id_similarity] = os.path.join(t2i_image_path, \"image.png\")\n","                # Search in image-to-image generated images\n","                if i2i_images:\n","                    i2i_gen_images_main_path = os.path.join(gen_images_class_path,\"i2i_gen_images\")\n","                    for i2i_images_paths in os.listdir(i2i_gen_images_main_path):\n","                        i2i_image_path = os.path.join(i2i_gen_images_main_path,i2i_images_paths)\n","                        gen_image_embedding = torch.load(os.path.join(i2i_image_path, \"image_embedding.pt\"))\n","        \n","                        i2i_similarity = F.cosine_similarity(image_embedding, gen_image_embedding)\n","                        if use_t2i_similarity:\n","                            similarity = (i2i_similarity + t2i_similarity)/2 # avg similarity\n","                        else:\n","                            similarity = i2i_similarity\n","                        if similarity < threshold: continue\n","                        if len(retrieved_images_paths) < num_images:\n","                            retrieved_images_similarity[len(retrieved_images_paths)] = similarity\n","                            retrieved_images_paths.append(os.path.join(t2i_image_path, \"image.png\"))\n","                        else:\n","                            min_similarity, id_similarity = retrieved_images_similarity.min(dim=0)\n","                            if similarity > min_similarity:\n","                                retrieved_images_similarity[id_similarity] = similarity\n","                                retrieved_images_paths[id_similarity] = os.path.join(i2i_image_path, \"image.png\")\n","        \n","        # Load and return the retrieved images as a tensor\n","        retrieved_images = []\n","        for image_path in retrieved_images_paths:\n","            # Apply image-to-tensor pipeline if provided\n","            if img_to_tensor_pipe:\n","                retrieved_images.append(img_to_tensor_pipe(Image.open(image_path)))\n","            else:\n","                retrieved_images.append(Image.open(image_path))\n","\n","        # Return tensor if image-to-tensor pipeline is provided, otherwise return list of PIL images\n","        if img_to_tensor_pipe:\n","            retrieved_images = torch.stack(retrieved_images) if len(retrieved_images) >= 1 else torch.tensor([])\n","        else:\n","            return retrieved_images\n","        return retrieved_images"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def scrape_images_imagenetA(img_style: str, imgenetA_gen_path: str, limit = 5):\n","    \"\"\"\n","    Scrape images for each imagenet-A class in the given directory path using the specified image style.\n","    Images are intially stored in a folder named as \"img_style + \\\" \\\" class_name\". Then the folder is renamed\n","    scraped_images.\n","    ---\n","    img_style (str): The style or keywords to use for image queries.\n","    imgenetA_gen_path (str): The path to the directory containing class subdirectories.\n","    limit (int, optional): The minimum number of images to scrape per class. Default is 5.\n","    \"\"\"\n","    class_list = os.listdir(imgenetA_gen_path) # get classes' name\n","    with tqdm(total=len(class_list), desc=\"Processing classes\") as pbar:\n","        for class_name in class_list:\n","            pbar.set_description(f\"Processing class: {class_name}\")\n","            class_path = os.path.join(imgenetA_gen_path, class_name)\n","            new_scraped_img_path = os.path.join(class_path, \"scraped_images\")\n","            # if for a class enough images have already been retrieved then skip it\n","            if os.path.exists(new_scraped_img_path):\n","                if len(os.listdir(new_scraped_img_path)) >= limit: \n","                    pbar.update(1)\n","                    continue \n","            query = img_style + \" \" + class_name\n","            downloader.download(query = query, \n","                                limit=limit, \n","                                output_dir=class_path, \n","                                adult_filter_off=True, \n","                                force_replace=False, \n","                                timeout=60,\n","                                verbose=False)\n","            current_scraped_img_path = os.path.join(class_path, query)\n","            os.rename(current_scraped_img_path, new_scraped_img_path)\n","            pbar.update(1)"]},{"cell_type":"markdown","metadata":{},"source":["#### **Install and run Ollama**\n","\n","To create the new images, we decided to use a StableDiffusion model to which we provided a list of prompts as input, one for each image to be generated. To generate the prompts, we used an LLM, specifically Llama 3.1. We were able to obtain this model through the Ollama library<sup>[6]</sup>, which provides the model and all the necessary configurations for its use. To get everything needed to download and run Ollama, and therefore Llama 3.1, simply execute the file **install_and_run_ollama.sh**. The script will download and install Ollama, start it, and instantiate Llama 3.1.\n","\n","```#!/bin/bash\n","\n","# Download and install Ollama\n","curl -fsSL https://ollama.com/install.sh | sh\n","\n","# Start the Ollama server in the background\n","ollama serve &\n","\n","# Pull the specified model\n","# To add more models: ollama pull [model_name]\n","# List of available models can be found at https://ollama.com/library \n","ollama pull llama3.1\n","\n","# Display the list of availavle model\n","# Used as a success message when the script completes\n","ollama list\n","\n","# Run the specified model\n","# Execute the following command ONLY if you want to run ollama from terminal\n","# ollama run llama3.1\n","```"]},{"cell_type":"markdown","metadata":{},"source":["#### **LLM context**\n","\n","The **llm_context.json** file is used to provide context for generating prompts for a text-to-image generator model.\n","\n","Each entry in the file specifies a Role and Content:\n","\n","* **Role**: Specifies who is speaking or interacting in the conversation. It can either be \"system\" (representing the LLM) or \"user\" (representing the person providing input to the LLM).\n","* **Content**: This field contains the actual message or instructions being communicated. It includes system instructions or user-provided input regarding the prompts to be generated.\n","\n","The messages can be of the following types:\n","1.    A message that sets the system's role and context, instructing the LLM on how to generate prompts for a text-to-image generation task.\n","2.    A message that serves as an example input a user might provide. It helps demonstrate how the user specifies the class name, number of prompts, and style of the picture.\n","3.    A message that provides an example output from the LLM, demonstrating the kind of response it should generate based on the provided input.\n","\n","In conclusion, the content of the file provides a framework that guides the LLM in understanding the context of the conversation, adhering to rules, and producing the desired output format."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ovsvQOCXUjx"},"outputs":[],"source":["imagenetA_generator = ImageGenerator()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["imgenetA_gen_path = \"INSERT THE PATH OF THE GENERATED DATASET\"\n","context_llm = [\n","    {\"role\": \"system\", \"content\": \"You are a system that generates reasonable and accurate prompts to be fed to generative text2image models. The prompt contains: [number of prompts to generate], [class name], [style of picture]. The prompts should slightly differ from one another in the background (e.g. on a beach, in a park etc.) or in the perspective (e.g. viewed from above, viewed from right etc.). This variation must be reasonable, so for example the prompt \\\"A chicken in the water viewed from above\\\" is not a good prompt because chickens don't swim. The prompt specifies that the [class name] must be centered in the generated image. Return only the prompts in a python list and nothing else (no comments, no explanations).\"}, \n","    {\"role\": \"user\", \"content\": \"class:warrior chief with tribal panther make up blue on red and serious eyes, number of prompts:2, style of picture: photo\"}, \n","    {\"role\": \"system\", \"content\": \"[\\\"portrait photo of a old warrior chief, tribal panther make up, blue on red, side profile, looking away, serious eyes 50mm portrait photography, hard rim lighting photography--beta --ar 2:3 --beta --upbeta\\\", portrait photo of a young warrior chief, tribal panther make up, blue on red, side profile, looking away, serious eyes 50mm portrait photography, hard rim lighting photography--beta --ar 2:3  --beta --upbeta\\\"]\"}\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["create_dir_generated_images(imgenetA_gen_path, imagenetA_classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4J5QQU9G6sSe"},"outputs":[],"source":["# generate prompts\n","skipped_classes = imagenetA_generator.generate_prompts(\n","    num_prompts_per_class=20,\n","    style_of_picture=\"photograph\",\n","    path=imgenetA_gen_path,\n","    context_llm = context_llm,\n","    llm_model = \"llama3.1\",\n","    clip_text_encoder = \"ViT-L/14\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["scrape_images_imagenetA(img_style = \"a photo of\",\n","                        imgenetA_gen_path = imgenetA_gen_path,\n","                        limit = 5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jU0i6Nr76uOT"},"outputs":[],"source":["# generate images\n","model_id = \"runwayml/stable-diffusion-v1-5\""]},{"cell_type":"markdown","metadata":{},"source":["By default the stable diffusion pipelines provided by `diffusers` use the `PNDMScheduler`. Given the large number of images to produce and the fact that we are not working on fine-grained classification we decided to use the `DPMSolverMultistepScheduler`. This scheduler produces results of a slightly lower quality, but still very good with a much lower `number_of_inference_steps` required. In fact, while the former requires `50` inference steps, the latter only `25` which means that generating each image takes only `4 seconds` regardless of the pipeline used, instead of `30` seconds on a `T4 GPU` and using `float16 ` tensors."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pipet2i = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n","pipet2i.scheduler = DPMSolverMultistepScheduler.from_config(pipet2i.scheduler.config)\n","pipet2i = pipet2i.to(\"cuda\")\n","num_inference_steps = 25"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pipei2i = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n","pipei2i.scheduler = DPMSolverMultistepScheduler.from_config(pipei2i.scheduler.config)\n","pipei2i = pipei2i.to(\"cuda\")\n","strength = 0.89 \n","num_inference_steps = int(strength**(-1)*25) # to make sure that the number of steps is 25 no matter the selected strength "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class_to_skip = []"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qr7Vc6S06xzX"},"outputs":[],"source":["imagenetA_generator.generate_images(path = imgenetA_gen_path,\n","                                    num_images_per_class = 30,\n","                                    class_to_skip = class_to_skip,\n","                                    image_generation_pipeline = pipet2i, # select one of pipei2i and pipet2i\n","                                    num_inference_steps = num_inference_steps,\n","                                    guidance_scale = 12,\n","                                    strength=strength)"]},{"cell_type":"markdown","metadata":{"id":"7gcvLUxbXa8-"},"source":["## **Testing**\n","\n","The Tester class is designed to facilitate the running of experiments involving a deep neural network model. It provides methods to manage various aspects of the experimental setup, including configuring models and optimizers, handling augmentations, computing statistics, and saving results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sGbfzoA-Xjah"},"outputs":[],"source":["class Tester:\n","    \"\"\"\n","    A class to run all the experiments. It stores all the informations to reproduce the experiments in a json file\n","    at exp_path.\n","    \"\"\"\n","    def __init__(self, model, optimizer, exp_path, device):\n","        self.__model = model\n","        self.__optimizer = optimizer\n","        self.__device = device\n","        self.__exp_path = exp_path\n","\n","    def save_result(self, accuracy, path_result, num_augmentations, augmentations, seed_augmentations, top_augmentations, MEMO, num_adaptation_steps, lr_setting, weights, prior_strength, time_test, use_MC, gen_aug_settings):\n","        \"\"\"\n","        Takes all information of the experiment saves it in a json file stored at exp_path\n","        \"\"\"\n","        data = {\n","            \"accuracy\": accuracy,\n","            \"top_augmentations\" : top_augmentations,\n","            \"use_MEMO\" : MEMO,\n","            \"num_adaptation_steps\" : num_adaptation_steps,\n","            \"lr_setting\" : lr_setting,\n","            \"weights\" : weights,\n","            \"num_augmentations\" : num_augmentations,\n","            \"seed_augmentations\": seed_augmentations,\n","            \"augmentations\" : [str(augmentation) for augmentation in augmentations],\n","            \"prior_strength\" : prior_strength,\n","            \"MC\" : use_MC,\n","            \"time_test\" : time_test,\n","            \"gen_aug_settings\" : gen_aug_settings\n","        }\n","        try:\n","            with open(path_result, 'w') as json_file:\n","                json.dump(data, json_file)\n","        except:\n","            print(\"Result were not saved\")\n","\n","    def get_model(self, weights_imagenet, MC):\n","        \"\"\"\n","        Utility function to instantiate a torch model. The argument weights_imagenet should have\n","        a value in accordance with the parameter weights of torchvision.models.\n","        \"\"\"\n","        if MC:\n","            self.__model=ResNet50Dropout(weights=weights_imagenet, dropout_rate=MC['dropout_rate'], dropout_positions = MC[\"dropout_positions\"])\n","            model = self.__model\n","        else:\n","            model = self.__model(weights=weights_imagenet)\n","\n","        model.to(self.__device)\n","        model.eval()\n","        return model\n","\n","    def get_optimizer(self, model, lr_setting:list):\n","        \"\"\"\n","        Utility function to instantiate a torch optimizer.\n","        ----------\n","        lr_setting: must be a list containing either one global lr for the whole model or a dictionary\n","        where each value is a list with a list of parameters' names and a lr for those parameters.\n","        e.g. \n","        lr_setting = [{\n","            \"classifier\" : [[\"fc.weight\", \"fc.bias\"], 0.00025]\n","            }, 0]\n","        lr_setting = [0.00025]\n","        \"\"\"\n","        if len(lr_setting) == 2:\n","            layers_groups = []\n","            lr_optimizer = []\n","            for layers, lr_param_name in lr_setting[0].items():\n","                layers_groups.extend(lr_param_name[0])\n","                params = [param for name, param in model.named_parameters() if name in lr_param_name[0]]\n","                lr_optimizer.append({\"params\":params, \"lr\": lr_param_name[1]})\n","            other_params = [param for name, param in model.named_parameters() if name not in layers_groups]\n","            lr_optimizer.append({\"params\":other_params})\n","            optimizer = self.__optimizer(lr_optimizer, lr = lr_setting[1], weight_decay = 0)\n","        else:\n","            optimizer = self.__optimizer(model.parameters(), lr = lr_setting[0], weight_decay = 0)\n","        return optimizer\n","\n","    def get_imagenetA_masking(self, imagenetA_masking_dict = None):\n","        \"\"\"\n","        All torchvision models output a tensor [B,1000] with \"B\" being the batch dimension. This function \n","        returns a list of indices to apply to the model's output to use the model on imagenet-A dataset.\n","        ----------\n","        indices_in_1k: list of indices to map [B,1000] -> [B,200]\n","        \"\"\"\n","        if imagenetA_masking_dict == None:\n","            imagenetA_masking_path = \"INSERT PATH TO IMAGENET A MASKING\"\n","            with open(imagenetA_masking_path, 'r') as json_file:\n","                imagenetA_masking = json.load(json_file)\n","            indices_in_1k = [int(k) for k in imagenetA_masking if imagenetA_masking[k] != -1]\n","        else:\n","            indices_in_1k = [int(k) for k in imagenetA_masking_dict if imagenetA_masking_dict[k] != -1]\n","        return indices_in_1k\n","\n","    def get_monte_carlo_statistics(self, mc_logits):\n","        \"\"\"\n","        Compute mean, median, mode and standard deviation of the Monte Carlo samples.\n","        \"\"\"\n","        statistics = {}\n","        mean_logits = mc_logits.mean(dim=0)\n","        statistics['mean'] = mean_logits\n","\n","        median_logits = mc_logits.median(dim=0).values\n","        statistics['median'] = median_logits\n","\n","        pred_classes = mc_logits.argmax(dim=1)\n","        pred_classes_cpu = pred_classes.cpu().numpy()\n","        mode_predictions, _ = stats.mode(pred_classes_cpu, axis=0)\n","        mode_predictions = torch.tensor(mode_predictions.squeeze(), dtype=torch.long)\n","        statistics['mode'] = mode_predictions\n","\n","        uncertainty = mc_logits.var(dim=0)\n","        statistics['std'] = uncertainty\n","        return statistics\n","\n","    def get_prediction(self, image_tensors, model, masking, TTA = False, top_augmentations = 0, MC = None):\n","        \"\"\"\n","        Takes a tensor of images and outputs a prediction for each image.\n","        ----------\n","        image_tensors: is a tensor of [B,C,H,W] if TTA is used or if both MEMO and TTA are not used, or of dimension [C,H,W]\n","                       if only MEMO is used\n","        masking: a list of indices to map the imagenet1k logits to the one of imagenet-A\n","        top_augmentations: a non-negative integer, if greater than 0 then the \"top_augmentations\" with the lowest entropy are\n","                           selected to make the final prediction\n","        MC: a dictionary containing the number of evaluations using Monte Carlo Dropout and the dropout rate\n","        \"\"\"\n","        if MC:\n","            model.train()  # enable dropout by setting the model to training mode\n","            mc_logits = []\n","            for _ in range(MC['num_samples']):\n","                logits = model(image_tensors)[:,masking] if image_tensors.dim() == 4 else model(image_tensors.unsqueeze(0))[:,masking]\n","                mc_logits.append(logits)\n","            mc_logits = torch.stack(mc_logits, dim=0)\n","            if TTA:\n","                # first mean is over MC samples, second mean is over TTA augmentations\n","                probab_augmentations = F.softmax(mc_logits - mc_logits.max(dim=2, keepdim=True)[0], dim=2)\n","                if top_augmentations:\n","                    probab_augmentations = self.get_best_augmentations(probab_augmentations, top_augmentations)\n","                y_pred = probab_augmentations.mean(dim=0).mean(dim=0).argmax().item()\n","                statistics = self.get_monte_carlo_statistics(probab_augmentations.mean(dim=1))\n","                return y_pred, statistics\n","            statistics = self.get_monte_carlo_statistics(mc_logits)\n","            return statistics['median'].argmax(dim=1), statistics\n","        else:\n","            logits = model(image_tensors)[:,masking] if image_tensors.dim() == 4 else model(image_tensors.unsqueeze(0))[:,masking]\n","            if TTA:\n","                probab_augmentations = F.softmax(logits - logits.max(dim=1)[0][:, None], dim=1)\n","                if top_augmentations:\n","                    probab_augmentations = self.get_best_augmentations(probab_augmentations, top_augmentations)\n","                y_pred = probab_augmentations.mean(dim=0).argmax().item()\n","                return y_pred, None\n","            return logits.argmax(dim=1), None\n","\n","    def compute_entropy(self, probabilities: torch.tensor):\n","        \"\"\"\n","        See MEMO.py\n","        \"\"\"\n","        return compute_entropy(probabilities)\n","\n","    def get_best_augmentations(self, probabilities: torch.tensor, top_k: int):\n","        \"\"\"\n","        See MEMO.py\n","        \"\"\"\n","        return get_best_augmentations(probabilities, top_k)\n","\n","    def get_test_augmentations(self, input:torch.tensor, augmentations:list, num_augmentations:int, seed_augmentations:int):\n","        \"\"\"\n","        See MEMO.py\n","        \"\"\"\n","        return get_test_augmentations(input, augmentations, num_augmentations, seed_augmentations)\n","\n","    def retrieve_generated_images(self, img, num_images, clip_model, clip_preprocess, img_to_tensor_pipe, data_path, use_t2i_similarity, t2i_images, i2i_images, threshold):\n","        \"\"\"\n","        See image_generator.py.\n","        \"\"\"\n","        return retrieve_gen_images(img = img,  \n","                                   num_images = num_images, \n","                                   clip_model = clip_model, \n","                                   clip_preprocess = clip_preprocess,\n","                                   img_to_tensor_pipe = img_to_tensor_pipe,\n","                                   data_path = data_path,\n","                                   use_t2i_similarity = use_t2i_similarity, \n","                                   t2i_images = t2i_images, \n","                                   i2i_images = i2i_images, \n","                                   threshold = threshold)\n","    \n","    def test(self,\n","             augmentations:list, \n","             num_augmentations:int, \n","             seed_augmentations:int,\n","             img_root:str,\n","             lr_setting:list,\n","             weights_imagenet = None,\n","             dataset = \"imagenetA\",\n","             imagenetA_masking = None,\n","             batch_size = 64,\n","             MEMO = False,\n","             num_adaptation_steps = 0,\n","             top_augmentations = 0,\n","             TTA = False,\n","             prior_strength = -1,\n","             verbose = True,\n","             log_interval = 1,\n","             MC = None,\n","             gen_aug_settings = None):\n","        \"\"\"\n","        Main function to test a torchvision model with different test-time adaptation techniques \n","        and keep track of the results and the experiment setting. \n","        ---\n","        augmentations: list of torchvision.transforms functions.\n","        num_augmentations: the number of augmentations to use for each sample to perform test-time adaptation.\n","        seed_augmentations: seed to reproduce the sampling of augmentations.\n","        img_root: str path to get a dataset in a torch format.\n","        lr_setting: list with lr instructions to adapt the model. See \"get_optimizer\" for more details.\n","        weights_imagenet: weights_imagenet should have a value in accordance with the parameter \n","                          weights of torchvision.models.\n","        dataset: the name of the dataset to use. Note: this parameter doesn't directly control the data \n","                 used, it's only used to use the right masking to map the models' outputs to the right dimensions. \n","                 At the moment only Imagenet-A masking is supported.\n","        MEMO: a boolean to use marginal entropy minimization with one test point\n","        TTA: a boolean to use test time augmentation\n","        top_augmentations: if MEMO or TTA are set to True, then values higher than zero select the top_augmentations \n","                           with the lowest entropy (highest confidence).\n","        prior_strength: defines the weight given to pre-trained statistics in BN adaptation. If negative, then no BN\n","                        adaptation is applied.\n","        verbose: use loading bar to visualize accuracy and number of batch during testing.\n","        log_interval: defines after how many batches a new accuracy should be displayed. Default is 1, thus \n","                      after each batch a new value is displayed. \n","        \"\"\"\n","        # check some basic conditions\n","        assert bool(num_adaptation_steps) == MEMO, \"When using MEMO adaptation steps should be > 1, otherwise equal to 0.\"  \n","        if not (MEMO or TTA):\n","            assert not (num_augmentations or top_augmentations), \"If both MEMO and TTA are set to False, then top_augmentations and num_augmentations must be 0\"\n","        assert not lr_setting if not MEMO else True, \"If MEMO is false, then lr_setting must be None\" \n","        assert isinstance(prior_strength, (float,int)) , \"Prior adaptation must be either a float or an int\"\n","        assert isinstance(gen_aug_settings, dict), \"gen_aug_settings must be a dict containing settings to retrieve the generated images\"\n","        \n","        # get the name of the weigths used and define the name of the experiment \n","        weights_name = str(weights_imagenet).split(\".\")[-1] if weights_imagenet else \"MEMO_repo\"\n","        use_MC = True if MC else False\n","        name_result = f\"MEMO_{MEMO}_AdaptSteps_{num_adaptation_steps}_adaptBN_{prior_strength}_TTA_{TTA}_aug_{num_augmentations}_topaug_{top_augmentations}_seed_aug_{seed_augmentations}_weights_{weights_name}_MC_{use_MC}_genAug_{bool(gen_aug_settings)}\"\n","        path_result = os.path.join(self.__exp_path,name_result)\n","        assert not os.path.exists(path_result),f\"MEMO test already exists: {path_result}\"\n","\n","        # in case of using dropout, check if the model is a ResNet50Dropout and the parameters are correct\n","        if MC:\n","            assert isinstance(self.__model, ResNet50Dropout), f\"To use dropout the model must be a ResNet50Dropout\"\n","            assert MC['num_samples'] > 1, f\"To use dropout the number of samples must be greater than 1\" \n","\n","        # transformation pipeline used in ResNet-50 original training\n","        transform_loader = T.Compose([\n","            T.Resize(256),\n","            T.CenterCrop(224),\n","            T.ToTensor()\n","        ])\n","\n","        # to use after model's update\n","        normalize_input = T.Compose([\n","                        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","                    ])\n","\n","        test_loader = get_data(batch_size, img_root, transform = transform_loader, split_data=False)\n","        model = self.get_model(weights_imagenet, MC)\n","\n","        # if MEMO is used, create a checkpoint to reload after each model and optimizer update\n","        if MEMO:\n","            optimizer = self.get_optimizer(model = model, lr_setting = lr_setting)\n","            MEMO_checkpoint_path = os.path.join(self.__exp_path,\"checkpoint.pth\")\n","            torch.save({\n","                'model': model.state_dict(),\n","                'optimizer': optimizer.state_dict(),\n","            }, MEMO_checkpoint_path)\n","            MEMO_checkpoint = torch.load(MEMO_checkpoint_path)\n","\n","        if dataset == \"imagenetA\":\n","            imagenetA_masking = self.get_imagenetA_masking(imagenetA_masking)\n","        \n","        if gen_aug_settings:\n","            clip_model, clip_preprocess = clip.load(gen_aug_settings[\"clip_img_encoder\"])\n","        \n","        if prior_strength < 0:\n","            torch.nn.BatchNorm2d.prior_strength = 1\n","        else:\n","            torch.nn.BatchNorm2d.prior_strength = prior_strength / (prior_strength + 1)\n","            torch.nn.BatchNorm2d.forward = adaptive_bn_forward\n","\n","        # Initialize a dictionary to store accumulated time for each step\n","        time_dict = {\n","            \"MEMO_update\": 0.0,\n","            \"get_augmentations\": 0.0,\n","            \"confidence_selection\": 0.0,\n","            \"get_prediction\": 0.0,\n","            \"get_gen_images\" : 0.0,\n","            \"total_time\": 0.0\n","        }\n","\n","        samples = 0.0\n","        cumulative_accuracy = 0.0\n","\n","        for batch_idx, (inputs, targets) in enumerate(test_loader):\n","            inputs, targets = inputs.to(self.__device), targets.to(self.__device)\n","            if MEMO or TTA:\n","                for input, target in zip(inputs, targets):\n","                    if MEMO:\n","                        model.load_state_dict(MEMO_checkpoint['model'])\n","                        model.eval()\n","                        optimizer.load_state_dict(MEMO_checkpoint['optimizer'])\n","\n","                    # get normalized augmentations\n","                    start_time_augmentations = time.time()\n","                    test_augmentations = self.get_test_augmentations(input, augmentations, num_augmentations, seed_augmentations)\n","                    end_time_augmentations = time.time()\n","                    time_dict[\"get_augmentations\"] += (end_time_augmentations - start_time_augmentations)\n","\n","                    # retrieve generated images\n","                    if gen_aug_settings:\n","                        start_time_gen_augmentations = time.time()\n","                        retrieved_gen_images = self.retrieve_generated_images(img = input, \n","                                                                              num_images = gen_aug_settings[\"num_img\"], \n","                                                                              clip_model = clip_model, \n","                                                                              clip_preprocess = clip_preprocess,\n","                                                                              img_to_tensor_pipe = transform_loader, \n","                                                                              data_path = gen_aug_settings[\"gen_data_path\"], \n","                                                                              use_t2i_similarity = gen_aug_settings[\"use_t2i_similarity\"], \n","                                                                              t2i_images = gen_aug_settings[\"t2i_img\"], \n","                                                                              i2i_images = gen_aug_settings[\"i2i_img\"],\n","                                                                              threshold = gen_aug_settings[\"threshold\"])\n","                        if len(retrieved_gen_images):\n","                            retrieved_gen_images = retrieved_gen_images.to(self.__device)\n","                        end_time_gen_augmentations = time.time()\n","                        time_dict[\"get_gen_images\"] += (end_time_gen_augmentations - start_time_gen_augmentations)\n","\n","                    test_augmentations = test_augmentations.to(self.__device)\n","\n","                    for _ in range(num_adaptation_steps):\n","                        logits = model(test_augmentations)\n","\n","                        # apply imagenetA masking\n","                        if dataset == \"imagenetA\":\n","                            logits = logits[:, imagenetA_masking]\n","                        # compute stable softmax\n","                        probab_augmentations = F.softmax(logits - logits.max(dim=1)[0][:, None], dim=1)\n","\n","                        # confidence selection for augmentations\n","                        if top_augmentations:\n","                            start_time_confidence_selection = time.time()\n","                            probab_augmentations = self.get_best_augmentations(probab_augmentations, top_augmentations)\n","                            end_time_confidence_selection = time.time()\n","                            time_dict[\"confidence_selection\"] += (end_time_confidence_selection - start_time_confidence_selection)\n","\n","                        if len(gen_aug_settings):\n","                            if len(retrieve_gen_images):\n","                                gen_images_logits = model(retrieved_gen_images)\n","                                if dataset == \"imagenetA\":\n","                                    gen_images_logits = gen_images_logits[:, imagenetA_masking]\n","                                probab_gen_augmentations = F.softmax(gen_images_logits - gen_images_logits.max(dim=1)[0][:, None], dim=1)\n","                                probab_augmentations = torch.cat([probab_augmentations,probab_gen_augmentations],dim=0)\n","\n","                        if MEMO:\n","                            start_time_memo_update = time.time()\n","                            marginal_output_distribution = torch.mean(probab_augmentations, dim=0)\n","                            marginal_loss = self.compute_entropy(marginal_output_distribution)\n","                            marginal_loss.backward()\n","                            optimizer.step()\n","                            optimizer.zero_grad()\n","                            end_time_memo_update = time.time()\n","                            time_dict[\"MEMO_update\"] += (end_time_memo_update - start_time_memo_update)\n","\n","                    start_time_prediction = time.time()\n","                    with torch.no_grad():\n","                        if TTA:\n","                            # statistics:\n","                            # dictionary containing statistics resulting from the application of monte carlo dropout\n","                            # look at get_monte_carlo_statistics() for more details\n","                            y_pred, statistics = self.get_prediction(test_augmentations, model, imagenetA_masking, TTA, top_augmentations, MC=MC)\n","                        else:\n","                            input = normalize_input(input)\n","                            y_pred, statistics = self.get_prediction(input, model, imagenetA_masking, MC=MC)\n","                        cumulative_accuracy += int(target == y_pred)\n","                    end_time_prediction = time.time()\n","                    time_dict[\"get_prediction\"] += (end_time_prediction - start_time_prediction)\n","            else:\n","                start_time_prediction = time.time()\n","                with torch.no_grad():\n","                    inputs = normalize_input(inputs)\n","                    y_pred, _ = self.get_prediction(inputs, model, imagenetA_masking, MC=MC)\n","                    correct_predictions = (targets == y_pred).sum().item()\n","                    cumulative_accuracy += correct_predictions\n","                end_time_prediction = time.time()\n","                time_dict[\"get_prediction\"] += (end_time_prediction - start_time_prediction)\n","\n","            samples += inputs.shape[0]\n","\n","            if verbose and batch_idx % log_interval == 0:\n","                current_accuracy = cumulative_accuracy / samples * 100\n","                print(f'Batch {batch_idx}/{len(test_loader)}, Accuracy: {current_accuracy:.2f}%', end='\\r')\n","\n","        accuracy = cumulative_accuracy / samples * 100\n","        time_dict[\"total_time\"] += sum(time_dict.values())\n","\n","        # save information to reproduce the experiment\n","        self.save_result(accuracy = accuracy,\n","                         path_result = path_result,\n","                         seed_augmentations = seed_augmentations,\n","                         num_augmentations = num_augmentations,\n","                         augmentations = augmentations,\n","                         top_augmentations = top_augmentations,\n","                         MEMO = MEMO,\n","                         num_adaptation_steps = num_adaptation_steps,\n","                         lr_setting = lr_setting,\n","                         weights = weights_name,\n","                         prior_strength = prior_strength,\n","                         use_MC = use_MC,\n","                         time_test = time_dict,\n","                         gen_aug_settings = gen_aug_settings)\n","\n","        return accuracy"]},{"cell_type":"markdown","metadata":{"id":"0v-lB2Zr49Ml"},"source":["To run the solution, simply execute the following code sections."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kbV1_cGTXoJI"},"outputs":[],"source":["imagenet_a_path = \"imagenet-a\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ePNiGdZh5eOG"},"outputs":[],"source":["device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1npyJMFH5gK_"},"outputs":[],"source":["augmix_augmentations = [\n","    T.AugMix(severity=3, mixture_width=3, chain_depth=3, alpha=1.0),\n","    T.AugMix(severity=2, mixture_width=2, chain_depth=3, alpha=1.0),\n","    T.AugMix(severity=4, mixture_width=4, chain_depth=3, alpha=1.0),\n","    T.AugMix(severity=3, mixture_width=3, chain_depth=3, alpha=1.0),\n","    T.AugMix(severity=2, mixture_width=2, chain_depth=3, alpha=1.0),\n","    T.AugMix(severity=4, mixture_width=4, chain_depth=3, alpha=1.0),\n","    T.AugMix(severity=3, mixture_width=3, chain_depth=3, alpha=1.0),\n","    T.AugMix(severity=2, mixture_width=2, chain_depth=3, alpha=1.0),\n","    T.AugMix(severity=4, mixture_width=4, chain_depth=3, alpha=1.0),\n","    T.AugMix(severity=3, mixture_width=3, chain_depth=3, alpha=1.0),\n","    T.AugMix(severity=2, mixture_width=2, chain_depth=3, alpha=1.0),\n","    T.AugMix(severity=4, mixture_width=4, chain_depth=3, alpha=1.0),\n","    T.AugMix(severity=3, mixture_width=3, chain_depth=3, alpha=1.0),\n","    T.AugMix(severity=2, mixture_width=2, chain_depth=3, alpha=1.0),\n","    T.AugMix(severity=4, mixture_width=4, chain_depth=3, alpha=1.0),\n","    T.AugMix(severity=3, mixture_width=3, chain_depth=3, alpha=1.0),\n","    T.AugMix(severity=2, mixture_width=2, chain_depth=3, alpha=1.0),\n","    T.AugMix(severity=4, mixture_width=4, chain_depth=3, alpha=1.0)\n","]"]},{"cell_type":"markdown","metadata":{"id":"oyy0zt7Y5mIv"},"source":["### Resnet50"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LZtqROYM5uR_"},"outputs":[],"source":["exp_path_a = \"INSERT THE PATH TO SAVE THE EXPERIMENTS DETAILS\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5aRVpU7l5wbX"},"outputs":[],"source":["MC = {\n","\t\"dropout_rate\": 0.2,\n","\t\"num_samples\": 10,\n","\t\"use_dropout\": False,\n","    \"dropout_positions\": dropout_positions\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yqmFK6qT5x4w"},"outputs":[],"source":["tester_resnet50 = Tester(\n","    model = ResNet50Dropout() if MC['use_dropout'] else models.resnet50,\n","    optimizer = torch.optim.SGD,\n","    exp_path = exp_path_a,\n","    device = device\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZwfV76pE50q-"},"outputs":[],"source":["# you can assign different learning rates to different layers of the model\n","#lr_setting = [{\n","#    \"classifier\" : [[\"fc.weight\", \"fc.bias\"], 0.00025]\n","#}, 0]\n","\n","# gloabal learning rate\n","lr_setting_sgd = [0.00025] # setting used in MEMO paper for SGD\n","lr_setting_adam = [0.0001] # setting used in MEMO paper for ADAM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qAR1-n6A52LG"},"outputs":[],"source":["imagenetV1_weights = models.ResNet50_Weights.IMAGENET1K_V1 # MEMO paper used these weights\n","imagenetV2_weights = models.ResNet50_Weights.IMAGENET1K_V2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zBGUSsHr54em"},"outputs":[],"source":["gen_aug_settings = {\n","    \"clip_img_encoder\" : \"ViT-L/14\",\n","    \"num_img\" : 40,\n","    \"gen_data_path\" : imgenetA_gen_path,\n","    \"use_t2i_similarity\" : True,\n","    \"t2i_img\" : True,\n","    \"i2i_img\" : True,\n","    \"threshold\" : 0.45\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q5FtcFC_56ZO"},"outputs":[],"source":["tester_resnet50.test(\n","     augmentations = augmix_augmentations,\n","     num_augmentations = 16,\n","     seed_augmentations = 42,\n","     batch_size = 64,\n","     img_root = imagenet_a_path,\n","     imagenetA_masking = imagenetA_masking,\n","     dataset = \"imagenetA\",\n","     num_adaptation_steps = 4,\n","     MEMO = True,\n","     lr_setting = lr_setting_sgd,\n","     top_augmentations = 8, \n","     weights_imagenet = imagenetV1_weights,\n","     prior_strength = 16,\n","     TTA = True,\n","     MC = None,\n","     gen_aug_settings = gen_aug_settings\n",")"]},{"cell_type":"markdown","metadata":{"id":"zwzAwcHxXtKG"},"source":["## **Results**\n","\n","<table>\n","  <thead>\n","    <tr>\n","      <th rowspan=\"1\">Experiment</th>\n","      <th rowspan=\"1\">Dataset</th>\n","      <th rowspan=\"1\">Base Model</th>\n","      <th rowspan=\"1\">Weights</th>\n","      <th rowspan=\"1\", colspan=\"2\">Optimizer</th>\n","      <th rowspan=\"1\">Optimization Steps</th>\n","      <th rowspan=\"1\", colspan=\"3\">Augmentations</th>\n","      <th rowspan=\"1\">Batch Size</th>\n","      <th rowspan=\"1\">MEMO</th>\n","      <th rowspan=\"1\">Confidence Selection</th>\n","      <th rowspan=\"1\", colspan=\"2\">BN</th>\n","      <th rowspan=\"1\">TTA</th>\n","      <th rowspan=\"1\", colspan=\"3\">MC</th>\n","      <th rowspan=\"1\">Efficient DiffTPT</th>\n","      <th rowspan=\"1\">Accuracy</th>\n","      <th rowspan=\"1\">Inference Time</th>\n","    </tr>\n","    <tr>\n","      <th>Nr.</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th>Type</th>\n","      <th>LR</th>\n","      <th>Nr.</th>\n","      <th>Type</th>\n","      <th>Number</th>\n","      <th>Seed</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th>Prior Strength</th>\n","      <th></th>\n","      <th></th>\n","      <th>Dropout rate</th>\n","      <th>Nr. Samples</th>\n","      <th></th>\n","      <th>%</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody align=\"center\">\n","    <tr>\n","      <th>1</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>64</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>False</td>\n","      <td>0.026</td>\n","      <td>00:00:20</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>False</td>\n","      <td>8</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>False</td>\n","      <td>0.253</td>\n","      <td>00:26:20</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>64</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>0.20</td>\n","      <td>10</td>\n","      <td>False</td>\n","      <td>1.613</td>\n","      <td>00:03:48</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>64</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>True</td>\n","      <td>16</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>False</td>\n","      <td>0.4</td>\n","      <td>00:00:23</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>SGD</td>\n","      <td>0.00025</td>\n","      <td>1</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>False</td>\n","      <td>0.16</td>\n","      <td>00:33:20</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>SGD</td>\n","      <td>0.00025</td>\n","      <td>1</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>True</td>\n","      <td>16</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>False</td>\n","      <td>0.853</td>\n","      <td>00:38:44</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>SGD</td>\n","      <td>0.00025</td>\n","      <td>1</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>True</td>\n","      <td>16</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>0.20</td>\n","      <td>10</td>\n","      <td>False</td>\n","      <td>1.053</td>\n","      <td>01:43:29</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>SGD</td>\n","      <td>0.00025</td>\n","      <td>4</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>False</td>\n","      <td>0.213</td>\n","      <td>00:42:03</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>SGD</td>\n","      <td>0.00025</td>\n","      <td>4</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>True</td>\n","      <td>16</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>False</td>\n","      <td>0.853</td>\n","      <td>00:47:37</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>SGD</td>\n","      <td>0.00025</td>\n","      <td>4</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>True</td>\n","      <td>16</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>0.20</td>\n","      <td>10</td>\n","      <td>False</td>\n","      <td>0.95</td>\n","      <td>01:49:42</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>ADAM</td>\n","      <td>0.0001</td>\n","      <td>1</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>False</td>\n","      <td>0.12</td>\n","      <td>00:31:45</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>ADAM</td>\n","      <td>0.0001</td>\n","      <td>1</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>True</td>\n","      <td>16</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>False</td>\n","      <td>0.826</td>\n","      <td>00:37:37</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>ADAM</td>\n","      <td>0.0001</td>\n","      <td>1</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>True</td>\n","      <td>16</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>0.20</td>\n","      <td>10</td>\n","      <td>False</td>\n","      <td>1.506</td>\n","      <td>01:39:47</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>ADAM</td>\n","      <td>0.0001</td>\n","      <td>4</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>False</td>\n","      <td>0.213</td>\n","      <td>00:41:19</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>ADAM</td>\n","      <td>0.0001</td>\n","      <td>4</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>True</td>\n","      <td>16</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>False</td>\n","      <td>0.826</td>\n","      <td>00:46:55</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>ADAM</td>\n","      <td>0.0001</td>\n","      <td>4</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>True</td>\n","      <td>16</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>0.20</td>\n","      <td>10</td>\n","      <td>False</td>\n","      <td>1.04</td>\n","      <td>01:44:57</td>\n","    </tr>  \n","    <tr>\n","      <th>17</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>ADAM</td>\n","      <td>0.0001</td>\n","      <td>4</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>True</td>\n","      <td>16</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td><strong>True<strong></td>\n","      <td><strong>9.5<strong></td>\n","      <td>03:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>ImageNet-A</td>\n","      <td>ViT-B16</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>False</td>\n","      <td>20.75</td>\n","      <td>00:01:17</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>ImageNet-A</td>\n","      <td>ViT-B16</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>ADAM</td>\n","      <td>0.0001</td>\n","      <td>4</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>True</td>\n","      <td>16</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td><strong>True<strong></td>\n","      <td><strong>36.5<strong></td>\n","      <td>04:20:00</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},{"cell_type":"markdown","metadata":{},"source":["Note:\n","- The hyperparameters used for Efficient DiffTPT are the same as the ones used in the code cell above i.e. \n","    - `clip_img_encoder` : \"ViT-L/14\"\n","    - `num_img` : 40\n","    - `use_t2i_similarity` : True\n","    - `t2i_img` : True\n","    - `i2i_img` : True\n","    - `threshold` : 0.45\n","- Due to the lack of time, the experiments with EfficientDiffTPT were performed on a subset of the dataset of 1280 samples. We assume that using the whole dataset should not change the final result much. "]},{"cell_type":"markdown","metadata":{"id":"J67VmQSzXvXe"},"source":["## **Discussion**"]},{"cell_type":"markdown","metadata":{},"source":["### Previously Existing Methods\n","\n","All methods implemented improve the accuracy score. Notably we were able to reproduce the result of the original MEMO paper by obtaining roughly 0.9% of accuracy. Additionally, as it was expected, using MC Dropout further increases the performance regardless of the optimizer and the other methods used. The only oddity is that the effect of MC seems not to be additive, but interactive with the other methods. Indeed, when using MC without any other method, aside with Efficient DiffTPT, we reach the highest performance with 1.6%. On the other hand, the second best result is obtained when we use all our methods with `ADAM` optimizer instead of `SGD`. Thus the improvement can not directly be attributed to the use of MC as in the experiment 4 `optim_steps` and a different optimizer. Regardless, in general `MC` seems to increase the performance despite the much higher `inference time`."]},{"cell_type":"markdown","metadata":{},"source":["### Effiecient DiffTPT"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["clip_image_encoder = \"ViT-L/14\"\n","clip_model, clip_preprocess = clip.load(clip_image_encoder)\n","\n","retrieved_images = retrieve_gen_images(img = \"put one of the dataset images here\",\n","                                       num_images = 40,\n","                                       data_path = imgenetA_gen_path,\n","                                       clip_model = clip_model,\n","                                       clip_preprocess = clip_preprocess,\n","                                       t2i_images = True,\n","                                       i2i_images = False,\n","                                       use_t2i_similarity = True,\n","                                       threshold = 0.45)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def create_image_grid(images, grid_width, save_path, cell_size=(100, 100)):\n","    \"\"\"\n","    Create a grid of images from a list of PIL images.\n","\n","    Args:\n","        images (list of PIL.Image): List of PIL images to arrange in a grid.\n","        grid_width (int): Number of columns in the grid.\n","        cell_size (tuple): Size of each cell in the grid (width, height).\n","\n","    Returns:\n","        PIL.Image: An image containing the grid of images.\n","    \"\"\"\n","    # Resize images to the specified cell size\n","    if len(images) == 0:\n","        print(\"No images\")\n","        return\n","        \n","    resized_images = [img.resize(cell_size) for img in images]\n","    \n","    # Calculate grid dimensions\n","    grid_height = math.ceil(len(images) / grid_width)  # Number of rows needed\n","    grid_img_width = cell_size[0] * grid_width\n","    grid_img_height = cell_size[1] * grid_height\n","\n","    # Create a blank canvas for the grid\n","    grid_img = Image.new('RGB', (grid_img_width, grid_img_height), (255, 255, 255))  # White background\n","\n","    # Paste images into the grid\n","    for i, img in enumerate(resized_images):\n","        row = i // grid_width\n","        col = i % grid_width\n","        x = col * cell_size[0]\n","        y = row * cell_size[1]\n","        grid_img.paste(img, (x, y))\n","\n","    if save_path:\n","        try:\n","            grid_img.save(save_path)\n","            print(f\"Grid image saved to {save_path}\")\n","        except Exception as e:\n","            print(f\"Error saving the image: {e}\")\n","\n","    return grid_img"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["create_image_grid(retrieved_images, \n","                  save_path = \"PUT YOUR PATH TO SAVE THE GRID IMAGE\",\n","                  grid_width = 4)"]},{"cell_type":"markdown","metadata":{},"source":["<p align=\"center\">\n","  <img src=\"images/generated_images/dragonfly_i2i.png\" width=\"500\" height=\"300\">  \n","</p>\n","\n","<p align=\"center\">\n","  <img src=\"images/generated_images/jelly_fish_i2i.png\" width=\"500\" height=\"300\">  \n","</p>\n","\n","<p align=\"center\">\n","  <img src=\"images/generated_images/stingray_i2i.png\" width=\"500\" height=\"300\">  \n","</p>\n","\n","<p align=\"center\">\n","  <img src=\"images/generated_images/goldfinch_i2i.png\" width=\"500\" height=\"300\">  \n","</p>"]},{"cell_type":"markdown","metadata":{},"source":["- **Trade-off Between Data Consistency and Data Scarcity**: Lowering the `threshold` allows for retrieving more generated augmentations, but this comes at the cost of introducing augmentations that may belong to other classes, potentially hindering the adaptation phase. This is especially problematic if the original image has particularly low quality or if the class object within it is difficult to distinguish. In such cases, either a few augmentations might be retrieved, or a high number of augmentations may be obtained, many of which could be unrelated.  \n","  - **Example**: The first and second images show pictures of `dragonflies` and `jellyfishes` that were retrieved in high quantity without any spurious samples. In contrast, the third image shows 40 pictures of `stingrays` (all with cosine similarity > `threshold`), which unfortunately contain two extraneous augmentations highlighted by the red circle. Finally, the last image illustrates the few images (6 out of 40) that were retrieved from an image of a `goldfinch`.\n","\n","- **Hyper-parameters**:  \n","  - **`threshold`**: Due to time constraints, it was not possible to experiment with different values. This parameter is critical, as it determines the aforementioned trade-off between data consistency and scarcity.  \n","  - **`i2i_images` and `use_t2i_similarity`**: Since no ablation study was performed, it remains unclear to what extent using images generated with the `i2i` Stable Diffusion pipeline and incorporating `t2i_similarity` may improve or possibly reduce performance.\n","\n","- **Efficient DiffTPT with MC**: In this study we didn't test the use of Efficient DiffTPT combined with MC Dropout due to both the lack of time and incredibly long time it would take to test this setting."]},{"cell_type":"markdown","metadata":{"id":"9BY3pvNJXwzd"},"source":["## **Conclusion**\n","\n","Possible improvements on Efficient DiffTPT:\n","- Conisder fine-tuned models for image generation\n","- LLM can guide the creation of prompts making them more realistic and tailored to some style to avoid wasting resources without sacrificing variability\n","- One can guide the generation using Images retrieved on Internet using the label along with a random prompts. However, it's not clear if it would be useful to use such images. Perhaps, sometimes it is (especially if very big domain shift), sometimes it doesn't make much difference or it might hurt performance. Thus further research is needed.\n","- Need to explore more powerful prompt engineering and models when using it for fine-grained domain-shift as images needs to be as accurate as possible. Perhaps, in such a use case the original DiffTPT would be preferred.\n","- Is it better to retrieve images using image embeddings, text embeddings that generated images or a mix of both?"]},{"cell_type":"markdown","metadata":{"id":"vkWfJE_fXhvw"},"source":["## **Bibliography**\n","\n","1. **Marvin Zhang and Sergey Levine and Chelsea Finn.** \"MEMO: Test Time Robustness via Adaptation and Augmentation.\" Advances in neural information processing systems, Vol. 35, 2021, pp. 38629-38642. [https://arxiv.org/abs/2110.09506](https://arxiv.org/abs/2110.09506).\n","\n","2. **Lyzhov, Alexander and Molchanova, Yuliya and Ashukha, Arsenii and Molchanov, Dmitry and Vetrov, Dmitry.** \"Greedy Policy Search: A Simple Baseline for Learnable Test-Time Augmentation.\" Proceedings of Machine Learning Research, Vol. 124, 2020, pp. 1308-1317. [https://proceedings.mlr.press/v124/lyzhov20a.html](https://proceedings.mlr.press/v124/lyzhov20a.html).\n","\n","3. **Schneider, Steffen and Rusak, Evgenia and Eck, Luisa and Bringmann, Oliver and Brendel, Wieland and Bethge, Matthias.** \"Improving robustness against common corruptions by covariate shift adaptation.\" Advances in Neural Information Processing Systems, Vol. 33, 2020, pp. 11539-11551. [https://proceedings.neurips.cc/paper_files/paper/2020/file/85690f81aadc1749175c187784afc9ee-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/85690f81aadc1749175c187784afc9ee-Paper.pdf).\n","\n","4. **Gal, Yarin and Ghahramani, Zoubin** \"Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning.\" Proceedings of The 33rd International Conference on Machine Learning, Vol. 48, 2016, pp. 1050-1059. [https://proceedings.mlr.press/v48/gal16.html](https://proceedings.mlr.press/v48/gal16.html).\n","\n","5. **Feng, Chun-Mei and Yu, Kai and Liu, Yong and Khan, Salman and Zuo, Wangmeng.** \"Diverse Data Augmentation with Diffusions for Effective Test-time Prompt Tuning.\" 2023 IEEE/CVF International Conference on Computer Vision (ICCV), 2023, pp. 2704-2714. [https://arxiv.org/abs/2308.06038](https://arxiv.org/abs/2308.06038).\n","\n","6. [Ollama library](https://ollama.com/)\n","\n","\n","\n"]}],"metadata":{"colab":{"collapsed_sections":["qLwYsF2LtEI6","hWtwo8X73663","1HVxP1v04Bhm","WrvcCQL716S2","cb0esIOp9jQb","7gcvLUxbXa8-"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":0}
