{"cells":[{"cell_type":"markdown","metadata":{"id":"rn1W7CwqyFWZ"},"source":["# **Deep Learning - Test-Time Adaptation**\n","---\n","**University of Trento, Academic Year 2023/2024**\n","\n","---\n","#### **Group 26**\n","> <a href=\"https://github.com/giuseppecurci\">Giuseppe Curci</a> \\\n","> 243049\n","\n","> <a href=\"https://github.com/andy295\">Andrea Cristiano</a> \\\n","> 229370\n","---\n","---"]},{"cell_type":"code","source":["!pip install ollama # if ollama is not available, install by executing\n","                    # the intall_and_run_ollama.sh script\n","!pip install diffusers\n","!pip install bing_image_downloader\n","!pip install git+https://github.com/openai/CLIP.git"],"metadata":{"id":"CIljqMXC6NuG"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f686ZK2OOXAc"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset\n","\n","import torchvision\n","import torchvision.transforms as T\n","import torchvision.models as models\n","from torchvision.transforms import Compose, Normalize, ToTensor\n","\n","import matplotlib\n","from matplotlib.lines import Line2D\n","from matplotlib import pyplot as plt\n","\n","from scipy import stats\n","from scipy.ndimage import zoom\n","\n","from typing import Dict, List\n","from typing import Callable, List, Optional, Tuple\n","\n","from PIL import Image\n","from tqdm import tqdm\n","from io import BytesIO\n","from pathlib import Path\n","from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, DPMSolverMultistepScheduler\n","\n","import boto3 # read and write for AWS buckets\n","import clip\n","import cv2\n","import gc\n","import json\n","import math\n","import numpy as np\n","import ollama\n","import os\n","import random\n","import time\n","import ttach as tta\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"h62t4jTDPk12"},"source":["## **Introduction**\n","\n","### **Domain Shift**\n","Domain shift refers to the change in the data distribution between the training phase and the testing phase of a machine learning model. In other words, the data that the model encounters during deployment (inference) differs from the data it was trained on.\n","\n","Machine learning models are typically trained on a specific dataset. The fundamental assumption is that the training data distribution is as representative as possible of what the models will encounter during deployment. In the presence of domain shift, the model may not generalize well, leading to a degradation in performance.\n","\n","In real-world applications, it is easy to identify cases where data distributions may vary, and the causes can be numerous, such as changes in the environment, noise in the sensors used to acquire information, etc.\n","\n","A model trained on images depicting a scene during the spring season may perform poorly when tested on images of the same scene but taken during the winter season, even if the scene's content is the same.\n","\n","### **Test-Time Adaptation (TTA)**\n","Test-Time Adaptation refers to techniques that allow a previously trained model to adapt to a new data distribution during inference (testing), without the need for a new training phase.\n","\n","The goal is to improve the model's performance on the shifted domain by adapting its parameters or predictions based on the data encountered during testing."]},{"cell_type":"markdown","metadata":{"id":"PicQiiSaPzCV"},"source":["## **Pipeline**\n","\n","(idealmente andrebbe creato un disegno che spieghi cosa stiamo facendo, lo possiamo fare anche dopo. Per ora puoi anche limitarti a spiegare a parole oppure lascialo e lo facciamo quando abbiamo il disegno)"]},{"cell_type":"markdown","metadata":{"id":"Ge0jVaTAWPYt"},"source":["## **Utils**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qLwYsF2LtEI6"},"source":["### **Recover dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7TDRMeMbWXmt"},"outputs":[],"source":["# Class that interacts with images stored in an Amazon S3 bucket.\n","# It allows to load and preprocess images on-the-fly during training or inference.\n","class S3ImageFolder(Dataset):\n","    def __init__(self, root, transform=None):\n","        self.s3_bucket = \"deeplearning2024-datasets\" # name of the bucket\n","        self.s3_region = \"eu-west-1\" # Ireland\n","        self.s3_client = boto3.client(\"s3\", region_name=self.s3_region, verify=True)\n","        self.transform = transform\n","\n","        # Get list of objects in the bucket\n","        response = self.s3_client.list_objects_v2(Bucket=self.s3_bucket, Prefix=root)\n","        objects = response.get(\"Contents\", [])\n","        while response.get(\"NextContinuationToken\"):\n","            response = self.s3_client.list_objects_v2(\n","                Bucket=self.s3_bucket,\n","                Prefix=root,\n","                ContinuationToken=response[\"NextContinuationToken\"]\n","            )\n","            objects.extend(response.get(\"Contents\", []))\n","\n","        # Iterate and keep valid files only\n","        self.instances = []\n","        for ds_idx, item in enumerate(objects):\n","            key = item[\"Key\"]\n","            path = Path(key)\n","\n","            # Check if file is valid\n","            if path.suffix.lower() not in (\".jpg\", \".jpeg\", \".png\", \".ppm\", \".bmp\", \".pgm\", \".tif\", \".tiff\", \".webp\"):\n","                continue\n","\n","            # Get label\n","            label = path.parent.name\n","\n","            # Keep track of valid instances\n","            self.instances.append((label, key))\n","\n","        # Sort classes in alphabetical order (as in ImageFolder)\n","        self.classes = sorted(set(label for label, _ in self.instances))\n","        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n","\n","    def __len__(self):\n","        return len(self.instances)\n","\n","    def __getitem__(self, idx):\n","        try:\n","            label, key = self.instances[idx]\n","\n","            # Download image from S3\n","            # response = self.s3_client.get_object(Bucket=self.s3_bucket, Key=key)\n","            # img_bytes = response[\"Body\"]._raw_stream.data\n","\n","            img_bytes = BytesIO()\n","            response = self.s3_client.download_fileobj(Bucket=self.s3_bucket, Key=key, Fileobj=img_bytes) # download each image\n","            # img_bytes = response[\"Body\"]._raw_stream.data\n","\n","            # Open image with PIL\n","            img = Image.open(img_bytes).convert(\"RGB\")\n","\n","            # Apply transformations if any\n","            if self.transform is not None:\n","                img = self.transform(img)\n","        except Exception as e:\n","            raise RuntimeError(f\"Error loading image at index {idx}: {str(e)}\")\n","\n","        return img, self.class_to_idx[label]\n","\n","# Function to create DataLoaders for training and evaluating models.\n","# Loads the dataset from the S3 bucket and optionally splits it into training,\n","# validation, and test sets. It then returns PyTorch DataLoader objects for these datasets.\n","def get_data(batch_size, img_root, seed = None, split_data = False, transform = None):\n","\n","    # Load data\n","    data = S3ImageFolder(root=img_root, transform=transform)\n","\n","    if split_data:\n","        # Create train and test splits (80/20)\n","        num_samples = len(data)\n","        training_samples = int(num_samples * 0.8 + 1)\n","        val_samples = int(num_samples * 0.1)\n","        test_samples = num_samples - training_samples - val_samples\n","\n","        torch.manual_seed(seed)\n","        training_data, val_data, test_data = torch.utils.data.random_split(data, [training_samples, val_samples, test_samples])\n","\n","        # Initialize dataloaders\n","        train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True, num_workers=4)\n","        val_loader = torch.utils.data.DataLoader(val_data, batch_size, shuffle=False, num_workers=4)\n","        test_loader = torch.utils.data.DataLoader(test_data, batch_size, shuffle=False, num_workers=4)\n","\n","        return train_loader, val_loader, test_loader\n","\n","    data_loader = torch.utils.data.DataLoader(data, batch_size, shuffle=False, num_workers=4)\n","    return data_loader"]},{"cell_type":"markdown","metadata":{"id":"DDAbe13ytTbG"},"source":["### **ImageNet-A masking**\n","\n","The **imagenetA_masking.json** file provides a masking for ImageNet-A dataset indices to the standard 1000-class ImageNet output indices used by pre-trained models in PyTorch's torchvision library.\n","\n","Each key in the file corresponds to an index in the standard 1000-class ImageNet output vector. The value associated with each key indicates whether that index should be considered when mapping the 1000-class output to a the smaller set of classes ImageNet-A.\n","\n","A value of -1 indicates that the corresponding index in the 1000-class output should be ignored in the subset of outputs for ImageNet-A.\n","A non-negative integer value indicates that the corresponding index in the 1000-class output should be included in the subset of outputs for ImageNet-A."]},{"cell_type":"markdown","metadata":{"id":"FXKWLOkhWn1N"},"source":["## **Test-time adaptation methods**\n","\n","During the testing phase, Test-Time Adaptation (TTA) methods allow for modifications to be made to the model. This enables the model to adapt to new data distributions, even if it has not encountered them before, thus maintaining a certain level of reliability.\n","\n","Below, we outline the techniques used to enhance the model's performance, providing a brief introduction to each method followed by its implementation."]},{"cell_type":"markdown","metadata":{"id":"hWtwo8X73663"},"source":["### **MEMO: Test Time Robustness via Adaptation and Augmentation<sup>[1]</sup>**\n","\n","In this paper, the authors propose a method called MEMO (Marginal Entropy Minimization with One Test Point) designed to address the problem of robustness in deep neural networks when confronted with distribution shifts or unexpected perturbations. To achieve this, the method employs both adaptation and augmentation strategies.\n","\n","<p align=\"center\">\n","  <img src=\"images/MEMO.png\" width=\"600\" height=\"300\">  \n","</p>\n","\n","Unlike traditional approaches that focus on modifying the training process, MEMO utilizes the information provided by test inputs. It applies data augmentations to a single test input to generate various versions of the input, and from these, it calculates the marginal output distribution. The model's parameters are then updated to minimize the entropy of this marginal distribution. Finally, the model uses these updated parameters to make a prediction on the original test input.\n","\n","This approach allows MEMO to enhance the model's robustness against unseen distribution changes by effectively adapting to new conditions during testing.\n","\n","<br>\n","\n","---\n","**Algorithm 1** Test time robustness via MEMO\n","\n","---\n","\n","**Require:** trained model f<sub>θ</sub>, test point $x$, number of augmentations $B$, learning rate $η$, update rule $G$\n","\n","1. Sample a<sub>1</sub>...a<sub>B</sub> $\\overset{\\text{i.i.d.}}{\\sim}$ $\\mathcal{U}$ ($\\mathcal{A}$) and produce augmented points $\\tilde{\\mathbf{x}}_i = a_i(\\mathbf{x})$ for $i \\in \\{1, \\ldots, B\\}$\n","2. Compute estimate $\\tilde{p} = \\frac{1}{B} \\sum_{i=1}^B p_0(y|\\tilde{\\mathbf{x}}_i) \\approx p_0(y|\\mathbf{x})$ and $\\tilde{\\ell} = H(\\tilde{p}) \\approx \\ell(\\theta; \\mathbf{x})$\n","3. Adapt parameters via update rule $\\theta' \\leftarrow G(\\theta, \\eta, \\tilde{\\ell})$\n","4. Predict $\\hat{y} \\triangleq \\arg \\max_y p_{\\theta'}(y|\\mathbf{x})$\n","---"]},{"cell_type":"code","source":["def compute_entropy(probabilities):\n","    \"\"\"\n","    Takes a tensor of probabilities [1,Classes] and computes the entropy returned as one-dimensional tensor.\n","    \"\"\"\n","    # Ensure probabilities are normalized (sum to 1)\n","    if not torch.isclose(probabilities.sum(), torch.tensor(1.0)):\n","        raise ValueError(\"The probabilities should sum to 1.\")\n","\n","    # Compute entropy\n","    # Adding a small value to avoid log(0) issues\n","    epsilon = 1e-10\n","    probabilities = torch.clamp(probabilities, min=epsilon)\n","    entropy = -torch.sum(probabilities * torch.log(probabilities))\n","\n","    return entropy\n","\n","def get_best_augmentations(probabilities, top_k):\n","    \"\"\"\n","    Takes a tensor of probabilities with dimension [num_augmentations,classes] or [mc_models,num_augmentations,200]\n","    and outputs a tensor containing the probabilities corresponding to the augmentations\n","    with the lowest entropy of dimension [top_k, classes] or [mc_models, top_k, classes].\n","    ----------\n","    top_k: number of augmentations to select\n","    probabilities: a tensor of dimension [num_augmentations,200]\n","    \"\"\"\n","    if probabilities.dim() == 2:\n","        probabilities = probabilities.unsqueeze(0)\n","\n","    # nested list comprehension needed if probabilities is a 3D tensor (MC dropout)\n","    entropies = torch.tensor([[compute_entropy(prob) for prob in prob_set] for prob_set in probabilities])\n","    _, top_k_indices = torch.topk(entropies, top_k, largest=False, sorted=False)\n","    sorted_top_k_indices = torch.stack([indices[torch.argsort(entropies[i, indices])]\n","                                            for i, indices in enumerate(top_k_indices)])\n","    top_k_probabilities = torch.stack([probabilities[i][sorted_top_k_indices[i]]\n","                                        for i in range(probabilities.shape[0])])\n","    if top_k_probabilities.shape[0] == 1:\n","        top_k_probabilities = top_k_probabilities.squeeze(0)\n","\n","    return top_k_probabilities\n","\n","def get_test_augmentations(input, augmentations, num_augmentations, seed_augmentations):\n","    \"\"\"\n","    Takes a tensor image of dimension [C,H,W] and returns a tensor of augmentations of dimension [num_augmentations, C,H,W].\n","    The augmentations are produced by sampling different torchvision.transforms from \"augmentations\".\n","    ----------\n","    input: an image tensor of dimension [C,H,W]\n","    augmentations: a list of torchvision.transforms augmentations\n","    num_augmentations: the number of augmentations to produce\n","    seed_augmentations: seed to reproduce the sampling of augmentations\n","    \"\"\"\n","    torch.manual_seed(seed_augmentations)\n","    random.seed(seed_augmentations)\n","    sampled_augmentations = random.sample(augmentations, num_augmentations)\n","    test_augmentations = torch.zeros((num_augmentations, 3, 224, 224))\n","    for i, augmentation in enumerate(sampled_augmentations):\n","        transform_MEMO = T.Compose([\n","            T.ToPILImage(),\n","            augmentation,\n","            T.ToTensor(),\n","            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","        ])\n","        augmented_input = transform_MEMO(input.cpu())\n","        test_augmentations[i] = augmented_input\n","    return test_augmentations"],"metadata":{"id":"1VxxoHWV84I4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gG6WmGER3-wT"},"source":["### **TTA - Greedy Policy Search: A Simple Baseline for Learnable Test-Time Augmentation<sup>[2]</sup>**\n","\n","In this paper, the authors propose a method called Greedy Policy Search (GPS) for learning test-time data augmentation policies that enhance the performance of machine learning models. The key idea is that data augmentation policies, typically designed for the training phase, can also be learned and optimized during the test phase to improve model performance.\n","\n","<p align=\"center\">\n","  <img src=\"images/GPS.png\" width=\"990\" height=\"300\"/>  \n","</p>\n","\n","The method involves iteratively selecting sub-policies that maximize a chosen performance criterion, such as the calibrated log-likelihood on a validation set. As the name suggests, GPS constructs the augmentation policy in a greedy, step-by-step manner, ensuring that each added sub-policy contributes to performance improvement.\n","\n","This approach offers a simple yet powerful baseline for learning test-time augmentation policies, providing a promising alternative to more complex methods like reinforcement learning or Bayesian optimization, which are traditionally used for training-phase augmentation.\n","\n","<br>\n","\n","---\n","**Algorithm 1** Greedy Policy Search (GPS)\n","\n","---\n","\n","**Require:** Trained neural network $p(y \\mid x, \\theta)$  \n","**Require:** Validation data $X_{val}, y_{val}$  \n","**Require:** Pool size $B$, policy size $T$  \n","**Require:** Prior over sub-policies $p(s)$  \n","\n","$S \\gets \\emptyset$ $\\hspace{2em}$ $\\triangleright$ Pool of candidate sub-policies\n","\n","**for** $i \\gets 1$ **to** $B$ **do**  \n","$\\hspace{2em}$ $s_i \\sim p(s)$  \n","$\\hspace{2em}$ $S \\gets S \\cup \\{s_i\\}$ $\\hspace{6.3em}$ $\\triangleright$ Add $s_i$ to pool  \n","$\\hspace{2em}$ $\\pi^{s_i}_{val} \\gets p(y \\mid s_i(X_{val}), \\theta)$ $\\hspace{2em}$ $\\triangleright$ Predict with $s_i$  \n","**end for**\n","\n","$P \\gets \\emptyset$ $\\hspace{2.8em}$ $\\triangleright$ GPS policy  \n","$\\pi^P_{val} \\gets 0$ $\\hspace{2em}$ $\\triangleright$ Predictions made with GPS policy\n","\n","**for** $t \\gets 1$ **to** $T$ **do**  \n","$\\hspace{0.5em}$ $\\triangleright$ Choose the best sub-policy $s^*$ based on **calibrated log-likelihood** on validation:  \n","$\\hspace{2em}$ $s^* \\gets \\arg\\max_{s \\in S}$ cLL $\\left( \\frac{t-1}{t} \\pi^P_{val} + \\frac{1}{t} \\pi^{s}_{val}; y_{val} \\right)$  \n","\n","$\\hspace{2em}$ $\\pi^P_{val} \\gets \\frac{t-1}{t} \\pi^P_{val} + \\frac{1}{t} \\pi^{s^*}_{val}$ $\\hspace{2em}$ $\\triangleright$ Update predictions  \n","$\\hspace{2em}$ $P \\gets P \\cup \\{s^*\\}$ $\\hspace{5.8em}$ $\\triangleright$ Update policy  \n","**end for**\n","\n","**return** policy $P$\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"1HVxP1v04Bhm"},"source":["### **Adaptive Batch Normalization - Improving robustness against common corruptions by covariate shift adaptation<sup>[3]<sup>**\n","\n","In this paper, the authors investigate methods to improve the robustness of machine learning models trained for computer vision against common image corruptions, such as blurring or compression artifacts. These corruptions often degrade model performance, reducing their effectiveness in real-world applications.\n","\n","Traditional approaches tend to underestimate model robustness in scenarios where models can adapt to corruptions found in multiple unlabeled examples. The authors argue that models should utilize these examples for unsupervised online adaptation, a strategy not commonly employed in current evaluations. Instead of relying on static batch normalization (BN) statistics computed during training, the authors propose that these statistics be dynamically updated by the models using data from corrupted images encountered during testing. This adaptive approach can significantly enhance model performance under real-world conditions."]},{"cell_type":"code","source":["def adaptive_bn_forward(self, input: torch.Tensor):\n","    \"\"\"\n","    Applies an adaptive batch normalization to the input tensor using precomputed running\n","    statistics that are updated in an adaptive manner using Schneider et al. [40] formula:\n","                        mean = N/(N+1)*mean_train + 1/(N+1)*mean_test\n","                        var = N/(N+1)*var_train + 1/(N+1)*var_test\n","    N corresponds to the weight that is given to the statistics of the pre-trained model.\n","    In the implementation, N/(N+1) corresponds to self.prior_strength and can be modified assigning\n","    a float/int to nn.BatchNorm2d.prior_strength.\n","    -----------\n","    input : input tensor of shape [N, C, H, W]\n","    \"\"\"\n","    # compute channel-wise statistics for the input\n","    point_mean = input.mean([0,2,3]).to(device = self.running_mean.device)\n","    point_var = input.var([0,2,3], unbiased=True).to(device = self.running_mean.device)\n","    # BN adaptation\n","    adapted_running_mean = self.prior_strength * self.running_mean + (1 - self.prior_strength) * point_mean\n","    adapted_running_var = self.prior_strength * self.running_var + (1 - self.prior_strength) * point_var\n","    # detach to avoid non-differentiable torch error\n","    adapted_running_mean = adapted_running_mean.detach()\n","    adapted_running_var = adapted_running_var.detach()\n","\n","    return torch.nn.functional.batch_norm(input, adapted_running_mean, adapted_running_var, self.weight, self.bias, False, 0, self.eps)"],"metadata":{"id":"NllCSGPu7X-P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hlq8QGqJ6byL"},"source":["### **Monte Carlo Dropout - Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning<sup>[4]</sup>**\n","\n","In this paper, the authors propose Monte Carlo Dropout, a technique that leverages dropout, a regularization method commonly used during training, to also perform approximate Bayesian inference during the testing phase.\n","\n","During the model training phase, the dropout technique randomly \"drops\" or deactivates a fraction of neurons in the network during each forward pass. The probability of dropping neurons can be controlled using a specific parameter. The aim is to prevent neurons from memorizing specific inputs, thus reducing overfitting and encouraging the network to learn more general representations.\n","\n","However, during the model test phase, dropout is usually turned off, allowing the full network to make predictions. The key idea behind Monte Carlo Dropout is to keep dropout active during the test phase and perform multiple forward passes through the network. The result is that for each forward pass, a different dropout mask is used, which means a random subset of neurons is activated, allowing for different predictions. By averaging these predictions, it is possible to obtain both the final prediction and a measure of the model's uncertainty."]},{"cell_type":"markdown","source":["#### **Dropout positions**\n","\n","The **dropout_positions.json** file is to define the locations within a custom ResNet50 model where dropout layers should be inserted.\n","\n","The dropout layers are incorporated to enhance the proposed method using Monte Carlo Dropout, a technique that improves model robustness and uncertainty estimation. For more details, refer to the **ResNet50Dropout** section."],"metadata":{"id":"WrvcCQL716S2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"diLFBJgTXA8d"},"outputs":[],"source":["class ResNet50Dropout(nn.Module):\n","\"\"\"\n","It creates a version of the ResNet-50 model that integrates dropout layers at\n","various points in the architecture. By using dropout, the model can be trained\n","with a regularization technique that allows for the implementation of\n","Monte Carlo Dropout.\n","----------\n","weights: Optional pre-trained weights for the ResNet-50 model.\n","dropout_rate: The probability of dropping out neurons during training.\n","A value of 0 means no dropout is applied and the architecture is identical to the\n","original ResNet-50.\n","\"\"\"\n","    def __init__(self, weights=None, dropout_rate=0.):\n","        super(ResNet50Dropout, self).__init__()\n","\n","        self.weights = weights\n","        self.model = models.resnet50(weights=self.weights)\n","        self.dropout_rate = dropout_rate\n","\n","        self.dropout_positions = []\n","        if self.dropout_rate > 0:\n","            self.dropout_positions = self.get_dropout_positions()\n","\n","        self._add_dropout()\n","\n","    # This method reads a JSON file that contains a list of layer names where\n","    # dropout should be applied.\n","    def get_dropout_positions(self):\n","        dropout_positions_path = \"/home/sagemaker-user/Domain-Shift-Computer-Vision/utility/data/dropout_positions.json\"\n","        with open(dropout_positions_path, 'r') as json_file:\n","            dropout_positions = json.load(json_file)\n","        dropout_positions = dropout_positions[\"dropout_positions\"]\n","\n","        return dropout_positions\n","\n","    # This method adds dropout layers to the ResNet-50 model at the specified\n","    # positions, by looking at the dropout_positions list.\n","    # For each specified layer, the method wraps the original layer in\n","    # a nn.Sequential block, which includes the original layer followed by a\n","    # nn.Dropout layer with the specified dropout rate.\n","    def _add_dropout(self):\n","        if 'conv1' in self.dropout_positions:\n","            self.model.conv1 = nn.Sequential(\n","                self.model.conv1,\n","                nn.Dropout(p=self.dropout_rate)\n","            )\n","\n","        if 'layer1' in self.dropout_positions:\n","            self.model.layer1 = nn.Sequential(\n","                self.model.layer1,\n","                nn.Dropout(p=self.dropout_rate)\n","            )\n","\n","        if 'layer2' in self.dropout_positions:\n","            self.model.layer2 = nn.Sequential(\n","                self.model.layer2,\n","                nn.Dropout(p=self.dropout_rate)\n","            )\n","\n","        if 'layer3' in self.dropout_positions:\n","            self.model.layer3 = nn.Sequential(\n","                self.model.layer3,\n","                nn.Dropout(p=self.dropout_rate)\n","            )\n","\n","        if 'layer4' in self.dropout_positions:\n","            self.model.layer4 = nn.Sequential(\n","                self.model.layer4,\n","                nn.Dropout(p=self.dropout_rate)\n","            )\n","\n","        if 'avgpool' in self.dropout_positions:\n","            self.model.avgpool = nn.Sequential(\n","                self.model.avgpool,\n","                nn.Dropout(p=self.dropout_rate)\n","            )\n","\n","        if 'fc' in self.dropout_positions:\n","            self.model.fc = nn.Sequential(\n","                nn.Dropout(p=self.dropout_rate),\n","                self.model.fc\n","            )\n","\n","    def forward(self, x):\n","        return self.model(x)"]},{"cell_type":"markdown","metadata":{"id":"6X8HL-vDoMRf"},"source":["### **DiffTPT - Diverse Data Augmentation with Diffusions for Effective Test-time Prompt Tuning<sup>[5]</sup>**\n","\n","Here's a revised and improved version of your text:\n","\n","---\n","\n","In this paper, the authors introduce a novel method called DiffTPT (Diverse Test-time Prompt Tuning) to enhance the performance of vision-language models in scenarios where test samples come from previously unseen domains. This approach is particularly relevant for models like CLIP, which rely on prompt tuning to adapt to new tasks without additional task-specific training data.\n","\n","**Test-time Prompt Tuning (TPT):** TPT refers to the process of adapting the prompts of pre-trained models on the fly during testing, especially when encountering new data that differs from the training distribution. TPT aims to generate adaptive prompts based on each test sample. Typically, this is achieved using two main approaches, either individually or in combination:\n","\n","- **Data augmentation techniques,** which unfortunately often fail to generate sufficiently diverse data.\n","- **Entropy-based confidence selection,** which does not always ensure accurate predictions.\n","\n","<p align=\"center\">\n","  <img src=\"images/DiffTPT.png\" width=\"1137\" height=\"300\"/>\n","</p>\n","\n","Unlike these traditional approaches, DiffTPT leverages pre-trained diffusion models to generate new images. These images typically exhibit a wide range of variations that data augmentation techniques alone cannot achieve. However, this broad variation does not always lead to model improvement due to the varying quality of the generated images.\n","\n","To address this issue and maintain the quality and relevance of the augmented data, the authors introduce a **cosine similarity-based filtration** mechanism. This mechanism ensures that only the most semantically similar augmented samples are selected, effectively balancing diversity and fidelity.\n","\n","Finally, the proposed approach combines both traditional augmentation methods and newly generated images to exploit their respective strengths, providing a richer set of training data during test time. This method enhances the model's ability to handle new, unseen test data effectively, making it a valuable approach for real-world applications where collecting labeled data for every new distribution is impractical."]},{"cell_type":"markdown","source":["#### **Image generation**"],"metadata":{"id":"cb0esIOp9jQb"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"9EbufrbhXD1H","executionInfo":{"status":"ok","timestamp":1724760773543,"user_tz":-120,"elapsed":4,"user":{"displayName":"Andrea Cristiano","userId":"18223328055463124392"}}},"outputs":[],"source":["def get_imagenetA_classes():\n","    \"\"\"\n","    ImageNet-A uses the same label structure as the original ImageNet (ImageNet-1K).\n","    Each class in ImageNet is represented by a synset ID (e.g., n01440764 for \"tench, Tinca tinca\").\n","    This function returns a dictionary that maps the synset IDs of ImageNet-A to the corresponding class names.\n","    ----------\n","    indices_in_1k: list of indices to map [B,1000] -> [B,200]\n","    \"\"\"\n","    imagenetA_classes_path = \"/home/sagemaker-user/Domain-Shift-Computer-Vision/utility/data/imagenetA_classes.json\"\n","    imagenetA_classes_dict = None\n","    with open(imagenetA_classes_path, 'r') as json_file:\n","        imagenetA_classes_dict = json.load(json_file)\n","\n","    # Ensure `class_dict` is a dictionary with keys as class IDs and values as class names\n","    class_dict = {k: v for k, v in imagenetA_classes_dict.items()}\n","    return class_dict\n","\n","def create_dir_generated_images(path):\n","    classes = list(get_imagenetA_classes().values())\n","    for class_name in classes:\n","        class_path = os.path.join(path, class_name)\n","        os.makedirs(class_path, exist_ok=True)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"ithq1PDCXO6R","executionInfo":{"status":"ok","timestamp":1724764249682,"user_tz":-120,"elapsed":2,"user":{"displayName":"Andrea Cristiano","userId":"18223328055463124392"}}},"outputs":[],"source":["class ImageGenerator:\n","    \"\"\"\n","    A class designed to handle the generation of text prompts and images,\n","    specifically for applications involving vision-language models and diffusion\n","    models. It provides methods for generating prompts, creating images based\n","    on those prompts, and retrieving generated images that are similar to a\n","    given input image using a CLIP model.\n","    \"\"\"\n","    def __init__(self):\n","        pass\n","\n","    def get_model(self):\n","        print(self.__model)\n","\n","    def get_text_embedding(self,clip_model, text):\n","        text_token = clip.tokenize(text).cuda()\n","        text_embedding = clip_model.encode_text(text_token).float()\n","        text_embedding /= text_embedding.norm()\n","        return text_embedding\n","\n","    def generate_prompts(self, num_prompts_per_class, style_of_picture, path, context_llm, llm_model = \"llama3.1\", clip_text_encoder = \"ViT-L/14\"):\n","        \"\"\"\n","        This method generates text prompts for image classes. It uses a large\n","        language model (LLM) to create prompts based on the class names and a\n","        specified picture style. The method then generates embeddings for these\n","        prompts using a CLIP model and saves them along with the prompt text to\n","        specific directories. If the LLM fails to generate sufficient prompts,\n","        the class is skipped, and an error message is logged.\n","        \"\"\"\n","        assert isinstance(llm_model,str), \"Model must be a str\"\n","        try:\n","          ollama.chat(llm_model)\n","        except ollama.ResponseError as e:\n","          print('Error:', e.error)\n","          if e.status_code == 404:\n","            print(\"Pulling the model...\")\n","            ollama.pull(llm_model)\n","\n","        if isinstance(context_llm,str):\n","            with open(context_llm, 'r') as file:\n","                context_llm = json.load(file)\n","\n","        skipped_classes = []\n","\n","        clip_model, _ = clip.load(clip_text_encoder)\n","        clip_model.cuda().eval()\n","\n","        class_list = os.listdir(path)\n","        with torch.no_grad():\n","            with tqdm(total=len(class_list), desc=\"Processing classes\") as pbar:\n","                for class_name in class_list:\n","                    pbar.set_description(f\"Processing class: {class_name}\")\n","                    sub_dir_class = os.path.join(path, class_name)\n","                    prompts_to_generate = num_prompts_per_class - len(os.listdir(sub_dir_class) - 1) # -1 to account for scraped_img folder\n","                    if prompts_to_generate <= 0:\n","                        pbar.update(1)\n","                        continue\n","                    counter_flag = 6\n","                    gen_prompts = []\n","                    original_prompts_to_gen = prompts_to_generate\n","                    while counter_flag>0:\n","                        prompts_generation_instruction = {\n","                            \"role\": \"user\",\n","                            \"content\": f\"class:{class_name}, number of prompts:{prompts_to_generate}, style of picture: {style_of_picture}\"\n","                        }\n","                        if len(context_llm) == 3:\n","                            context_llm.append(prompts_generation_instruction)\n","                        else:\n","                            context_llm[3] = prompts_generation_instruction\n","                        try:\n","                            response = ollama.chat(model=llm_model, messages=context_llm)\n","                            content = json.loads(response['message']['content'])  # json.loads to convert str to list\n","                            if len(content) > prompts_to_generate:\n","                                prompts_to_generate -= len(content)\n","                                gen_prompts.extend(content)\n","                                gen_prompts = gen_prompts[:original_prompts_to_gen]\n","                                counter_flag = -1\n","                            else:\n","                                prompts_to_generate -= len(content)\n","                                gen_prompts.extend(content)\n","                        except Exception as e:\n","                            counter_flag -= 1\n","\n","                    if len(gen_prompts) != 1:\n","                        counter_flag = -1\n","\n","                    if counter_flag == -1:\n","                        num_prompts_already_gen = len(os.listdir(sub_dir_class))\n","                        for i in range(num_prompts_already_gen, num_prompts_already_gen + len(gen_prompts)):\n","                            new_sub_dir = os.path.join(path, class_name, str(i))\n","                            os.makedirs(new_sub_dir, exist_ok=True)\n","                            prompt = gen_prompts[i - num_prompts_already_gen]\n","                            prompt_embedding = self.get_text_embedding(clip_model, prompt)\n","                            with open(os.path.join(new_sub_dir, \"prompt.txt\"), 'w') as file:\n","                                file.write(prompt)\n","                            torch.save(prompt_embedding, os.path.join(new_sub_dir,\"prompt_clip_embedding.pt\"))\n","                    else:\n","                        skipped_classes.append(class_name)\n","                        print(f\"Skipping class {class_name}.\")\n","\n","        return skipped_classes\n","\n","    def get_image_embedding(self,clip_model, preprocess, image):\n","        image_preprocessed = preprocess(image).unsqueeze(0).cuda()\n","        image_embedding = clip_model.encode_image(image_preprocessed)\n","        image_embedding /= image_embedding.norm()\n","        return image_embedding\n","\n","    def generate_images(self,\n","                        path,\n","                        num_images,\n","                        image_generation_pipeline,\n","                        num_inference_steps,\n","                        guidance_scale = 9,\n","                        strength=1,\n","                        clip_image_encoder = \"ViT-L/14\"):\n","        \"\"\"\n","        This method generates a specified number of images for each class in the\n","        given path using a diffusion-based image generation pipeline.\n","        It either generates images from text prompts or uses image-to-image\n","        generation if the pipeline supports it. The generated images are\n","        embedded using a CLIP model, and the embeddings are saved along with\n","        the images.\n","        \"\"\"\n","        assert image_generation_pipeline.__class__.__name__ in (\"StableDiffusionPipeline\", \"StableDiffusionImg2ImgPipeline\"), \"image_generation_pipeline must be one of StableDiffusionPipeline or StableDiffusionImg2ImgPipeline\"\n","\n","        random.seed(42)\n","\n","        print(\"Loading CLIP model...\")\n","        clip_model, preprocess = clip.load(clip_image_encoder)\n","        clip_model.cuda().eval()\n","\n","        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n","        for class_name in tqdm(os.listdir(path), desc=\"Processing classes\"):\n","            mem_allocated_before = torch.cuda.memory_allocated()\n","            mem_reserved_before = torch.cuda.memory_reserved()\n","            class_path = os.path.join(path, class_name)\n","            num_prompts = len(os.listdir(class_path))\n","            if image_generation_pipeline.__class__.__name__ == \"StableDiffusionImg2ImgPipeline\":\n","                print(\"Loading scraped images...\")\n","                scraped_image_paths = os.path.join(class_path, \"scraped_images\")\n","                scraped_images = []\n","                for scraped_image_path in scraped_image_paths:\n","                    scraped_image = Image.open(scraped_image_path)\n","                    scraped_image = scraped_image.resize((512,512))\n","                    scraped_images.append(scraped_image)\n","            num_prompts = len(os.listdir(class_path))\n","            n_perm = math.ceil(num_images / num_prompts)\n","            num_gen_images = 0 # needed if num_images < num_prompts\n","            for gen_images_class in os.listdir(class_path):\n","                if num_gen_images == num_images: break\n","                gen_image_class = os.path.join(class_path,gen_images_class)\n","                with open(os.path.join(gen_image_class, \"prompt.txt\"), 'r') as file:\n","                    text_prompt = file.read()\n","                if image_generation_pipeline.__class__.__name__ == \"StableDiffusionImg2ImgPipeline\":\n","                    i2i_image_path = os.path.join(gen_image_class,\"i2i_gen_images\")\n","                    os.makedirs(i2i_image_path,exist_ok=True)\n","                    for _ in range(n_perm):\n","                        scraped_image = random.sample(scraped_images,1)[0]\n","                        with torch.no_grad():\n","                            gen_image = image_generation_pipeline(prompt=text_prompt,\n","                                                                  image=scraped_image,\n","                                                                  strength=strength,\n","                                                                  guidance_scale=guidance_scale,\n","                                                                  num_inference_steps=num_inference_steps).images[0]\n","                            del scraped_image\n","                            gen_image_embedding = self.get_image_embedding(clip_model, preprocess, gen_image)\n","                            save_gen_image_path = os.path.join(i2i_image_path,str(len(os.listdir(i2i_image_path))))\n","                            os.makedirs(save_gen_image_path)\n","                            torch.save(gen_image_embedding, os.path.join(save_gen_image_path, \"image_embedding.pt\"))\n","                            gen_image.save(os.path.join(save_gen_image_path, \"image.png\"))\n","                            num_gen_images += 1\n","                            if num_gen_images == num_images: break\n","                else:\n","                    t2i_image_path = os.path.join(gen_image_class,\"t2i_gen_images\")\n","                    os.makedirs(t2i_image_path,exist_ok=True)\n","                    for _ in range(n_perm):\n","                        with torch.no_grad():\n","                            gen_image = image_generation_pipeline(prompt=text_prompt,\n","                                                                  strength=strength,\n","                                                                  guidance_scale=guidance_scale,\n","                                                                  num_inference_steps=num_inference_steps).images[0]\n","                            gen_image_embedding = self.get_image_embedding(clip_model, preprocess, gen_image)\n","                            save_gen_image_path = os.path.join(t2i_image_path,str(len(os.listdir(t2i_image_path))))\n","                            os.makedirs(save_gen_image_path)\n","                            torch.save(gen_image_embedding, os.path.join(save_gen_image_path, \"image_embedding.pt\"))\n","                            gen_image.save(os.path.join(save_gen_image_path, \"image.png\"))\n","                            num_gen_images += 1\n","                            if num_gen_images == num_images: break\n","\n","def retrieve_gen_images(img,\n","                        num_images,\n","                        clip_model,\n","                        preprocess,\n","                        img_to_tensor_pipe,\n","                        data_path = \"/home/sagemaker-user/Domain-Shift-Computer-Vision/imagenetA_generated\",\n","                        use_t2i_similarity = False,\n","                        t2i_images = True,\n","                        i2i_images = False,\n","                        threshold = 0.):\n","        \"\"\"\n","        This function retrieves a specified number of images from the generated\n","        dataset that are most similar to a given input image. It uses cosine\n","        similarity between the CLIP embeddings of the input image and the\n","        generated images. The function can consider both text-to-image (t2i)\n","        and image-to-image (i2i) generated images based on the provided flags,\n","        and it returns the most similar images as tensors.\n","        \"\"\"\n","        assert i2i_images or t2i_images, \"One of t2i_images and i2i_images must be true\"\n","        assert isinstance(use_t2i_similarity, bool), \"use_t2i_similarity must be a bool\"\n","        assert isinstance(t2i_images, bool), \"t2i_images must be a bool\"\n","        assert isinstance(i2i_images, bool), \"i2i_images must be a bool\"\n","        assert isinstance(num_images, int), \"num_images must be an int\"\n","        assert isinstance(threshold, float) and 0 < threshold < 1, \"threshold must be a float and between 0 and 1\"\n","\n","        if isinstance(img, torch.Tensor):\n","            img = T.ToPILImage()(img)\n","\n","        retrieved_images_paths = []\n","        retrieved_images_similarity = torch.zeros(num_images)\n","        with torch.no_grad():\n","            image_embedding = clip_model.encode_image(preprocess(img).unsqueeze(0).cuda())\n","            image_embedding /= image_embedding.norm()\n","\n","        for class_name in os.listdir(data_path):\n","            class_path = os.path.join(data_path, class_name)\n","            for gen_images_class in os.listdir(class_path):\n","                gen_images_class_path = os.path.join(class_path,gen_images_class)\n","                gen_prompt_embedding = torch.load(os.path.join(gen_images_class_path, \"prompt_clip_embedding.pt\"))\n","                t2i_similarity = F.cosine_similarity(image_embedding, gen_prompt_embedding)\n","                if t2i_images:\n","                    t2i_gen_images_main_path = os.path.join(gen_images_class_path,\"t2i_gen_images\")\n","                    try: # needed bc some prompts don't have a corresponding image yet\n","                        for t2i_images_paths in os.listdir(t2i_gen_images_main_path):\n","                            t2i_image_path = os.path.join(t2i_gen_images_main_path,t2i_images_paths)\n","                            gen_image_embedding = torch.load(os.path.join(t2i_image_path, \"image_embedding.pt\"))\n","                            i2i_similarity = F.cosine_similarity(image_embedding, gen_image_embedding)\n","                            if use_t2i_similarity:\n","                                similarity = (i2i_similarity + t2i_similarity)/2 # avg similarity\n","                            else:\n","                                similarity = i2i_similarity\n","                            if similarity < threshold: continue\n","                            if len(retrieved_images_paths) < num_images:\n","                                retrieved_images_similarity[len(retrieved_images_paths)] = similarity\n","                                retrieved_images_paths.append(os.path.join(t2i_image_path, \"image.png\"))\n","                            else:\n","                                min_similarity, id_similarity = retrieved_images_similarity.min(dim=0)\n","                                if similarity > min_similarity:\n","                                    retrieved_images_similarity[id_similarity] = similarity\n","                                    retrieved_images_paths[id_similarity] = os.path.join(t2i_image_path, \"image.png\")\n","                    except:\n","                        pass\n","                if i2i_images:\n","                    i2i_gen_images_main_path = os.path.join(gen_images_class_path,\"i2i_gen_images\")\n","                    for i2i_images_paths in os.listdir(i2i_gen_images_main_path):\n","                        i2i_image_path = os.path.join(i2i_gen_images_main_path,i2i_images_paths)\n","                        gen_image_embedding = torch.load(os.path.join(i2i_image_path, \"image_embedding.pt\"))\n","\n","                        i2i_similarity = F.cosine_similarity(image_embedding, gen_image_embedding)\n","                        if use_t2i_similarity:\n","                            similarity = (i2i_similarity + t2i_similarity)/2 # avg similarity\n","                        else:\n","                            similarity = i2i_similarity\n","                        if similarity < threshold: continue\n","                        if len(retrieved_images_paths) < num_images:\n","                            retrieved_images_similarity[len(retrieved_images_paths)] = similarity\n","                            retrieved_images_paths.append(os.path.join(t2i_image_path, \"image.png\"))\n","                        else:\n","                            min_similarity, id_similarity = retrieved_images_similarity.min(dim=0)\n","                            if similarity > min_similarity:\n","                                retrieved_images_similarity[id_similarity] = similarity\n","                                retrieved_images_paths[id_similarity] = os.path.join(i2i_image_path, \"image.png\")\n","\n","        retrieved_images = []\n","        for image_path in retrieved_images_paths:\n","            retrieved_images.append(img_to_tensor_pipe(Image.open(image_path)))\n","        retrieved_images = torch.stack(retrieved_images) if len(retrieved_images) >= 1 else None\n","\n","        return retrieved_images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ovsvQOCXUjx"},"outputs":[],"source":["imagenetA_generator = ImageGenerator()"]},{"cell_type":"code","source":["# generate prompts\n","skipped_classes = imagenetA_generator.generate_prompts(\n","    num_prompts_per_class=20,\n","    style_of_picture=\"photograph\",\n","    path=\"/home/sagemaker-user/Domain-Shift-Computer-Vision/imagenetA_generated\",\n","    context_llm = \"/home/sagemaker-user/Domain-Shift-Computer-Vision/test_time_adaptation/image_generation/llm_context.json\",\n","    llm_model = \"llama3.1\",\n","    clip_text_encoder = \"ViT-L/14\"\n",")"],"metadata":{"id":"4J5QQU9G6sSe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# generate images\n","model_id = \"runwayml/stable-diffusion-v1-5\"\n","pipet2i = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n","pipet2i.scheduler = DPMSolverMultistepScheduler.from_config(pipet2i.scheduler.config)\n","pipet2i = pipet2i.to(\"cuda\")"],"metadata":{"id":"jU0i6Nr76uOT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["imagenet_a_generated_path = \"/home/sagemaker-user/Domain-Shift-Computer-Vision/imagenetA_generated\""],"metadata":{"id":"LyGrjw5g6v32"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["imagenetA_generator.generate_images(path = imagenet_a_generated_path,\n","                                    num_images = 1,\n","                                    image_generation_pipeline = pipet2i,\n","                                    num_inference_steps = 25,\n","                                    guidance_scale = 9,\n","                                    strength=1)"],"metadata":{"id":"qr7Vc6S06xzX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Retrieving Images**"],"metadata":{"id":"BaT9e0Vz61Mf"}},{"cell_type":"code","source":["dataloader = get_data(batch_size=32,\n","                      img_root = \"imagenet-a\",\n","                      split_data=False)"],"metadata":{"id":"zxZ-g6y762eg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["candle_img = dataloader.dataset[20][0]"],"metadata":{"id":"BVB7Ny4P7Bu4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clip_image_encoder = \"ViT-L/14\"\n","clip_model, preprocess = clip.load(clip_image_encoder)"],"metadata":{"id":"JubJsxIn7DFK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["imagenet_a_generated_path = \"/home/sagemaker-user/Domain-Shift-Computer-Vision/imagenetA_generated"],"metadata":{"id":"E7pUAkxH7EgX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["retrieved_images = retrieve_gen_images(img = candle_img,\n","                                       num_images = 30,\n","                                       data_path = imagenet_a_generated_path,\n","                                       clip_model = clip_model,\n","                                       preprocess = preprocess,\n","                                       t2i_images = True,\n","                                       use_t2i_similarity = False,\n","                                       threshold = 0.7)"],"metadata":{"id":"UxBUv6Fr7GLv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scrape_images_imagenetA(img_style = \"a photo of\",\n","                        imgenetA_gen_path = \"/home/sagemaker-user/Domain-Shift-Computer-Vision/imagenetA_generated\",\n","                        limit = 5)"],"metadata":{"id":"xpT7oiVJ7H4_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **ImageNet-A classes**\n","\n","The **imagenetA_classes.json** file provides a mapping between the synset IDs used in the ImageNet dataset and their corresponding class names. This mapping is essential for converting model outputs from synset IDs to human-readable labels.\n","\n","The class IDs are mainly used to create the directory structure where newly generated images will be stored, forming a secondary dataset. This dataset can then be utilized for training and inference activities. For more details, refer to the **ToDo** section."],"metadata":{"id":"kkgCqWTC2LGa"}},{"cell_type":"markdown","metadata":{"id":"BVdw53L-qQzS"},"source":["#### **Install and run Ollama**\n","\n","To create the new images, we decided to use a StableDiffusion model to which we provided a list of prompts as input, one for each image to be generated. To generate the prompts, we used an LLM, specifically Llama 3.1. We were able to obtain this model through the Ollama library<sup>[6]</sup>, which provides the model and all the necessary configurations for its use. To get everything needed to download and run Ollama, and therefore Llama 3.1, simply execute the file **install_and_run_ollama.sh**. The script will download and install Ollama, start it, and instantiate Llama 3.1."]},{"cell_type":"markdown","metadata":{"id":"NnC1hwRQwWEj"},"source":["#### **LLM context**\n","\n","The **llm_context.json** file is used to provide context for generating prompts for a text-to-image generator model.\n","\n","Each entry in the file specifies a Role and Content:\n","\n","* **Role**: Specifies who is speaking or interacting in the conversation. It can either be \"system\" (representing the LLM) or \"user\" (representing the person providing input to the LLM).\n","* **Content**: This field contains the actual message or instructions being communicated. It includes system instructions or user-provided input regarding the prompts to be generated.\n","\n","The messages can be of the following types:\n","1.    A message that sets the system's role and context, instructing the LLM on how to generate prompts for a text-to-image generation task.\n","2.    A message that serves as an example input a user might provide. It helps demonstrate how the user specifies the class name, number of prompts, and style of the picture.\n","3    A message that provides an example output from the LLM, demonstrating the kind of response it should generate based on the provided input.\n","\n","In conclusion, the content of the file provides a framework that guides the LLM in understanding the context of the conversation, adhering to rules, and producing the desired output format."]},{"cell_type":"markdown","metadata":{"id":"7gcvLUxbXa8-"},"source":["## **Testing**\n","\n","The Tester class is designed to facilitate the running of experiments involving a deep neural network model. It provides methods to manage various aspects of the experimental setup, including configuring models and optimizers, handling augmentations, computing statistics, and saving results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sGbfzoA-Xjah"},"outputs":[],"source":["class Tester:\n","    \"\"\"\n","    A class to run all the experiments. It stores all the informations to reproduce the experiments in a json file\n","    at exp_path.\n","    \"\"\"\n","    def __init__(self, model, optimizer, exp_path, device):\n","        self.__model = model\n","        self.__optimizer = optimizer\n","        self.__device = device\n","        self.__exp_path = exp_path\n","\n","    def save_result(self, accuracy, path_result, num_augmentations, augmentations, seed_augmentations, top_augmentations, MEMO, num_adaptation_steps, lr_setting, weights, prior_strength, time_test, use_MC, gen_aug_settings):\n","        \"\"\"\n","        Takes all information of the experiment saves it in a json file stored at exp_path\n","        \"\"\"\n","        data = {\n","            \"accuracy\": accuracy,\n","            \"top_augmentations\" : top_augmentations,\n","            \"use_MEMO\" : MEMO,\n","            \"num_adaptation_steps\" : num_adaptation_steps,\n","            \"lr_setting\" : lr_setting,\n","            \"weights\" : weights,\n","            \"num_augmentations\" : num_augmentations,\n","            \"seed_augmentations\": seed_augmentations,\n","            \"augmentations\" : [str(augmentation) for augmentation in augmentations],\n","            \"prior_strength\" : prior_strength,\n","            \"MC\" : use_MC,\n","            \"time_test\" : time_test,\n","            \"gen_aug_settings\" : gen_aug_settings\n","        }\n","        try:\n","            with open(path_result, 'w') as json_file:\n","                json.dump(data, json_file)\n","        except:\n","            print(\"Result were not saved\")\n","\n","    def get_model(self, weights_imagenet, MC):\n","        \"\"\"\n","        Utility function to instantiate a torch model. The argument weights_imagenet should have\n","        a value in accordance with the parameter weights of torchvision.models.\n","        \"\"\"\n","        if MC:\n","            self.__model=ResNet50Dropout(weights=weights_imagenet, dropout_rate=MC['dropout_rate'])\n","            model = self.__model\n","        else:\n","            model = self.__model(weights=weights_imagenet)\n","\n","        model.to(self.__device)\n","        model.eval()\n","        return model\n","\n","    def get_optimizer(self, model, lr_setting:list):\n","        \"\"\"\n","        Utility function to instantiate a torch optimizer.\n","        ----------\n","        lr_setting: must be a list containing either one global lr for the whole model or a dictionary\n","        where each value is a list with a list of parameters' names and a lr for those parameters.\n","        e.g.\n","        lr_setting = [{\n","            \"classifier\" : [[\"fc.weight\", \"fc.bias\"], 0.00025]\n","            }, 0]\n","        lr_setting = [0.00025]\n","        \"\"\"\n","        if len(lr_setting) == 2:\n","            layers_groups = []\n","            lr_optimizer = []\n","            for layers, lr_param_name in lr_setting[0].items():\n","                layers_groups.extend(lr_param_name[0])\n","                params = [param for name, param in model.named_parameters() if name in lr_param_name[0]]\n","                lr_optimizer.append({\"params\":params, \"lr\": lr_param_name[1]})\n","            other_params = [param for name, param in model.named_parameters() if name not in layers_groups]\n","            lr_optimizer.append({\"params\":other_params})\n","            optimizer = self.__optimizer(lr_optimizer, lr = lr_setting[1], weight_decay = 0)\n","        else:\n","            optimizer = self.__optimizer(model.parameters(), lr = lr_setting[0], weight_decay = 0)\n","        return optimizer\n","\n","    def get_imagenetA_masking(self):\n","        \"\"\"\n","        All torchvision models output a tensor [B,1000] with \"B\" being the batch dimension. This function\n","        returns a list of indices to apply to the model's output to use the model on imagenet-A dataset.\n","        ----------\n","        indices_in_1k: list of indices to map [B,1000] -> [B,200]\n","        \"\"\"\n","        imagenetA_masking_path = \"/home/sagemaker-user/Domain-Shift-Computer-Vision/utility/data/imagenetA_masking.json\"\n","        with open(imagenetA_masking_path, 'r') as json_file:\n","            imagenetA_masking = json.load(json_file)\n","        indices_in_1k = [int(k) for k in imagenetA_masking if imagenetA_masking[k] != -1]\n","        return indices_in_1k\n","\n","    def get_monte_carlo_statistics(self, mc_logits):\n","        \"\"\"\n","        Compute mean, median, mode and standard deviation of the Monte Carlo samples.\n","        \"\"\"\n","        statistics = {}\n","        mean_logits = mc_logits.mean(dim=0)\n","        statistics['mean'] = mean_logits\n","\n","        median_logits = mc_logits.median(dim=0).values\n","        statistics['median'] = median_logits\n","\n","        pred_classes = mc_logits.argmax(dim=1)\n","        pred_classes_cpu = pred_classes.cpu().numpy()\n","        mode_predictions, _ = stats.mode(pred_classes_cpu, axis=0)\n","        mode_predictions = torch.tensor(mode_predictions.squeeze(), dtype=torch.long)\n","        statistics['mode'] = mode_predictions\n","\n","        uncertainty = mc_logits.var(dim=0)\n","        statistics['std'] = uncertainty\n","        return statistics\n","\n","    def get_prediction(self, image_tensors, model, masking, TTA = False, top_augmentations = 0, MC = None):\n","        \"\"\"\n","        Takes a tensor of images and outputs a prediction for each image.\n","        ----------\n","        image_tensors: is a tensor of [B,C,H,W] if TTA is used or if both MEMO and TTA are not used, or of dimension [C,H,W]\n","                       if only MEMO is used\n","        masking: a list of indices to map the imagenet1k logits to the one of imagenet-A\n","        top_augmentations: a non-negative integer, if greater than 0 then the \"top_augmentations\" with the lowest entropy are\n","                           selected to make the final prediction\n","        MC: a dictionary containing the number of evaluations using Monte Carlo Dropout and the dropout rate\n","        \"\"\"\n","        if MC:\n","            model.train()  # enable dropout by setting the model to training mode\n","            mc_logits = []\n","            for _ in range(MC['num_samples']):\n","                logits = model(image_tensors)[:,masking] if image_tensors.dim() == 4 else model(image_tensors.unsqueeze(0))[:,masking]\n","                mc_logits.append(logits)\n","            mc_logits = torch.stack(mc_logits, dim=0)\n","            if TTA:\n","                # first mean is over MC samples, second mean is over TTA augmentations\n","                probab_augmentations = F.softmax(mc_logits - mc_logits.max(dim=2, keepdim=True)[0], dim=2)\n","                if top_augmentations:\n","                    probab_augmentations = self.get_best_augmentations(probab_augmentations, top_augmentations)\n","                y_pred = probab_augmentations.mean(dim=0).mean(dim=0).argmax().item()\n","                statistics = self.get_monte_carlo_statistics(probab_augmentations.mean(dim=1))\n","                return y_pred, statistics\n","            statistics = self.get_monte_carlo_statistics(mc_logits)\n","            return statistics['median'].argmax(dim=1), statistics\n","        else:\n","            logits = model(image_tensors)[:,masking] if image_tensors.dim() == 4 else model(image_tensors.unsqueeze(0))[:,masking]\n","            if TTA:\n","                probab_augmentations = F.softmax(logits - logits.max(dim=1)[0][:, None], dim=1)\n","                if top_augmentations:\n","                    probab_augmentations = self.get_best_augmentations(probab_augmentations, top_augmentations)\n","                y_pred = probab_augmentations.mean(dim=0).argmax().item()\n","                return y_pred, None\n","            return logits.argmax(dim=1), None\n","\n","    def compute_entropy(self, probabilities: torch.tensor):\n","        \"\"\"\n","        See MEMO section\n","        \"\"\"\n","        return compute_entropy(probabilities)\n","\n","    def get_best_augmentations(self, probabilities: torch.tensor, top_k: int):\n","        \"\"\"\n","        See MEMO section\n","        \"\"\"\n","        return get_best_augmentations(probabilities, top_k)\n","\n","    def get_test_augmentations(self, input:torch.tensor, augmentations:list, num_augmentations:int, seed_augmentations:int):\n","        \"\"\"\n","        See MEMO section\n","        \"\"\"\n","        return get_test_augmentations(input, augmentations, num_augmentations, seed_augmentations)\n","\n","    def retrieve_generated_images(self, img, num_images, clip_model, preprocess, img_to_tensor_pipe, data_path, use_t2i_similarity, t2i_images, i2i_images, threshold):\n","        \"\"\"\n","        See Image generation\n","        \"\"\"\n","        return retrieve_gen_images(img = img,\n","                                   num_images = num_images,\n","                                   clip_model = clip_model,\n","                                   preprocess = preprocess,\n","                                   img_to_tensor_pipe = img_to_tensor_pipe,\n","                                   data_path = data_path,\n","                                   use_t2i_similarity = use_t2i_similarity,\n","                                   t2i_images = t2i_images,\n","                                   i2i_images = i2i_images,\n","                                   threshold = threshold)\n","\n","    def test(self,\n","             augmentations:list,\n","             num_augmentations:int,\n","             seed_augmentations:int,\n","             img_root:str,\n","             lr_setting:list,\n","             weights_imagenet = None,\n","             dataset = \"imagenetA\",\n","             batch_size = 64,\n","             MEMO = False,\n","             num_adaptation_steps = 0,\n","             top_augmentations = 0,\n","             TTA = False,\n","             prior_strength = -1,\n","             verbose = True,\n","             log_interval = 1,\n","             MC = None,\n","             gen_aug_settings = None):\n","        \"\"\"\n","        Main function to test a torchvision model with different test-time adaptation techniques\n","        and keep track of the results and the experiment setting.\n","        ---\n","        augmentations: list of torchvision.transforms functions.\n","        num_augmentations: the number of augmentations to use for each sample to perform test-time adaptation.\n","        seed_augmentations: seed to reproduce the sampling of augmentations.\n","        img_root: str path to get a dataset in a torch format.\n","        lr_setting: list with lr instructions to adapt the model. See \"get_optimizer\" for more details.\n","        weights_imagenet: weights_imagenet should have a value in accordance with the parameter\n","                          weights of torchvision.models.\n","        dataset: the name of the dataset to use. Note: this parameter doesn't directly control the data\n","                 used, it's only used to use the right masking to map the models' outputs to the right dimensions.\n","                 At the moment only Imagenet-A masking is supported.\n","        MEMO: a boolean to use marginal entropy minimization with one test point\n","        TTA: a boolean to use test time augmentation\n","        top_augmentations: if MEMO or TTA are set to True, then values higher than zero select the top_augmentations\n","                           with the lowest entropy (highest confidence).\n","        prior_strength: defines the weight given to pre-trained statistics in BN adaptation. If negative, then no BN\n","                        adaptation is applied.\n","        verbose: use loading bar to visualize accuracy and number of batch during testing.\n","        log_interval: defines after how many batches a new accuracy should be displayed. Default is 1, thus\n","                      after each batch a new value is displayed.\n","        num_adaptation_steps: (TODO)\n","        MC: dictionary containing the number of evaluations using Monte Carlo Dropout and the dropout rate.\n","\t\t    gen_aug_settings\n","        \"\"\"\n","        # check some basic conditions\n","        assert bool(num_adaptation_steps) == MEMO, \"When using MEMO adaptation steps should be >= 1, otherwise equal to 0.\"\n","        if not (MEMO or TTA):\n","            assert not (num_augmentations or top_augmentations), \"If both MEMO and TTA are set to False, then top_augmentations and num_augmentations must be 0\"\n","        assert not lr_setting if not MEMO else True, \"If MEMO is false, then lr_setting must be None\"\n","        assert isinstance(prior_strength, (float,int)) , \"Prior adaptation must be either a float or an int\"\n","        assert isinstance(gen_aug_settings, dict), \"gen_aug_settings must be a dict containing settings to retrieve the generated images\"\n","\n","        # get the name of the weigths used and define the name of the experiment\n","        weights_name = str(weights_imagenet).split(\".\")[-1] if weights_imagenet else \"MEMO_repo\"\n","        use_MC = True if MC else False\n","        name_result = f\"MEMO_{MEMO}_AdaptSteps_{num_adaptation_steps}_adaptBN_{prior_strength}_TTA_{TTA}_aug_{num_augmentations}_topaug_{top_augmentations}_seed_aug_{seed_augmentations}_weights_{weights_name}_MC_{use_MC}_genAug_{bool(gen_aug_settings)}\"\n","        path_result = os.path.join(self.__exp_path,name_result)\n","        assert not os.path.exists(path_result),f\"MEMO test already exists: {path_result}\"\n","\n","        # in case of using dropout, check if the model is a ResNet50Dropout and the parameters are correct\n","        if MC:\n","            assert isinstance(self.__model, ResNet50Dropout), f\"To use dropout the model must be a ResNet50Dropout\"\n","            assert MC['num_samples'] > 1, f\"To use dropout the number of samples must be greater than 1\"\n","\n","        # transformation pipeline used in ResNet-50 original training\n","        transform_loader = T.Compose([\n","            T.Resize(256),\n","            T.CenterCrop(224),\n","            T.ToTensor()\n","        ])\n","\n","        # to use after model's update\n","        normalize_input = T.Compose([\n","                        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","                    ])\n","\n","        test_loader = get_data(batch_size, img_root, transform = transform_loader, split_data=False)\n","        model = self.get_model(weights_imagenet, MC)\n","\n","        # if MEMO is used, create a checkpoint to reload after each model and optimizer update\n","        if MEMO:\n","            optimizer = self.get_optimizer(model = model, lr_setting = lr_setting)\n","            MEMO_checkpoint_path = os.path.join(self.__exp_path,\"checkpoint.pth\")\n","            torch.save({\n","                'model': model.state_dict(),\n","                'optimizer': optimizer.state_dict(),\n","            }, MEMO_checkpoint_path)\n","            MEMO_checkpoint = torch.load(MEMO_checkpoint_path)\n","\n","        if dataset == \"imagenetA\":\n","            imagenetA_masking = self.get_imagenetA_masking()\n","\n","        if gen_aug_settings:\n","            clip_model, preprocess = clip.load(gen_aug_settings[\"clip_img_encoder\"])\n","\n","        if prior_strength < 0:\n","            torch.nn.BatchNorm2d.prior_strength = 1\n","        else:\n","            torch.nn.BatchNorm2d.prior_strength = prior_strength / (prior_strength + 1)\n","            torch.nn.BatchNorm2d.forward = adaptive_bn_forward\n","\n","        # Initialize a dictionary to store accumulated time for each step\n","        time_dict = {\n","            \"MEMO_update\": 0.0,\n","            \"get_augmentations\": 0.0,\n","            \"confidence_selection\": 0.0,\n","            \"get_prediction\": 0.0,\n","            \"get_gen_images\" : 0.0,\n","            \"total_time\": 0.0\n","        }\n","\n","        samples = 0.0\n","        cumulative_accuracy = 0.0\n","\n","        for batch_idx, (inputs, targets) in enumerate(test_loader):\n","            inputs, targets = inputs.to(self.__device), targets.to(self.__device)\n","            if MEMO or TTA:\n","                for input, target in zip(inputs, targets):\n","                    if MEMO:\n","                        model.load_state_dict(MEMO_checkpoint['model'])\n","                        model.eval()\n","                        optimizer.load_state_dict(MEMO_checkpoint['optimizer'])\n","\n","                    # get normalized augmentations\n","                    start_time_augmentations = time.time()\n","                    test_augmentations = self.get_test_augmentations(input, augmentations, num_augmentations, seed_augmentations)\n","                    end_time_augmentations = time.time()\n","                    time_dict[\"get_augmentations\"] += (end_time_augmentations - start_time_augmentations)\n","\n","                    if gen_aug_settings:\n","                        start_time_gen_augmentations = time.time()\n","                        retrieved_gen_images = self.retrieve_generated_images(img = input,\n","                                                                              num_images = gen_aug_settings[\"num_img\"],\n","                                                                              clip_model = clip_model,\n","                                                                              preprocess = preprocess,\n","                                                                              img_to_tensor_pipe = transform_loader,\n","                                                                              data_path = gen_aug_settings[\"gen_data_path\"],\n","                                                                              use_t2i_similarity = gen_aug_settings[\"use_t2i_similarity\"],\n","                                                                              t2i_images = gen_aug_settings[\"t2i_img\"],\n","                                                                              i2i_images = gen_aug_settings[\"i2i_img\"],\n","                                                                              threshold = gen_aug_settings[\"threshold\"])\n","                        if retrieved_gen_images:\n","                            test_augmentations = torch.cat([test_augmentations,retrieved_gen_images],dim=0)\n","                        end_time_gen_augmentations = time.time()\n","                        time_dict[\"get_gen_images\"] += (end_time_gen_augmentations - start_time_gen_augmentations)\n","\n","                    test_augmentations = test_augmentations.to(self.__device)\n","\n","                    for _ in range(num_adaptation_steps):\n","                        logits = model(test_augmentations)\n","\n","                        # apply imagenetA masking\n","                        if dataset == \"imagenetA\":\n","                            logits = logits[:, imagenetA_masking]\n","                        # compute stable softmax\n","                        probab_augmentations = F.softmax(logits - logits.max(dim=1)[0][:, None], dim=1)\n","\n","                        # confidence selection for augmentations\n","                        if top_augmentations:\n","                            start_time_confidence_selection = time.time()\n","                            probab_augmentations = self.get_best_augmentations(probab_augmentations, top_augmentations)\n","                            end_time_confidence_selection = time.time()\n","                            time_dict[\"confidence_selection\"] += (end_time_confidence_selection - start_time_confidence_selection)\n","\n","                        if MEMO:\n","                            start_time_memo_update = time.time()\n","                            marginal_output_distribution = torch.mean(probab_augmentations, dim=0)\n","                            marginal_loss = self.compute_entropy(marginal_output_distribution)\n","                            marginal_loss.backward()\n","                            optimizer.step()\n","                            optimizer.zero_grad()\n","                            end_time_memo_update = time.time()\n","                            time_dict[\"MEMO_update\"] += (end_time_memo_update - start_time_memo_update)\n","\n","                    start_time_prediction = time.time()\n","                    with torch.no_grad():\n","                        if TTA:\n","                            # statistics:\n","                            # dictionary containing statistics resulting from the application of monte carlo dropout\n","                            # look at get_monte_carlo_statistics() for more details\n","                            y_pred, statistics = self.get_prediction(test_augmentations, model, imagenetA_masking, TTA, top_augmentations, MC=MC)\n","                        else:\n","                            input = normalize_input(input)\n","                            y_pred, statistics = self.get_prediction(input, model, imagenetA_masking, MC=MC)\n","                        cumulative_accuracy += int(target == y_pred)\n","                    end_time_prediction = time.time()\n","                    time_dict[\"get_prediction\"] += (end_time_prediction - start_time_prediction)\n","            else:\n","                start_time_prediction = time.time()\n","                with torch.no_grad():\n","                    inputs = normalize_input(inputs)\n","                    y_pred, _ = self.get_prediction(inputs, model, imagenetA_masking, MC=MC)\n","                    correct_predictions = (targets == y_pred).sum().item()\n","                    cumulative_accuracy += correct_predictions\n","                end_time_prediction = time.time()\n","                time_dict[\"get_prediction\"] += (end_time_prediction - start_time_prediction)\n","\n","            samples += inputs.shape[0]\n","\n","            if verbose and batch_idx % log_interval == 0:\n","                current_accuracy = cumulative_accuracy / samples * 100\n","                print(f'Batch {batch_idx}/{len(test_loader)}, Accuracy: {current_accuracy:.2f}%', end='\\r')\n","\n","        accuracy = cumulative_accuracy / samples * 100\n","        time_dict[\"total_time\"] += sum(time_dict.values())\n","\n","        self.save_result(accuracy = accuracy,\n","                         path_result = path_result,\n","                         seed_augmentations = seed_augmentations,\n","                         num_augmentations = num_augmentations,\n","                         augmentations = augmentations,\n","                         top_augmentations = top_augmentations,\n","                         MEMO = MEMO,\n","                         num_adaptation_steps = num_adaptation_steps,\n","                         lr_setting = lr_setting,\n","                         weights = weights_name,\n","                         prior_strength = prior_strength,\n","                         use_MC = use_MC,\n","                         time_test = time_dict,\n","                         gen_aug_settings = gen_aug_settings)\n","\n","        return accuracy"]},{"cell_type":"markdown","source":["To run the solution, simply execute the following code sections."],"metadata":{"id":"0v-lB2Zr49Ml"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"kbV1_cGTXoJI"},"outputs":[],"source":["imagenet_a_path = \"imagenet-a\"\n","imagenet_b_path = \"imagenetv2-matched-frequency-format-val/\""]},{"cell_type":"code","source":["device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"],"metadata":{"id":"ePNiGdZh5eOG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","augmentations = [\n","    T.RandomHorizontalFlip(p=1),\n","    T.RandomVerticalFlip(p=1),\n","    T.RandomRotation(degrees=30),\n","    T.RandomRotation(degrees=60),\n","    T.ColorJitter(brightness=0.2),\n","    T.ColorJitter(contrast=0.2),\n","    T.ColorJitter(saturation=0.2),\n","    T.ColorJitter(hue=0.2),\n","    T.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n","    T.RandomRotation(degrees=15),\n","    T.RandomAdjustSharpness(sharpness_factor=2, p=1),\n","    T.RandomGrayscale(p=1),\n","    T.RandomInvert(p=1),\n","    T.RandomAutocontrast(p=1),\n","    T.GaussianBlur(kernel_size=5),\n","]\n","\n","augmix_augmentations = [\n","    T.AugMix(severity=3, mixture_width=3, chain_depth=3, alpha=1.0),\n","    T.AugMix(severity=2, mixture_width=2, chain_depth=3, alpha=1.0),\n","    T.AugMix(severity=4, mixture_width=4, chain_depth=3, alpha=1.0),\n","    T.AugMix(severity=3, mixture_width=3, chain_depth=3, alpha=1.0),\n","    T.AugMix(severity=2, mixture_width=2, chain_depth=3, alpha=1.0),\n","    T.AugMix(severity=4, mixture_width=4, chain_depth=3, alpha=1.0),\n","    T.AugMix(severity=3, mixture_width=3, chain_depth=3, alpha=1.0),\n","    T.AugMix(severity=2, mixture_width=2, chain_depth=3, alpha=1.0),\n","    T.AugMix(severity=4, mixture_width=4, chain_depth=3, alpha=1.0),\n","    T.AugMix(severity=3, mixture_width=3, chain_depth=3, alpha=1.0),\n","    T.AugMix(severity=2, mixture_width=2, chain_depth=3, alpha=1.0),\n","    T.AugMix(severity=4, mixture_width=4, chain_depth=3, alpha=1.0),\n","    T.AugMix(severity=3, mixture_width=3, chain_depth=3, alpha=1.0),\n","    T.AugMix(severity=2, mixture_width=2, chain_depth=3, alpha=1.0),\n","    T.AugMix(severity=4, mixture_width=4, chain_depth=3, alpha=1.0),\n","    T.AugMix(severity=3, mixture_width=3, chain_depth=3, alpha=1.0),\n","    T.AugMix(severity=2, mixture_width=2, chain_depth=3, alpha=1.0),\n","    T.AugMix(severity=4, mixture_width=4, chain_depth=3, alpha=1.0)\n","]"],"metadata":{"id":"1npyJMFH5gK_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Resnet50"],"metadata":{"id":"oyy0zt7Y5mIv"}},{"cell_type":"code","source":["exp_path_a = \"/home/sagemaker-user/Domain-Shift-Computer-Vision/experiments/Resnet50_ImagenetA_SGD\""],"metadata":{"id":"LZtqROYM5uR_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MC = {\n","\t\"dropout_rate\": 0.3,\n","\t\"num_samples\": 10,\n","\t\"use_dropout\": False\n","}"],"metadata":{"id":"5aRVpU7l5wbX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tester_resnet50 = Tester(\n","    model = ResNet50Dropout() if MC['use_dropout'] else models.resnet50,\n","    optimizer = torch.optim.SGD,\n","    exp_path = exp_path_a,\n","    device = device\n",")"],"metadata":{"id":"yqmFK6qT5x4w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#lr_setting = [{\n","#    \"classifier\" : [[\"fc.weight\", \"fc.bias\"], 0.00025]\n","#}, 0]\n","lr_setting_sgd = [0.00025] # setting used in MEMO paper for SGD\n","lr_setting_adam = [0.0001] # setting used in MEMO paper for ADAM"],"metadata":{"id":"ZwfV76pE50q-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["imagenetV1_weights = models.ResNet50_Weights.IMAGENET1K_V1 # MEMO paper used these weights\n","imagenetV2_weights = models.ResNet50_Weights.IMAGENET1K_V2"],"metadata":{"id":"qAR1-n6A52LG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gen_aug_settings = {\n","    \"clip_img_encoder\" : \"ViT-L/14\",\n","    \"num_img\" : 30,\n","    \"gen_data_path\" : \"/home/sagemaker-user/Domain-Shift-Computer-Vision/imagenetA_generated\",\n","    \"use_t2i_similarity\" : True,\n","    \"t2i_img\" : True,\n","    \"i2i_img\" : False,\n","    \"threshold\" : 0.7\n","}"],"metadata":{"id":"zBGUSsHr54em"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tester_resnet50.test(\n","     augmentations = augmix_augmentations,\n","     num_augmentations = 16,\n","     seed_augmentations = 42,\n","     batch_size = 64,\n","     img_root = imagenet_a_path,\n","     num_adaptation_steps = 2,\n","     MEMO = True,\n","     lr_setting = lr_setting_sgd,\n","     top_augmentations = 0, # if using gen_aug run with 0 bc otherwise might not be used at all\n","     weights_imagenet = imagenetV1_weights,\n","     prior_strength = 16,\n","     TTA = True,\n","     MC = None,\n","     gen_aug_settings = gen_aug_settings\n",")"],"metadata":{"id":"Q5FtcFC_56ZO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zwzAwcHxXtKG"},"source":["## **Results**\n","\n","<table>\n","  <thead>\n","    <tr>\n","      <th rowspan=\"1\">Experiment</th>\n","      <th rowspan=\"1\">Dataset</th>\n","      <th rowspan=\"1\">Base Model</th>\n","      <th rowspan=\"1\">Weights</th>\n","      <th rowspan=\"1\", colspan=\"2\">Optimizer</th>\n","      <th rowspan=\"1\">Optimization Steps</th>\n","      <th rowspan=\"1\", colspan=\"3\">Augmentations</th>\n","      <th rowspan=\"1\">Batch Size</th>\n","      <th rowspan=\"1\">MEMO</th>\n","      <th rowspan=\"1\">Confidence Selection</th>\n","      <th rowspan=\"1\", colspan=\"2\">BN</th>\n","      <th rowspan=\"1\">TTA</th>\n","      <th rowspan=\"1\", colspan=\"3\">MC</th>\n","      <th rowspan=\"1\">Accuracy</th>\n","      <th rowspan=\"1\">Execution Time</th>\n","    </tr>\n","    <tr>\n","      <th>Nr.</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th>Type</th>\n","      <th>LR</th>\n","      <th>Nr.</th>\n","      <th>Type</th>\n","      <th>Number</th>\n","      <th>Seed</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th>Prior Strength</th>\n","      <th></th>\n","      <th></th>\n","      <th>Dropout rate</th>\n","      <th>Nr. Samples</th>\n","      <th>%</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody align=\"center\">\n","    <tr>\n","      <th>1</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>SGD</td>\n","      <td>-</td>\n","      <td>1</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>64</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>0.026</td>\n","      <td>00:00:20</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>SGD</td>\n","      <td>-</td>\n","      <td>1</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>False</td>\n","      <td>8</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>0.253</td>\n","      <td>00:26:20</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>SGD</td>\n","      <td>-</td>\n","      <td>1</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>64</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>0.20</td>\n","      <td>10</td>\n","      <td>1.613</td>\n","      <td>00:03:48</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>SGD</td>\n","      <td>-</td>\n","      <td>1</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>64</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>True</td>\n","      <td>16</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>0.026</td>\n","      <td>00:00:23</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>SGD</td>\n","      <td>0.00025</td>\n","      <td>1</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>0.16</td>\n","      <td>00:33:20</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>SGD</td>\n","      <td>0.00025</td>\n","      <td>1</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>True</td>\n","      <td>16</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>0.853</td>\n","      <td>00:38:44</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>SGD</td>\n","      <td>0.00025</td>\n","      <td>1</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>True</td>\n","      <td>16</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>0.20</td>\n","      <td>10</td>\n","      <td>1.053</td>\n","      <td>01:43:29</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>SGD</td>\n","      <td>0.00025</td>\n","      <td>4</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>0.213</td>\n","      <td>00:42:03</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>SGD</td>\n","      <td>0.00025</td>\n","      <td>4</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>True</td>\n","      <td>16</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>0.853</td>\n","      <td>00:47:37</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>SGD</td>\n","      <td>0.00025</td>\n","      <td>4</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>True</td>\n","      <td>16</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>0.20</td>\n","      <td>10</td>\n","      <td>0.95</td>\n","      <td>01:49:42</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>ADAM</td>\n","      <td>0.0001</td>\n","      <td>1</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>0.12</td>\n","      <td>00:31:45</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>ADAM</td>\n","      <td>0.0001</td>\n","      <td>1</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>True</td>\n","      <td>16</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>0.826</td>\n","      <td>00:37:37</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>ADAM</td>\n","      <td>0.0001</td>\n","      <td>1</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>True</td>\n","      <td>16</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>0.20</td>\n","      <td>10</td>\n","      <td>1.506</td>\n","      <td>01:39:47</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>ADAM</td>\n","      <td>0.0001</td>\n","      <td>4</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>0.213</td>\n","      <td>00:41:19</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>ADAM</td>\n","      <td>0.0001</td>\n","      <td>4</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>True</td>\n","      <td>16</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>0.826</td>\n","      <td>00:46:55</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>ImageNet-A</td>\n","      <td>ResNet50</td>\n","      <td>Imagenet_1K_V1</td>\n","      <td>ADAM</td>\n","      <td>0.0001</td>\n","      <td>4</td>\n","      <td>AugMix</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>64</td>\n","      <td>True</td>\n","      <td>8</td>\n","      <td>True</td>\n","      <td>16</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>0.20</td>\n","      <td>10</td>\n","      <td>1.04</td>\n","      <td>01:44:57</td>\n","    </tr>  \n","  </tbody>\n","</table>\n"]},{"cell_type":"markdown","metadata":{"id":"J67VmQSzXvXe"},"source":["## **Discussion**\n","\n","Discussione dei risultati"]},{"cell_type":"markdown","metadata":{"id":"9BY3pvNJXwzd"},"source":["## **Conclusion**\n","\n","wrap up di quello fatto e possibili alternative per future sperimentazioni"]},{"cell_type":"markdown","metadata":{"id":"vkWfJE_fXhvw"},"source":["## **Bibliography**\n","\n","1. **Marvin Zhang and Sergey Levine and Chelsea Finn.** \"MEMO: Test Time Robustness via Adaptation and Augmentation.\" Advances in neural information processing systems, Vol. 35, 2021, pp. 38629-38642. [https://arxiv.org/abs/2110.09506](https://arxiv.org/abs/2110.09506).\n","\n","2. **Lyzhov, Alexander and Molchanova, Yuliya and Ashukha, Arsenii and Molchanov, Dmitry and Vetrov, Dmitry.** \"Greedy Policy Search: A Simple Baseline for Learnable Test-Time Augmentation.\" Proceedings of Machine Learning Research, Vol. 124, 2020, pp. 1308-1317. [https://proceedings.mlr.press/v124/lyzhov20a.html](https://proceedings.mlr.press/v124/lyzhov20a.html).\n","\n","3. **Schneider, Steffen and Rusak, Evgenia and Eck, Luisa and Bringmann, Oliver and Brendel, Wieland and Bethge, Matthias.** \"Improving robustness against common corruptions by covariate shift adaptation.\" Advances in Neural Information Processing Systems, Vol. 33, 2020, pp. 11539-11551. [https://proceedings.neurips.cc/paper_files/paper/2020/file/85690f81aadc1749175c187784afc9ee-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/85690f81aadc1749175c187784afc9ee-Paper.pdf).\n","\n","4. **Gal, Yarin and Ghahramani, Zoubin** \"Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning.\" Proceedings of The 33rd International Conference on Machine Learning, Vol. 48, 2016, pp. 1050-1059. [https://proceedings.mlr.press/v48/gal16.html](https://proceedings.mlr.press/v48/gal16.html).\n","\n","5. **Feng, Chun-Mei and Yu, Kai and Liu, Yong and Khan, Salman and Zuo, Wangmeng.** \"Diverse Data Augmentation with Diffusions for Effective Test-time Prompt Tuning.\" 2023 IEEE/CVF International Conference on Computer Vision (ICCV), 2023, pp. 2704-2714. [https://arxiv.org/abs/2308.06038](https://arxiv.org/abs/2308.06038).\n","\n","6. [Ollama library](https://ollama.com/)\n","\n","\n","\n"]}],"metadata":{"colab":{"collapsed_sections":["qLwYsF2LtEI6","hWtwo8X73663","1HVxP1v04Bhm","WrvcCQL716S2","cb0esIOp9jQb","7gcvLUxbXa8-"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}