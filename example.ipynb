{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1ddb39b-484b-40a8-a6e1-763d3071ddb9",
   "metadata": {},
   "source": [
    "# Test-Time Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1851005d-1fc7-4abd-a04f-5550b7067939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "from test_methods.test import Tester\n",
    "from test_time_adaptation.resnet50_dropout import ResNet50Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "990934cc-7795-415b-adbd-17227bed3510",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_a_path = \"imagenet-a\"\n",
    "imagenet_b_path = \"imagenetv2-matched-frequency-format-val/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ace4045-d733-4897-95fd-f667a42acc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31a14b6b-c730-4fd5-a9a9-6607e098ddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "augmentations = [\n",
    "    T.RandomHorizontalFlip(p=1),\n",
    "    T.RandomVerticalFlip(p=1),\n",
    "    T.RandomRotation(degrees=30),\n",
    "    T.RandomRotation(degrees=60),\n",
    "    T.ColorJitter(brightness=0.2),\n",
    "    T.ColorJitter(contrast=0.2),\n",
    "    T.ColorJitter(saturation=0.2),\n",
    "    T.ColorJitter(hue=0.2),\n",
    "    T.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    T.RandomRotation(degrees=15),\n",
    "    T.RandomAdjustSharpness(sharpness_factor=2, p=1),\n",
    "    T.RandomGrayscale(p=1),\n",
    "    T.RandomInvert(p=1),\n",
    "    T.RandomAutocontrast(p=1),\n",
    "    T.GaussianBlur(kernel_size=5),\n",
    "]\n",
    "\n",
    "augmix_augmentations = [\n",
    "    T.AugMix(severity=3, mixture_width=3, chain_depth=3, alpha=1.0),\n",
    "    T.AugMix(severity=2, mixture_width=2, chain_depth=3, alpha=1.0),\n",
    "    T.AugMix(severity=4, mixture_width=4, chain_depth=3, alpha=1.0),\n",
    "    T.AugMix(severity=3, mixture_width=3, chain_depth=3, alpha=1.0),\n",
    "    T.AugMix(severity=2, mixture_width=2, chain_depth=3, alpha=1.0),\n",
    "    T.AugMix(severity=4, mixture_width=4, chain_depth=3, alpha=1.0),\n",
    "    T.AugMix(severity=3, mixture_width=3, chain_depth=3, alpha=1.0),\n",
    "    T.AugMix(severity=2, mixture_width=2, chain_depth=3, alpha=1.0),\n",
    "    T.AugMix(severity=4, mixture_width=4, chain_depth=3, alpha=1.0),\n",
    "    T.AugMix(severity=3, mixture_width=3, chain_depth=3, alpha=1.0),\n",
    "    T.AugMix(severity=2, mixture_width=2, chain_depth=3, alpha=1.0),\n",
    "    T.AugMix(severity=4, mixture_width=4, chain_depth=3, alpha=1.0),\n",
    "    T.AugMix(severity=3, mixture_width=3, chain_depth=3, alpha=1.0),\n",
    "    T.AugMix(severity=2, mixture_width=2, chain_depth=3, alpha=1.0),\n",
    "    T.AugMix(severity=4, mixture_width=4, chain_depth=3, alpha=1.0),\n",
    "    T.AugMix(severity=3, mixture_width=3, chain_depth=3, alpha=1.0),\n",
    "    T.AugMix(severity=2, mixture_width=2, chain_depth=3, alpha=1.0),\n",
    "    T.AugMix(severity=4, mixture_width=4, chain_depth=3, alpha=1.0)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b444fa-d1e4-412d-814f-36ba12981ff0",
   "metadata": {},
   "source": [
    "## Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e752756-e914-48e7-a203-21ae62dba5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_path_a = \"/home/sagemaker-user/Domain-Shift-Computer-Vision/experiments/Resnet50_ImagenetA_SGD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cf183bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MC = {\n",
    "\t\"dropout_rate\": 0.5,\n",
    "\t\"num_samples\": 10,\n",
    "\t\"use_dropout\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c522c6a-84b6-453d-8921-5a9af1e9453e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_resnet50 = Tester(\n",
    "    model = ResNet50Dropout() if MC['use_dropout'] else models.resnet50,\n",
    "    optimizer = torch.optim.SGD,\n",
    "    exp_path = exp_path_a,\n",
    "    device = device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c6041ec-e1e3-4d1a-a419-f86a79c1ac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_setting = [{\n",
    "#    \"classifier\" : [[\"fc.weight\", \"fc.bias\"], 0.00025]    \n",
    "#}, 0]\n",
    "lr_setting_sgd = [0.00025] # setting used in MEMO paper for SGD\n",
    "lr_setting_adam = [0.0001] # setting used in MEMO paper for ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "200a4ed2-9d24-4de5-8da4-01c8ade487b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenetV1_weights = models.ResNet50_Weights.IMAGENET1K_V1 # MEMO paper used these weights\n",
    "imagenetV2_weights = models.ResNet50_Weights.IMAGENET1K_V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bb98e4-9b44-40b6-b343-3599f1eaff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_resnet50.test(\n",
    "     augmentations = augmix_augmentations, \n",
    "     num_augmentations = 16,\n",
    "     seed_augmentations = 42,\n",
    "     batch_size = 64, \n",
    "     img_root = imagenet_a_path,\n",
    "     MEMO = False,\n",
    "     lr_setting = None,\n",
    "     top_augmentations = 8,\n",
    "     weights_imagenet = imagenetV1_weights,\n",
    "     prior_strength = 0.,\n",
    "     TTA = True,\n",
    "     MC = MC\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5a3855-f888-4eea-af94-77a123dc58a2",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f230e3f-5566-4d7c-863c-835a5ff5a6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c37523-1bff-415c-b483-523e0b4a15ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c46a892a-b689-41cf-9a1a-c208df0f70d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-4xzrocls\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-4xzrocls\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting ftfy (from clip==1.0)\n",
      "  Using cached ftfy-6.2.3-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (23.2)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (4.66.4)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2.0.0.post104)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (0.15.2a0+ab7b3e6)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (1.13.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.4)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.26.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (2.32.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
      "Using cached ftfy-6.2.3-py3-none-any.whl (43 kB)\n",
      "Building wheels for collected packages: clip\n",
      "  Building wheel for clip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369492 sha256=248665e3e232817c7bbfbff0485258b40b84b09df5d0833b1a194b61c074ead2\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-hfdf6mod/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
      "Successfully built clip\n",
      "Installing collected packages: ftfy, clip\n",
      "Successfully installed clip-1.0 ftfy-6.2.3\n"
     ]
    }
   ],
   "source": [
    "! pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c24db59-82a3-4852-b4dc-2889d703150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62432a84-d1d3-45e3-8233-bca007cf3bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_time_adaptation.image_generation.image_generator import ImageGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbb6faec-3a9b-46d7-be72-825f01487e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenetA_generator = ImageGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032714e4-de94-47c2-b52f-e4f7e7142f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate prompts\n",
    "skipped_classes = imagenetA_generator.generate_prompts(\n",
    "    num_prompts=2,\n",
    "    style_of_picture=\"photograph\",\n",
    "    path=\"/home/sagemaker-user/Domain-Shift-Computer-Vision/imagenetA_generated\",\n",
    "    context_llm = \"/home/sagemaker-user/Domain-Shift-Computer-Vision/test_time_adaptation/image_generation/llm_context.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6741bb-e7e6-4f7e-9e60-0e07ed4b5bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate images\n",
    "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, DPMSolverMultistepScheduler\n",
    "\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipet2i = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "pipet2i.scheduler = DPMSolverMultistepScheduler.from_config(pipet2i.scheduler.config)\n",
    "pipet2i = pipet2i.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103c72f4-8900-4f86-9a2e-4df731b313c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenetA_generator.generate_images(path = \"/home/sagemaker-user/Domain-Shift-Computer-Vision/imagenetA_generated\",\n",
    "                                    num_images = 1,\n",
    "                                    image_generation_pipeline = pipet2i,\n",
    "                                    num_inference_steps = 25,\n",
    "                                    guidance_scale = 9,\n",
    "                                    strength=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d969f7f1-a25b-4759-a0fa-87805a374609",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0a5f8d4c-4c0b-4e59-a36e-2bcac75e28b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility.data.get_data import get_data\n",
    "import clip\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e5fd05b-238a-4a95-b410-2ade85563a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "dataloader = get_data(batch_size=32, \n",
    "                      img_root = \"imagenet-a\",\n",
    "                      split_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0402185c-8779-4e60-83e7-c692b633069a",
   "metadata": {},
   "outputs": [],
   "source": [
    "candle_img = dataloader.dataset[5200][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a856cf-f4d6-4507-8f90-0dd4dda18777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "path_gen = \"/home/sagemaker-user/Domain-Shift-Computer-Vision/imagenetA_generated\"\n",
    "for class_name in os.listdir(path_gen):\n",
    "    class_path = os.path.join(path_gen, class_name)\n",
    "    for gen_images_class in os.listdir(class_path):\n",
    "        gen_images_class = os.path.join(class_path,gen_images_class)\n",
    "        for gen_image in os.listdir(gen_image_class):\n",
    "            gen_image_path = os.path.join(gen_images_class, gen_image)\n",
    "            prompt_embedding_path = os.path.join(gen_image_class, \"prompt_clip_embedding.pt\")\n",
    "            prompt_embedding_clip = torch.load(prompt_embedding_path)\n",
    "            if not torch.isclose(prompt_embedding_clip.norm(),torch.tensor(1.)).item():\n",
    "                prompt_embedding_clip /= prompt_embedding_clip.norm()\n",
    "                torch.save(prompt_embedding_clip, prompt_embedding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c45d975-d5de-4788-ae16-14774f9633a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_image_encoder = \"ViT-L/14\"\n",
    "clip_model, preprocess = clip.load(clip_image_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fdd52b1e-b5e2-4881-9340-93ae9fdf6d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_gen_images(img, data_path, num_images, clip_model, use_t2i_similarity = False, t2i_images = True, i2i_images = False):\n",
    "\n",
    "    assert any(i2i_images, t2i_images), \"One of t2i_images and i2i_images must be true\"\n",
    "\n",
    "    retrieved_images_paths = []\n",
    "    retrieved_images_similarity = torch.zeros(num_images)\n",
    "    image_embedding = clip_model.encode_image(preprocess(image).unsqueeze(0).cuda())\n",
    "    image_embedding /= image_embedding.norm()\n",
    "    \n",
    "    for class_name in os.listdir(data_path):\n",
    "        class_path = os.path.join(data_path, class_name)\n",
    "        for gen_images_class in os.listdir(class_path):\n",
    "            gen_images_class_path = os.path.join(class_path,gen_images_class)\n",
    "            for gen_images in os.listdir(gen_images_class_path):\n",
    "                gen_image_path = os.path.join(gen_images_class_path, gen_images)\n",
    "                gen_prompt_embedding_clip = torch.load(os.path.join(gen_image_class, \"prompt_clip_embedding.pt\"))\n",
    "                t2i_similarity = F.cosine_similarity(image_embedding, gen_prompt_embedding, dim=0)\n",
    "                if t2i_images:\n",
    "                    t2i_gen_images_main_path = os.path.join(gen_image_path,\"t2i_gen_images\")\n",
    "                    for t2i_images_paths in os.listdir(t2i_gen_images_main_path):\n",
    "                        t2i_image_path = os.path.join(t2i_gen_images_main_path,t2i_images_paths)\n",
    "                        gen_image_embedding = torch.load(os.path.join(t2i_image_path, \"image_embedding.pt\"))\n",
    "                        i2i_similarity = F.cosine_similarity(image_embedding, gen_image_embedding, dim=0)\n",
    "                        if use_t2i_similarity:\n",
    "                            similarity = (i2i_similarity + t2i_similarity)/2 # avg similarity\n",
    "                        else:\n",
    "                            similarity = i2i_similarity\n",
    "                        if len(retrieved_images) < num_images:\n",
    "                            retrieved_images_similarity[len(retrieved_images)] = similarity\n",
    "                            retrieved_images.append(os.path.join(t2i_image_path, \"image.png\"))\n",
    "                        else:\n",
    "                            min_similarity, id_similarity = retrieved_images_similarity.min()\n",
    "                            if similarity > min_similarity:\n",
    "                                retrieved_images_similarity[id_similarity] = similarity\n",
    "                                retrieved_images[id_similarity] = os.path.join(t2i_image_path, \"image.png\")\n",
    "                if i2i_images:\n",
    "                    i2i_gen_images_main_path = os.path.join(gen_image_path,\"i2i_gen_images\")\n",
    "                    for i2i_images_paths in os.listdir(i2i_gen_images_main_path):\n",
    "                        i2i_image_path = os.path.join(i2i_gen_images_main_path,i2i_images_paths)\n",
    "                        gen_image_embedding = torch.load(os.path.join(i2i_image_path, \"image_embedding.pt\"))\n",
    "    \n",
    "                        i2i_similarity = F.cosine_similarity(image_embedding, gen_image_embedding, dim=0)\n",
    "                        if use_t2i_similarity:\n",
    "                            similarity = (i2i_similarity + t2i_similarity)/2 # avg similarity\n",
    "                        else:\n",
    "                            similarity = i2i_similarity\n",
    "                        if len(retrieved_images) < num_images:\n",
    "                            retrieved_images_similarity[len(retrieved_images)] = similarity\n",
    "                            retrieved_images.append(os.path.join(t2i_image_path, \"image.png\"))\n",
    "                        else:\n",
    "                            min_similarity, id_similarity = retrieved_images_similarity.min()\n",
    "                            if similarity > min_similarity:\n",
    "                                retrieved_images_similarity[id_similarity] = similarity\n",
    "                                retrieved_images[id_similarity] = os.path.join(i2i_image_path, \"image.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
