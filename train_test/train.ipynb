{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: the working directory was changed to /home/peppe/01_Study/01_University/Semester/2/Deep_learning/Project/Repository/Domain-Shift-Computer-Vision\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# to import modules from other directories\n",
    "os.chdir(\"/home/peppe/01_Study/01_University/Semester/2/Deep_learning/Project/Repository/Domain-Shift-Computer-Vision\") \n",
    "print(\"Warning: the working directory was changed to\", os.getcwd())\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from train_test.test import Tester\n",
    "\n",
    "class Trainer(Tester):\n",
    "\n",
    "    def __init__(self, \n",
    "                 data_loaders: dict, \n",
    "                 dataset_name: str,\n",
    "                 model: torch.nn.Module, \n",
    "                 optimizer: torch.optim, \n",
    "                 loss_fn: torch.nn, \n",
    "                 device, \n",
    "                 seed: int,\n",
    "                 exp_name, # the name of this experiment\n",
    "                 exp_path, # where you keep all the experiments\n",
    "                 use_early_stopping=True,\n",
    "                 patience=5,\n",
    "                 delta=1e-3,\n",
    "                 scheduler=None,\n",
    "                 num_classes = None):\n",
    "        \"\"\"\n",
    "        The exp_name should be a string containing all the information about the experiment:\n",
    "        - model \n",
    "        - optimizer \n",
    "        - loss function\n",
    "        - other hyperparameters \n",
    "        \"\"\"\n",
    "        super().__init__(model, data_loaders, device, loss_fn, num_classes)\n",
    "        self.__optimizer = optimizer        \n",
    "        self.__use_early_stopping = use_early_stopping\n",
    "        self.__seed = seed\n",
    "        self.__epoch = 0\n",
    "        self.__trained = False\n",
    "        self.__scheduler = scheduler\n",
    "        self.__best_loss = float(\"inf\")\n",
    "\n",
    "        assert os.path.exists(exp_path), \"Experiment path does not exist\"\n",
    "        assert os.path.exists(os.path.join(exp_path, exp_name)) == False, \"The experiment already exists\"\n",
    "        assert isinstance(patience, int) and patience > 0, \"patience must be a positive integer\"\n",
    "        assert isinstance(delta, float) and delta > 0, \"delta must be a positive float\"\n",
    "        assert isinstance(use_early_stopping, bool), \"use_early_stopping must be a boolean\"\n",
    "        assert isinstance(data_loaders, dict), \"data_loaders must be a dictionary with keys 'train_loader', 'val_loader', 'test_loader'\"\n",
    "\n",
    "        self.__exp_name = os.path.join(exp_path, exp_name)     \n",
    "\n",
    "        if self.__use_early_stopping:\n",
    "            from utility.early_stopping import EarlyStopping\n",
    "            self.__early_stopping = EarlyStopping(patience=patience, \n",
    "                                                  delta=delta,\n",
    "                                                  path=os.path.join(self.__exp_name,\"checkpoint.pth\"))      \n",
    "\n",
    "        os.makedirs(self.__exp_name, exist_ok=True) \n",
    "        self.__writer = SummaryWriter(log_dir=f\"{self.__exp_name}\")\n",
    "        self.__save_config(dataset_name, patience, delta)\n",
    "\n",
    "    def get_model(self):\n",
    "        return self._Tester__model\n",
    "    \n",
    "    def get_optimizer(self):\n",
    "        return self.__optimizer\n",
    "    \n",
    "    def get_scheduler(self):\n",
    "        return self.__scheduler\n",
    "    \n",
    "    def get_device(self):\n",
    "        return self._Tester__device\n",
    "    \n",
    "    def get_exp_name(self):\n",
    "        return self.__exp_name\n",
    "\n",
    "    def __train_step(self, verbose, log_interval):\n",
    "        \"\"\"\n",
    "        log_interval can be an integer or a float between 0 and 1. If it is an integer, the function will print the statistics every log_interval steps.\n",
    "        If it is a float, the function will print the statistics every log_interval*num_of_batches steps.\n",
    "        \"\"\"\n",
    "        samples = 0.0\n",
    "        cumulative_loss = 0.0\n",
    "        cumulative_accuracy = 0.0\n",
    "        \n",
    "        assert isinstance(verbose, bool), \"verbose must be a boolean\"\n",
    "        assert isinstance(log_interval, (int, float)) and log_interval>0, \"log_interval must be an integer or a float and non-negative\"\n",
    "        if log_interval < 1:\n",
    "            log_interval = int(len(self._Tester__data_loaders[\"train_loader\"])*log_interval)\n",
    "\n",
    "        self._Tester__model.train()\n",
    "\n",
    "        num_samples = len(self._Tester__data_loaders[\"train_loader\"].dataset)\n",
    "        y_true = np.zeros(num_samples, dtype=int)\n",
    "        y_pred = np.zeros(num_samples, dtype=int)\n",
    "\n",
    "        index = 0\n",
    "        for batch_idx, (inputs, targets) in enumerate(self._Tester__data_loaders[\"train_loader\"]):\n",
    "            inputs, targets = inputs.to(self._Tester__device), targets.to(self._Tester__device)\n",
    "            \n",
    "            outputs = self._Tester__model(inputs)\n",
    "            loss = self._Tester__loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            self.__optimizer.step()\n",
    "            self.__optimizer.zero_grad()\n",
    "            cumulative_loss += loss.item()\n",
    "            _, predicted = outputs.max(dim=1)\n",
    "            cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "\n",
    "            samples += inputs.shape[0]\n",
    "\n",
    "            if verbose and batch_idx % log_interval == 0:\n",
    "                current_loss = cumulative_loss / samples\n",
    "                current_accuracy = cumulative_accuracy / samples * 100\n",
    "                print(f'Batch {batch_idx}/{len(self._Tester__data_loaders[\"train_loader\"])}, Loss: {current_loss:.4f}, Accuracy: {current_accuracy:.2f}%', end='\\r')\n",
    "            \n",
    "            batch_size = inputs.shape[0]\n",
    "            y_true[index:index + batch_size] = targets.cpu().numpy()\n",
    "            y_pred[index:index + batch_size] = predicted.cpu().numpy()\n",
    "            index += batch_size\n",
    "\n",
    "        if self.__scheduler:\n",
    "            self.__scheduler.step()\n",
    "\n",
    "        accuracy = cumulative_accuracy / samples * 100    \n",
    "        avg_loss = cumulative_loss / samples    \n",
    "\n",
    "        return avg_loss, accuracy\n",
    "\n",
    "    def main(self,\n",
    "             epochs=10,\n",
    "             verbose_steps=True, # print after log_interval-learning steps\n",
    "             log_interval=10): \n",
    "\n",
    "        from utility.initialize import initialize\n",
    "        initialize(self.__seed)\n",
    "            \n",
    "        self._Tester__model.to(self._Tester__device)\n",
    "                \n",
    "        # Log to TensorBoard\n",
    "        if self.__trained == False:\n",
    "            self.__trained = True\n",
    "            print(\"Before training:\")\n",
    "            train_loss, train_accuracy, train_precision, train_recall = self.test_step(train=True, precision=True, recall=True)\n",
    "            val_loss, val_accuracy, val_precision, val_recall = self.test_step(eval=True,precision=True, recall=True) \n",
    "            test_loss, test_accuracy, test_precision, test_recall = self.test_step(test=True,precision=True, recall=True)\n",
    "            self.__log_values(self.__writer, self.__epoch, train_loss, train_accuracy,train_precision, train_recall, \"Train\")\n",
    "            self.__log_values(self.__writer, self.__epoch, val_loss, val_accuracy, val_precision, val_recall, \"Validation\")\n",
    "            self.__log_values(self.__writer, self.__epoch, test_loss, test_accuracy, test_precision, test_recall, \"Test\")\n",
    "            self.__print_statistics(train_loss, train_accuracy, val_loss, val_accuracy, test_loss, test_accuracy)\n",
    "        \n",
    "        pbar = tqdm(range(epochs), desc=\"Training\")\n",
    "        for _ in pbar:\n",
    "            train_loss, train_accuracy, train_precision, train_recall = self.__train_step(verbose=verbose_steps, log_interval=log_interval)\n",
    "            val_loss, val_accuracy, val_precision, val_recall = self.test_step(eval=True) \n",
    "\n",
    "            print(\"-----------------------------------------------------\")\n",
    "            self.__epoch += 1\n",
    "            self.__log_values(self.__writer, self.__epoch, train_loss, train_accuracy,train_precision, train_recall, \"Train\")\n",
    "            self.__log_values(self.__writer, self.__epoch, val_loss, val_accuracy, val_precision, val_recall, \"Validation\")\n",
    "\n",
    "            pbar.set_postfix(train_loss=train_loss, train_accuracy=train_accuracy, val_loss=val_loss, val_accuracy=val_accuracy)\n",
    "\n",
    "            if self.__use_early_stopping:\n",
    "                self.__early_stopping(val_loss,self._Tester__model, self.__optimizer, self.__scheduler)\n",
    "                if self.__early_stopping.early_stop:\n",
    "                    print(\"Early stopping\")\n",
    "                    break       \n",
    "            else:\n",
    "                if val_loss < self.__best_loss:\n",
    "                    self.__best_loss = val_loss\n",
    "                    torch.save({\n",
    "                        \"model\": self._Tester__model.state_dict(),\n",
    "                        \"optimizer\": self.__optimizer.state_dict(),\n",
    "                        \"scheduler\": self.__scheduler.state_dict() if self.__scheduler is not None else None\n",
    "                        }, \n",
    "                        os.path.join(self.__exp_name,\"checkpoint.pth\"))\n",
    "        \n",
    "        # Compute final evaluation results\n",
    "        print(\"After training:\")\n",
    "        train_loss, train_accuracy = self.test_step(train=True)\n",
    "        val_loss, val_accuracy = self.test_step(eval=True) \n",
    "        test_loss, test_accuracy = self.test_step(test=True)\n",
    "\n",
    "        self.__log_values(self.__writer, self.__epoch, train_loss, train_accuracy, \"Train\")\n",
    "        self.__log_values(self.__writer, self.__epoch, val_loss, val_accuracy, \"Validation\")\n",
    "        self.__log_values(self.__writer, self.__epoch, test_loss, test_accuracy, \"Test\")\n",
    "\n",
    "        self.__print_statistics(train_loss, train_accuracy, val_loss, val_accuracy, test_loss, test_accuracy)\n",
    "\n",
    "        # Flush the logs to disk \n",
    "        self.__writer.flush()            \n",
    "\n",
    "    def close_writer(self):\n",
    "        self.__writer.close()\n",
    "        print(\"Writer closed\")\n",
    "\n",
    "    def open_writer(self):\n",
    "        self.__writer = SummaryWriter(log_dir=f\"{self.__exp_name}\")\n",
    "        print(\"A new writer was opened \")\n",
    "\n",
    "    def set_exp_name(self, new_name):\n",
    "        self.__exp_name = new_name\n",
    "        self.__writer = SummaryWriter(log_dir=f\"{self.__exp_name}\")\n",
    "        print(f\"Experiment name was changed to {new_name}\")\n",
    "    \n",
    "    def __print_statistics(self, train_loss, train_accuracy, val_loss, val_accuracy, test_loss, test_accuracy):\n",
    "        print(f\"\\tTraining loss {train_loss:.5f}, Training accuracy {train_accuracy:.2f}\")\n",
    "        print(f\"\\tValidation loss {val_loss:.5f}, Validation accuracy {val_accuracy:.2f}\")\n",
    "        print(f\"\\tTest loss {test_loss:.5f}, Test accuracy {test_accuracy:.2f}\")\n",
    "        print(\"-----------------------------------------------------\")\n",
    "\n",
    "    # tensorboard logging utilities\n",
    "    def __log_values(self, writer, step, loss, accuracy, prefix):\n",
    "        writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
    "        writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)\n",
    "        \n",
    "    def __save_config(self, dataset_name, patience, delta):\n",
    "        config = {\n",
    "            'data': { \n",
    "                'batch_size':self._Tester__data_loaders[\"train_loader\"].batch_size,\n",
    "                'dataset_name': dataset_name\n",
    "            }, \n",
    "            'model': self._Tester__model.__class__.__name__,\n",
    "            'optimizer': {\n",
    "                'optimizer': self.__optimizer.__class__.__name__,\n",
    "                'momentum': self.__optimizer.param_groups[0]['momentum'],\n",
    "                'weight_decay': self.__optimizer.param_groups[0]['weight_decay'],\n",
    "                'lr': self.__optimizer.param_groups[0]['lr'],\n",
    "            } ,\n",
    "            'scheduler': self.__scheduler.__class__.__name__ if self.__scheduler is not None else None,\n",
    "            'loss_fn': {\n",
    "                'loss_fn': self._Tester__loss_fn.__class__.__name__,\n",
    "                'smoothing': self._Tester__loss_fn.label_smoothing\n",
    "            },\n",
    "            'seed': self.__seed,\n",
    "            'early_stopping': {\n",
    "                'use_early_stopping': self.__use_early_stopping,\n",
    "                'patience': patience,\n",
    "                'delta': delta\n",
    "            }\n",
    "        }\n",
    "        import json\n",
    "        config_file_path = f\"{self.__exp_name}/config.json\"\n",
    "        with open(config_file_path, 'w') as file:\n",
    "            json.dump(config, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
